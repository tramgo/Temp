# --- TD3 Model Initialization and Training ---

# Initialize TD3 model with best hyperparameters and enable CPU usage
try:
    model = TD3(
        'MlpPolicy',
        vec_env_train,
        verbose=1,  # Enable logging
        seed=RANDOM_SEED,
        policy_kwargs=policy_kwargs,
        learning_rate=best_params.get('learning_rate', 3e-4),
        buffer_size=best_params.get('buffer_size', 1000000),  # Off-policy replay buffer size
        batch_size=best_params.get('batch_size', 256),
        tau=best_params.get('tau', 0.005),  # Soft update coefficient for target networks
        gamma=best_params.get('gamma', 0.99),
        tensorboard_log=str(main_training_log_dir),
        device='cpu'
    )
except Exception as e:
    main_logger.critical(f"TD3 Model initialization failed: {e}")
    exit()

# Define checkpoint and custom callbacks (same as before)
checkpoint_callback = CheckpointCallback(
    save_freq=50000,  # Adjust as needed
    save_path=str(RESULTS_DIR / "checkpoints"),
    name_prefix="td3_model"
)
custom_callback = CustomTensorboardCallback()
early_stopping_callback = EarlyStoppingCallback(
    monitor='train/reward_env',
    patience=10000,
    min_delta=1e-5,
    verbose=1
)
callback_list = CallbackList([custom_callback, checkpoint_callback, early_stopping_callback])

# Start training
start_time = time.time()
try:
    model.learn(
        total_timesteps=500000,  # Adjust training steps as needed
        callback=callback_list
    )
except Exception as e:
    main_logger.critical(f"Training failed: {e}")
    exit()
duration = time.time() - start_time

# Log Phase: Main Training Completed
log_phase("Main Training", "Completed", {"env_rank": main_env_rank, "total_timesteps": 500000}, duration)

# Save the trained model
model_path = RESULTS_DIR / f"td3_model_{TICKER}.zip"
model.save(str(model_path))
main_logger.info(f"TD3 model trained and saved at {model_path}")

============

# --- SAC Model Initialization and Training ---

# Initialize SAC model with best hyperparameters and enable CPU usage
try:
    model = SAC(
        'MlpPolicy',
        vec_env_train,
        verbose=1,  # Enable logging
        seed=RANDOM_SEED,
        policy_kwargs=policy_kwargs,
        learning_rate=best_params.get('learning_rate', 3e-4),
        batch_size=best_params.get('batch_size', 256),
        tau=best_params.get('tau', 0.005),  # Soft update coefficient
        gamma=best_params.get('gamma', 0.99),
        ent_coef=best_params.get('ent_coef', 0.02),  # SAC uses entropy coefficient for exploration
        tensorboard_log=str(main_training_log_dir),
        device='cpu'
    )
except Exception as e:
    main_logger.critical(f"SAC Model initialization failed: {e}")
    exit()

# Define checkpoint and custom callbacks (similar to TD3)
checkpoint_callback = CheckpointCallback(
    save_freq=50000,  # Adjust as needed
    save_path=str(RESULTS_DIR / "checkpoints"),
    name_prefix="sac_model"
)
custom_callback = CustomTensorboardCallback()
early_stopping_callback = EarlyStoppingCallback(
    monitor='train/reward_env',
    patience=10000,
    min_delta=1e-5,
    verbose=1
)
callback_list = CallbackList([custom_callback, checkpoint_callback, early_stopping_callback])

# Start training
start_time = time.time()
try:
    model.learn(
        total_timesteps=500000,  # Adjust training steps as needed
        callback=callback_list
    )
except Exception as e:
    main_logger.critical(f"Training failed: {e}")
    exit()
duration = time.time() - start_time

# Log Phase: Main Training Completed
log_phase("Main Training", "Completed", {"env_rank": main_env_rank, "total_timesteps": 500000}, duration)

# Save the trained model
model_path = RESULTS_DIR / f"sac_model_{TICKER}.zip"
model.save(str(model_path))
main_logger.info(f"SAC model trained and saved at {model_path}")

