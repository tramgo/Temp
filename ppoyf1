import os
import gymnasium as gym
from gymnasium import spaces
import numpy as np
import pandas as pd
import yfinance as yf
from ta import trend, momentum, volatility, volume
import matplotlib.pyplot as plt
import seaborn as sns
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv
from stable_baselines3.common.env_checker import check_env
from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback, CallbackList
import torch
import warnings
from typing import Optional, Tuple
import random
import datetime
from sklearn.preprocessing import StandardScaler
import math
import logging
from pathlib import Path
import optuna
import joblib
import time
import multiprocessing

# Import ConcurrentRotatingFileHandler for robust multi-process logging
try:
    from concurrent_log_handler import ConcurrentRotatingFileHandler
except ImportError:
    raise ImportError("Please install 'concurrent-log-handler' package via pip: pip install concurrent-log-handler")

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

# Set random seeds for reproducibility
RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)

# Define feature sets as constants
FEATURES_TO_SCALE = [
    'Close', 'SMA10', 'SMA50', 'RSI', 'MACD',
    'BB_Upper', 'BB_Lower', 'Bollinger_Width',
    'EMA20', 'VWAP', 'Lagged_Return', 'Volatility'
]

UNSCALED_FEATURES = [
    f"{feature}_unscaled" for feature in FEATURES_TO_SCALE if feature != 'Volatility'
] + ['Volatility_unscaled']

# Define directories for results and plots
BASE_DIR = Path('.').resolve()
RESULTS_DIR = BASE_DIR / 'results'
PLOTS_DIR = BASE_DIR / 'plots'
TB_LOG_DIR = BASE_DIR / 'tensorboard_logs'
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
PLOTS_DIR.mkdir(parents=True, exist_ok=True)
TB_LOG_DIR.mkdir(parents=True, exist_ok=True)

# Print the absolute paths
print(f"Base Directory: {BASE_DIR}")
print(f"Results Directory: {RESULTS_DIR}")
print(f"Plots Directory: {PLOTS_DIR}")
print(f"TensorBoard Logs Directory: {TB_LOG_DIR}")

# Configure Logging
logging.basicConfig(level=logging.DEBUG)  # Set to DEBUG for detailed logs
logger = logging.getLogger()
logger.handlers = []  # Remove any default handlers
handler = ConcurrentRotatingFileHandler(str(RESULTS_DIR / 'training.log'), maxBytes=10**6, backupCount=5)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)

# Log the absolute paths
logging.info(f"Base Directory: {BASE_DIR}")
logging.info(f"Results Directory: {RESULTS_DIR}")
logging.info(f"Plots Directory: {PLOTS_DIR}")
logging.info(f"TensorBoard Logs Directory: {TB_LOG_DIR}")

##############################################
# Version Checks
##############################################

def check_versions():
    """
    Checks and logs the versions of key libraries to ensure compatibility.
    """
    import stable_baselines3
    import gymnasium
    import optuna

    sb3_version = stable_baselines3.__version__
    gymnasium_version = gymnasium.__version__
    optuna_version = optuna.__version__

    logging.debug(f"Stable Baselines3 version: {sb3_version}")
    logging.debug(f"Gymnasium version: {gymnasium_version}")
    logging.debug(f"Optuna version: {optuna_version}")

    # Ensure SB3 is at least version 1.4.0
    try:
        sb3_major, sb3_minor, sb3_patch = map(int, sb3_version.split('.')[:3])
        if sb3_major < 1 or (sb3_major == 1 and sb3_minor < 4):
            logging.error("Stable Baselines3 version must be at least 1.4.0. Please upgrade SB3.")
            exit()
    except:
        logging.error("Unable to parse Stable Baselines3 version. Please ensure it's installed correctly.")
        exit()

    # Ensure Gymnasium is updated
    if gymnasium_version < '0.28.1':  # Example minimum version
        logging.warning("Consider upgrading Gymnasium to the latest version for better compatibility.")

check_versions()

##############################################
# Fetch and Prepare Data
##############################################

def get_data(ticker: str, start_date: str, end_date: str) -> pd.DataFrame:
    """
    Fetches historical stock data from Yahoo Finance and calculates technical indicators.

    Args:
        ticker (str): Stock ticker symbol.
        start_date (str): Start date in 'YYYY-MM-DD' format.
        end_date (str): End date in 'YYYY-MM-DD' format.

    Returns:
        pd.DataFrame: Processed DataFrame with technical indicators and both scaled/unscaled features.
    """
    logging.info(f"Fetching data for {ticker} from {start_date} to {end_date}")
    df = yf.download(ticker, start=start_date, end=end_date, progress=False)
    if df.empty:
        logging.error(f"No data fetched for {ticker}")
        return df

    required_columns = ['Close', 'High', 'Low', 'Volume']
    for col in required_columns:
        if col not in df.columns:
            logging.error(f"{col} not in downloaded data for {ticker}.")
            return pd.DataFrame()

    if len(df) < 200:
        logging.error(f"Not enough data points for {ticker}.")
        return pd.DataFrame()

    close_col = 'Close'
    try:
        close = df[close_col].squeeze()
        high = df['High'].squeeze()
        low = df['Low'].squeeze()
        volume_col = df['Volume'].squeeze()

        # Calculate technical indicators
        sma10 = trend.SMAIndicator(close=close, window=10).sma_indicator()
        sma50 = trend.SMAIndicator(close=close, window=50).sma_indicator()
        rsi = momentum.RSIIndicator(close=close, window=14).rsi()
        macd = trend.MACD(close=close).macd()
        adx = trend.ADXIndicator(high=high, low=low, close=close, window=14).adx()
        bollinger = volatility.BollingerBands(close=close, window=20, window_dev=2)
        bb_upper = bollinger.bollinger_hband()
        bb_lower = bollinger.bollinger_lband()
        bollinger_width = bollinger.bollinger_wband()
        ema20 = trend.EMAIndicator(close=close, window=20).ema_indicator()
        vwap = volume.VolumeWeightedAveragePrice(high=high, low=low, close=close, volume=volume_col, window=14).volume_weighted_average_price()
        lagged_return = close.pct_change().fillna(0)
        atr = volatility.AverageTrueRange(high=high, low=low, close=close, window=14).average_true_range()
    except Exception as e:
        logging.error(f"Error calculating indicators for {ticker}: {e}")
        return pd.DataFrame()

    # Append indicators to DataFrame
    df['SMA10'] = sma10
    df['SMA50'] = sma50
    df['RSI'] = rsi
    df['MACD'] = macd
    df['ADX'] = adx
    df['BB_Upper'] = bb_upper
    df['BB_Lower'] = bb_lower
    df['Bollinger_Width'] = bollinger_width
    df['EMA20'] = ema20
    df['VWAP'] = vwap
    df['Lagged_Return'] = lagged_return
    df['Volatility'] = atr

    # Add unscaled versions of relevant features
    df['Close_unscaled'] = df['Close']
    df['SMA10_unscaled'] = df['SMA10']
    df['SMA50_unscaled'] = df['SMA50']
    df['RSI_unscaled'] = df['RSI']
    df['MACD_unscaled'] = df['MACD']
    df['ADX_unscaled'] = df['ADX']
    df['BB_Upper_unscaled'] = df['BB_Upper']
    df['BB_Lower_unscaled'] = df['BB_Lower']
    df['EMA20_unscaled'] = df['EMA20']
    df['VWAP_unscaled'] = df['VWAP']
    df['Bollinger_Width_unscaled'] = df['Bollinger_Width']
    df['Lagged_Return_unscaled'] = df['Lagged_Return']
    df['Volatility_unscaled'] = df['Volatility']

    # Handle missing values
    df.fillna(method='ffill', inplace=True)
    df.fillna(0, inplace=True)
    df.reset_index(inplace=True)

    # Select features for scaling
    features_to_scale = FEATURES_TO_SCALE

    # Initialize scaler
    scaler = StandardScaler()

    # Scale features
    df[features_to_scale] = scaler.fit_transform(df[features_to_scale])

    # Save the scaler for future use (e.g., deployment)
    scaler_filename = RESULTS_DIR / 'scaler.pkl'
    joblib.dump(scaler, scaler_filename)
    logging.info(f"Scaler fitted on training data and saved as {scaler_filename}")

    # Check for missing unscaled features
    missing_unscaled = [feature for feature in UNSCALED_FEATURES if feature not in df.columns]
    if missing_unscaled:
        logging.error(f"Missing unscaled features after processing: {missing_unscaled}")
    else:
        logging.debug("All unscaled features are present in the DataFrame.")
        logging.debug(f"DataFrame columns: {df.columns.tolist()}")
        logging.debug(f"First 5 rows:\n{df.head()}")

    logging.info(f"Data for {ticker} fetched and processed successfully.")
    return df

##############################################
# Custom Trading Environment
##############################################

class SingleStockTradingEnv(gym.Env):
    """
    A custom Gym environment for single stock trading.
    """
    metadata = {'render.modes': ['human']}

    def __init__(self, df: pd.DataFrame, scaler: StandardScaler,
                 initial_balance: float = 100000,
                 stop_loss: float = 0.90, take_profit: float = 1.10,
                 max_position_size: float = 0.25, max_drawdown: float = 0.20,
                 annual_trading_days: int = 252, transaction_cost: float = 0.001,
                 env_rank: int = 0):
        super(SingleStockTradingEnv, self).__init__()

        self.env_rank = env_rank  # Unique identifier for the environment

        self.df = df.copy().reset_index(drop=True)
        self.scaler = scaler
        self.initial_balance = initial_balance
        self.current_step = 0
        self.stop_loss = stop_loss
        self.take_profit = take_profit
        self.max_position_size = max_position_size
        self.max_drawdown = max_drawdown
        self.annual_trading_days = annual_trading_days
        self.transaction_cost = transaction_cost  # 0.1% per trade

        # Action space: Discrete actions {0: Hold, 1: Buy, 2: Sell}
        self.action_space = spaces.Discrete(3)

        # Observation space: features + balance, net worth, position + market phase
        self.num_features = len(FEATURES_TO_SCALE)
        self.market_phase = ['Bull', 'Bear', 'Sideways']
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf,
            shape=(self.num_features + 3 + len(self.market_phase),),
            dtype=np.float32
        )

        self.feature_names = FEATURES_TO_SCALE

        # Risk Management Parameters
        self.dynamic_drawdown = True  # Enable dynamic drawdown
        self.volatility_window = 14  # Number of periods to calculate rolling ATR
        self.drawdown_multiplier = 1.5  # Multiplier to adjust drawdown based on ATR
        self.base_drawdown = max_drawdown
        self.current_drawdown = max_drawdown

        # Initialize environment state
        self.reset()

    def seed(self, seed=None):
        """
        Sets the seed for the environment's random number generators.

        Args:
            seed (int, optional): Seed value. Defaults to None.
        """
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        logging.debug(f"[Env {self.env_rank}] Seed set to {seed}")

    def _next_observation(self) -> np.ndarray:
        """
        Returns the next observation.
        """
        if self.current_step >= len(self.df):
            self.current_step = len(self.df) - 1
        current_data = self.df.iloc[self.current_step]
        features = current_data[self.feature_names].values
        obs = list(features)

        # Append balance, net worth, and position
        obs.append(self.balance / self.initial_balance)
        obs.append(self.net_worth / self.initial_balance)
        obs.append(self.position / self.initial_balance)

        # Determine market phase
        try:
            adx = float(current_data['ADX_unscaled'])  # Ensure this matches the DataFrame's feature name
        except KeyError:
            logging.error(f"[Env {self.env_rank}] 'ADX_unscaled' not found in current_data at step {self.current_step}. Setting ADX to 0.")
            adx = 0.0

        if adx > 25:
            if float(current_data['SMA10_unscaled']) > float(current_data['SMA50_unscaled']):
                phase = 'Bull'
            else:
                phase = 'Bear'
        else:
            phase = 'Sideways'

        # One-hot encode market phase
        for p in self.market_phase:
            obs.append(1.0 if phase == p else 0.0)

        obs = np.array(obs, dtype=np.float32)
        if np.isnan(obs).any() or np.isinf(obs).any():
            obs = np.nan_to_num(obs, nan=0.0, posinf=0.0, neginf=0.0)

        # Sanity check
        assert obs.shape[0] == self.observation_space.shape[0], "Observation shape mismatch!"
        assert not np.isnan(obs).any(), "Observation contains NaN!"

        return obs

    def _calculate_dynamic_drawdown(self) -> float:
        """
        Adjusts the drawdown threshold based on recent volatility.
        """
        if self.current_step < self.volatility_window:
            # Not enough data to adjust drawdown
            return self.base_drawdown
        recent_atr = self.df['Volatility'].iloc[self.current_step - self.volatility_window:self.current_step].mean()
        # Higher ATR -> Lower drawdown threshold
        dynamic_threshold = self.base_drawdown / (1 + self.drawdown_multiplier * recent_atr)
        dynamic_threshold = max(0.05, min(dynamic_threshold, self.base_drawdown))  # Clamp between 5% and base_drawdown
        return dynamic_threshold

    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, dict]:
        """
        Executes one time step within the environment based on the discrete action.

        Args:
            action (int): Action to take (0: Hold, 1: Buy, 2: Sell)

        Returns:
            Tuple containing:
            - obs (np.ndarray): Next observation.
            - reward (float): Reward obtained.
            - terminated (bool): Whether the episode has terminated.
            - truncated (bool): Whether the episode was truncated.
            - info (dict): Additional information.
        """
        try:
            # Prevent buying immediately after selling and vice versa
            if (self.last_action == 1 and action == 2) or (self.last_action == 2 and action == 1):
                logging.warning(f"[Env {self.env_rank}] Step {self.current_step}: Prevented action {action} following action {self.last_action}")
                action = 0  # Force Hold action

            # Validate action
            assert self.action_space.contains(action), f"[Env {self.env_rank}] Invalid action: {action}"

            # Fetch current data
            if self.current_step >= len(self.df):
                logging.debug(f"[Env {self.env_rank}] Current step {self.current_step} exceeds data length. Terminating episode.")
                terminated = True
                truncated = False
                reward = -1000  # Penalty for exceeding data
                obs = self._next_observation()
                logging.error(f"[Env {self.env_rank}] Terminating episode at step {self.current_step} due to data overflow.")
                return obs, reward, terminated, truncated, {}

            current_data = self.df.iloc[self.current_step]
            current_price = float(current_data['Close_unscaled'])
            buy_signal_price = np.nan
            sell_signal_price = np.nan

            # Initialize reward components
            reward_profit = 0.0
            reward_sharpe = 0.0
            reward_sortino = 0.0
            reward_costs = 0.0
            reward_drawdown = 0.0
            reward_inactivity = 0.0

            # Initialize shares_to_sell to prevent reference before assignment
            shares_to_sell = 0

            # Handle actions
            if action == 1:  # Buy
                shares_to_buy = math.floor(self.max_position_size * self.balance / current_price)
                if shares_to_buy > 0:
                    cost = shares_to_buy * current_price * (1 + self.transaction_cost)
                    if self.balance >= cost:
                        self.balance -= cost
                        self.position += shares_to_buy
                        buy_signal_price = current_price
                        logging.debug(f"[Env {self.env_rank}] Step {self.current_step}: Bought {shares_to_buy} shares at {current_price:.2f}")
                        self.last_action = 1  # Update last action
                        # For transaction costs reward component
                        reward_costs = -self.transaction_cost * shares_to_buy
            elif action == 2:  # Sell
                shares_to_sell = math.floor(0.5 * self.position)  # Sell 50% of holdings
                if shares_to_sell > 0:
                    proceeds = shares_to_sell * current_price * (1 - self.transaction_cost)
                    self.balance += proceeds
                    self.position -= shares_to_sell
                    sell_signal_price = current_price
                    logging.debug(f"[Env {self.env_rank}] Step {self.current_step}: Sold {shares_to_sell} shares at {current_price:.2f}")
                    self.last_action = 2  # Update last action
                    # For transaction costs reward component
                    reward_costs = -self.transaction_cost * shares_to_sell
            else:
                self.last_action = 0  # Reset last action on Hold

            # Update net worth
            self.net_worth = self.balance + self.position * current_price

            # Reward Components
            # (1) Profitability: Net worth change
            net_worth_change = self.net_worth - self.prev_net_worth
            reward_profit = net_worth_change / (self.prev_net_worth + 1e-8)

            # Initialize returns_window if not present
            if not hasattr(self, 'returns_window'):
                self.returns_window = []

            # (2) Sharpe Ratio: Risk-adjusted performance
            self.returns_window.append(net_worth_change / (self.prev_net_worth + 1e-8))
            if len(self.returns_window) > self.annual_trading_days:  # Use annual trading days for rolling Sharpe
                self.returns_window.pop(0)
            if len(self.returns_window) > 1:
                returns = np.array(self.returns_window)
                mean_return = returns.mean()
                std_return = returns.std() + 1e-8  # Avoid division by zero
                reward_sharpe = (mean_return / std_return) * math.sqrt(self.annual_trading_days)
                # (3) Sortino Ratio
                downside_returns = returns[returns < 0]
                downside_std = downside_returns.std() * math.sqrt(self.annual_trading_days) if len(downside_returns) > 0 else 0.0
                reward_sortino = (mean_return / (downside_std + 1e-8)) if downside_std > 0 else 0.0

            # (4) Drawdown Penalty
            if self.dynamic_drawdown:
                self.current_drawdown = self._calculate_dynamic_drawdown()
            else:
                self.current_drawdown = self.base_drawdown

            self.peak = max(self.peak, self.net_worth)
            drawdown = (self.net_worth - self.peak) / self.peak
            if drawdown < -self.current_drawdown:
                reward_drawdown = min(-abs(drawdown) * 100, -100)  # Scale and cap penalty

            # (5) Inactivity Penalty
            if abs(self.position) < 1e-3:
                reward_inactivity = -0.1  # Small penalty for inactivity

            # Combine reward components
            # Revised weights for balanced contribution
            weight_profit = 10
            weight_sharpe = 5
            weight_sortino = 5
            weight_drawdown = 2
            weight_inactivity = 0.1

            reward = (
                weight_profit * np.tanh(reward_profit) +
                weight_sharpe * reward_sharpe +
                weight_sortino * reward_sortino +
                reward_costs +
                weight_drawdown * reward_drawdown +
                weight_inactivity * reward_inactivity
            )

            # Avoid excessive clamping to retain information on extreme cases
            reward = max(-1000, min(reward, 1000))  # Clamp rewards between -1000 and +1000
            logging.debug(f"[Env {self.env_rank}] Step {self.current_step}: Reward = {reward:.4f}, "
                          f"Net Worth = {self.net_worth:.2f}, Previous Net Worth = {self.prev_net_worth:.2f}")

            # Check for episode termination
            terminated = False
            truncated = False

            if self.net_worth <= 0:
                terminated = True
                reward -= 1000  # Severe penalty for bankruptcy
                logging.error(f"[Env {self.env_rank}] Bankruptcy occurred. Terminating episode at step {self.current_step}.")
            elif self.current_step >= len(self.df) - 1:
                terminated = True
                logging.info(f"[Env {self.env_rank}] Reached end of data at step {self.current_step}. Terminating episode.")

            # Append to history
            self.history.append({
                'Step': self.current_step,
                'Date': current_data['Date'],
                'Balance': self.balance,
                'Position': self.position,
                'Net Worth': self.net_worth,
                'Reward': reward,
                'Buy_Signal_Price': buy_signal_price,
                'Sell_Signal_Price': sell_signal_price,
                'Action': action  # Log the action taken
            })

            # Advance to next step if not terminated
            if not terminated:
                self.prev_net_worth = self.net_worth
                logging.debug(f"[Env {self.env_rank}] Before increment: Step {self.current_step}")
                self.current_step += 1  # Ensure step increments
                logging.debug(f"[Env {self.env_rank}] After increment: Step {self.current_step}")
            else:
                # If terminated, do not increment step to prevent overshooting
                logging.debug(f"[Env {self.env_rank}] Episode terminated at step {self.current_step}")

            # Ensure current_step does not exceed data length
            self.current_step = min(self.current_step, len(self.df) - 1)

            # Observation after action
            obs = self._next_observation()

            # Logging every 100 steps or upon termination
            if self.current_step % 100 == 0 or terminated:
                logging.debug(f"[Env {self.env_rank}] Step {self.current_step}: Reward = {reward:.4f}, "
                              f"Net Worth = {self.net_worth:.2f}, Balance = {self.balance:.2f}, Position = {self.position}")

            # Log detailed state changes
            logging.debug(f"[Env {self.env_rank}] After Action {action}: Balance = {self.balance}, Position = {self.position}, Net Worth = {self.net_worth}")

            return obs, reward, terminated, truncated, {}
			
        except Exception as e:
            logging.critical(f"Error during step execution: {e}")
            raise e
			
    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None) -> Tuple[np.ndarray, dict]:
        """
        Resets the environment to an initial state.

        Returns:
            Tuple containing:
            - obs (np.ndarray): Initial observation.
            - info (dict): Additional information.
        """
        try:
            super().reset(seed=seed)
        except Exception as e:
            logging.critical(f"[Env {self.env_rank}] Superclass reset failed: {e}")
            raise e  # Re-raise the exception after logging

        try:
            self.balance = self.initial_balance
            self.position = 0.0  # Changed to float for fractional shares
            self.net_worth = self.initial_balance
            self.prev_net_worth = self.initial_balance
            self.peak = self.initial_balance
            self.current_step = 0
            self.history = []
            self.last_action = 0  # Initialize last action as Hold
            self.returns_window = []
            obs = self._next_observation()
            info = {}  # Return an empty info dictionary
            logging.info(f"[Env {self.env_rank}] Environment reset successfully. History cleared and step reset to 0.")
            return obs, info  # Return both obs and info
        except Exception as e:
            logging.critical(f"[Env {self.env_rank}] Error during environment reset: {e}")
            raise e  # Re-raise the exception after logging

##############################################
# Baseline Strategies
##############################################

def buy_and_hold(df: pd.DataFrame, initial_balance: float = 100000, transaction_cost: float = 0.001) -> dict:
    """
    Implements a Buy and Hold strategy.

    Args:
        df (pd.DataFrame): DataFrame containing stock prices.
        initial_balance (float): Starting balance.
        transaction_cost (float): Transaction cost per trade.

    Returns:
        dict: Results of the strategy.
    """
    # Invest the entire initial balance
    investment_percentage = 1.0  # 100% investment
    investment_amount = initial_balance * investment_percentage

    buy_price = df.iloc[0]['Close_unscaled']
    holdings = math.floor(investment_amount / buy_price)
    invested_capital = holdings * buy_price
    cost = holdings * buy_price * transaction_cost
    balance = initial_balance - invested_capital - cost  # Remaining balance after buying
    net_worth = balance + holdings * df.iloc[-1]['Close_unscaled']
    profit = net_worth - initial_balance
    return {
        'Strategy': 'Buy and Hold',
        'Initial Balance': initial_balance,
        'Final Net Worth': net_worth,
        'Profit': profit,
        'Invested Capital': invested_capital,
        'Transaction Costs': cost
    }

def moving_average_crossover(df: pd.DataFrame, initial_balance: float = 100000, transaction_cost: float = 0.001) -> dict:
    """
    Implements a Moving Average Crossover strategy with RSI confirmation.

    Args:
        df (pd.DataFrame): DataFrame containing stock prices and indicators.
        initial_balance (float): Starting balance.
        transaction_cost (float): Transaction cost per trade.

    Returns:
        dict: Results of the strategy.
    """
    balance = initial_balance
    holdings = 0
    net_worth = initial_balance

    for i in range(1, len(df)):
        # Golden cross with RSI < 30
        if df.loc[i-1, 'SMA10_unscaled'] < df.loc[i-1, 'SMA50_unscaled'] and df.loc[i, 'SMA10_unscaled'] >= df.loc[i, 'SMA50_unscaled']:
            if df.loc[i, 'RSI_unscaled'] < 30:
                # Buy
                atr = float(df.loc[i, 'Volatility'])  # ATR as volatility measure
                risk_per_trade = 0.01  # 1% risk per trade
                position_size = (balance * risk_per_trade) / (atr + 1e-8)  # Avoid division by zero
                shares_to_buy = math.floor(max_position_size := 0.25 * balance / df.loc[i, 'Close_unscaled'])
                if shares_to_buy > 0 and balance >= shares_to_buy * df.loc[i, 'Close_unscaled']:
                    cost = shares_to_buy * df.loc[i, 'Close_unscaled'] * transaction_cost
                    holdings += shares_to_buy
                    balance -= shares_to_buy * df.loc[i, 'Close_unscaled'] + cost
                    logging.debug(f"[Strategy: MA Crossover] Bought {shares_to_buy} shares at {df.loc[i, 'Close_unscaled']:.2f} on step {i}")
        # Death cross with RSI > 70
        elif df.loc[i-1, 'SMA10_unscaled'] > df.loc[i-1, 'SMA50_unscaled'] and df.loc[i, 'SMA10_unscaled'] <= df.loc[i, 'SMA50_unscaled']:
            if df.loc[i, 'RSI_unscaled'] > 70:
                # Sell
                shares_to_sell = math.floor(0.5 * holdings)
                if shares_to_sell > 0:
                    proceeds = shares_to_sell * df.loc[i, 'Close_unscaled']
                    cost = proceeds * transaction_cost
                    holdings -= shares_to_sell
                    balance += proceeds - cost
                    logging.debug(f"[Strategy: MA Crossover] Sold {shares_to_sell} shares at {df.loc[i, 'Close_unscaled']:.2f} on step {i}")
        net_worth = balance + holdings * df.loc[i, 'Close_unscaled']

    profit = net_worth - initial_balance
    return {
        'Strategy': 'Moving Average Crossover with RSI',
        'Initial Balance': initial_balance,
        'Final Net Worth': net_worth,
        'Profit': profit
    }

def macd_strategy(df: pd.DataFrame, initial_balance: float = 100000, transaction_cost: float = 0.001) -> dict:
    """
    Implements a MACD strategy.

    Args:
        df (pd.DataFrame): DataFrame containing stock prices and MACD indicators.
        initial_balance (float): Starting balance.
        transaction_cost (float): Transaction cost per trade.

    Returns:
        dict: Results of the strategy.
    """
    balance = initial_balance
    holdings = 0
    net_worth = initial_balance

    for i in range(1, len(df)):
        # MACD crossover
        if df.loc[i-1, 'MACD_unscaled'] < 0 and df.loc[i, 'MACD_unscaled'] >= 0:
            # Buy
            atr = float(df.loc[i, 'Volatility'])  # ATR as volatility measure
            risk_per_trade = 0.01  # 1% risk per trade
            position_size = (balance * risk_per_trade) / (atr + 1e-8)  # Avoid division by zero
            shares_to_buy = math.floor(max_position_size := 0.25 * balance / df.loc[i, 'Close_unscaled'])
            if shares_to_buy > 0 and balance >= shares_to_buy * df.loc[i, 'Close_unscaled']:
                cost = shares_to_buy * df.loc[i, 'Close_unscaled'] * transaction_cost
                holdings += shares_to_buy
                balance -= shares_to_buy * df.loc[i, 'Close_unscaled'] + cost
                logging.debug(f"[Strategy: MACD Crossover] Bought {shares_to_buy} shares at {df.loc[i, 'Close_unscaled']:.2f} on step {i}")
        elif df.loc[i-1, 'MACD_unscaled'] > 0 and df.loc[i, 'MACD_unscaled'] <= 0:
            # Sell
            shares_to_sell = math.floor(0.5 * holdings)
            if shares_to_sell > 0:
                proceeds = shares_to_sell * df.loc[i, 'Close_unscaled']
                cost = proceeds * transaction_cost
                holdings -= shares_to_sell
                balance += proceeds - cost
                logging.debug(f"[Strategy: MACD Crossover] Sold {shares_to_sell} shares at {df.loc[i, 'Close_unscaled']:.2f} on step {i}")
        net_worth = balance + holdings * df.loc[i, 'Close_unscaled']

    profit = net_worth - initial_balance
    return {
        'Strategy': 'MACD Crossover',
        'Initial Balance': initial_balance,
        'Final Net Worth': net_worth,
        'Profit': profit
    }

def bollinger_bands_strategy(df: pd.DataFrame, initial_balance: float = 100000, transaction_cost: float = 0.001) -> dict:
    """
    Implements a Bollinger Bands strategy.

    Args:
        df (pd.DataFrame): DataFrame containing stock prices and Bollinger Bands.
        initial_balance (float): Starting balance.
        transaction_cost (float): Transaction cost per trade.

    Returns:
        dict: Results of the strategy.
    """
    balance = initial_balance
    holdings = 0
    net_worth = initial_balance

    for i in range(len(df)):
        # Buy when price crosses below lower band
        if df.loc[i, 'Close_unscaled'] < df.loc[i, 'BB_Lower_unscaled']:
            atr = float(df.loc[i, 'Volatility'])  # ATR as volatility measure
            risk_per_trade = 0.01  # 1% risk per trade
            position_size = (balance * risk_per_trade) / (atr + 1e-8)  # Avoid division by zero
            shares_to_buy = math.floor(max_position_size := 0.25 * balance / df.loc[i, 'Close_unscaled'])
            if shares_to_buy > 0 and balance >= shares_to_buy * df.loc[i, 'Close_unscaled']:
                cost = shares_to_buy * df.loc[i, 'Close_unscaled'] * transaction_cost
                holdings += shares_to_buy
                balance -= shares_to_buy * df.loc[i, 'Close_unscaled'] + cost
                logging.debug(f"[Strategy: Bollinger Bands] Bought {shares_to_buy} shares at {df.loc[i, 'Close_unscaled']:.2f} on step {i}")
        # Sell when price crosses above upper band
        elif df.loc[i, 'Close_unscaled'] > df.loc[i, 'BB_Upper_unscaled']:
            shares_to_sell = math.floor(0.5 * holdings)
            if shares_to_sell > 0:
                proceeds = shares_to_sell * df.loc[i, 'Close_unscaled']
                cost = proceeds * transaction_cost
                holdings -= shares_to_sell
                balance += proceeds - cost
                logging.debug(f"[Strategy: Bollinger Bands] Sold {shares_to_sell} shares at {df.loc[i, 'Close_unscaled']:.2f} on step {i}")
        net_worth = balance + holdings * df.loc[i, 'Close_unscaled']

    profit = net_worth - initial_balance
    return {
        'Strategy': 'Bollinger Bands',
        'Initial Balance': initial_balance,
        'Final Net Worth': net_worth,
        'Profit': profit
    }

def random_strategy(df: pd.DataFrame, initial_balance: float = 100000, transaction_cost: float = 0.001) -> dict:
    """
    Implements a Random strategy.

    Args:
        df (pd.DataFrame): DataFrame containing stock prices.
        initial_balance (float): Starting balance.
        transaction_cost (float): Transaction cost per trade.

    Returns:
        dict: Results of the strategy.
    """
    balance = initial_balance
    holdings = 0
    net_worth = initial_balance

    for i in range(len(df)):
        action = random.choice(['Buy', 'Sell', 'Hold'])
        if action == 'Buy':
            atr = float(df.loc[i, 'Volatility'])  # ATR as volatility measure
            risk_per_trade = 0.01  # 1% risk per trade
            position_size = (balance * risk_per_trade) / (atr + 1e-8)  # Avoid division by zero
            shares_to_buy = math.floor(max_position_size := 0.25 * balance / df.loc[i, 'Close_unscaled'])
            if shares_to_buy > 0 and balance >= shares_to_buy * df.loc[i, 'Close_unscaled']:
                cost = shares_to_buy * df.loc[i, 'Close_unscaled'] * transaction_cost
                holdings += shares_to_buy
                balance -= shares_to_buy * df.loc[i, 'Close_unscaled'] + cost
                logging.debug(f"[Strategy: Random] Bought {shares_to_buy} shares at {df.loc[i, 'Close_unscaled']:.2f} on step {i}")
        elif action == 'Sell':
            shares_to_sell = math.floor(0.5 * holdings)
            if shares_to_sell > 0:
                proceeds = shares_to_sell * df.loc[i, 'Close_unscaled']
                cost = proceeds * transaction_cost
                holdings -= shares_to_sell
                balance += proceeds - cost
                logging.debug(f"[Strategy: Random] Sold {shares_to_sell} shares at {df.loc[i, 'Close_unscaled']:.2f} on step {i}")
        # If Hold, do nothing
        net_worth = balance + holdings * df.loc[i, 'Close_unscaled']

    profit = net_worth - initial_balance
    return {
        'Strategy': 'Random Strategy',
        'Initial Balance': initial_balance,
        'Final Net Worth': net_worth,
        'Profit': profit
    }

##############################################
# Evaluation and Plotting Functions
##############################################

def plot_rl_training_history(rl_df: pd.DataFrame):
    """
    Plots the RL agent's net worth and rewards over time.

    Args:
        rl_df (pd.DataFrame): DataFrame containing the RL agent's trading history.
    """
    if rl_df.empty:
        logging.error("RL history is empty. Cannot plot training history.")
        return

    plt.figure(figsize=(14,7))

    # Plot Net Worth
    plt.subplot(2, 1, 1)
    plt.plot(rl_df['Step'], rl_df['Net Worth'], label='Net Worth', color='blue')
    plt.title('RL Agent Net Worth Over Time')
    plt.xlabel('Step')
    plt.ylabel('Net Worth ($)')
    plt.legend()
    plt.grid(True)

    # Plot Reward
    plt.subplot(2, 1, 2)
    plt.plot(rl_df['Step'], rl_df['Reward'], label='Reward', color='green')
    plt.title('RL Agent Reward Over Time')
    plt.xlabel('Step')
    plt.ylabel('Reward')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.savefig(PLOTS_DIR / "rl_training_history.png")
    plt.show()
    logging.info("RL training history plotted successfully.")

def plot_results(df: pd.DataFrame, rl_df: pd.DataFrame, ticker: str):
    """
    Plots trading actions and net worth.

    Args:
        df (pd.DataFrame): Stock data.
        rl_df (pd.DataFrame): RL agent's trading history.
        ticker (str): Stock ticker.
    """
    if rl_df.empty:
        logging.critical("RL history is empty. Skipping plots.")
        return

    # Ensure rl_df and df are aligned
    min_length = min(len(df), len(rl_df))
    aligned_df = df.iloc[:min_length].reset_index(drop=True)
    aligned_rl_df = rl_df.iloc[:min_length].reset_index(drop=True)

    fig, axs = plt.subplots(2, 1, figsize=(14, 12))

    # Plot Price with Buy/Sell Signals using unscaled data
    axs[0].plot(aligned_df['Date'], aligned_df['Close_unscaled'], label='Close Price (Unscaled)', color='blue', alpha=0.6)

    # Plot Buy Signals
    buy_signals = aligned_rl_df[aligned_rl_df['Buy_Signal_Price'].notna()]
    if not buy_signals.empty:
        axs[0].scatter(buy_signals['Date'], buy_signals['Buy_Signal_Price'], color='green', marker='^', s=100, label='Buy Signal')

    # Plot Sell Signals
    sell_signals = aligned_rl_df[aligned_rl_df['Sell_Signal_Price'].notna()]
    if not sell_signals.empty:
        axs[0].scatter(sell_signals['Date'], sell_signals['Sell_Signal_Price'], color='red', marker='v', s=100, label='Sell Signal')

    axs[0].set_title(f'{ticker} Price with Buy/Sell Signals (Unscaled)', fontsize=16)
    axs[0].set_xlabel('Date', fontsize=14)
    axs[0].set_ylabel('Price ($)', fontsize=14)
    axs[0].legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=12)
    axs[0].tick_params(axis='both', which='major', labelsize=12)
    axs[0].grid(True)

    # Plot Net Worth and Drawdown
    axs[1].plot(aligned_rl_df['Date'], aligned_rl_df['Net Worth'], label='Net Worth', color='blue')
    axs[1].set_title(f'{ticker} RL Agent Net Worth Over Time', fontsize=16)
    axs[1].set_xlabel('Date', fontsize=14)
    axs[1].set_ylabel('Net Worth ($)', fontsize=14)
    axs[1].legend(fontsize=12)
    axs[1].tick_params(axis='both', which='major', labelsize=12)
    axs[1].grid(True)

    # Annotate Drawdowns
    axs[1].fill_between(aligned_rl_df['Date'], aligned_rl_df['Net Worth'], aligned_rl_df['Net Worth'].cummax(),
                        color='red', alpha=0.3, label='Drawdown')

    plt.tight_layout()
    plt.savefig(PLOTS_DIR / f"{ticker}_combined_plot.png")
    plt.close()

    logging.critical(f"Combined plots saved in {PLOTS_DIR}")

def plot_agent_performance(rl_df: pd.DataFrame, ticker: str):
    """
    Plots the RL agent's performance metrics over time.
    
    Args:
        rl_df (pd.DataFrame): DataFrame containing the RL agent's trading history.
        ticker (str): Stock ticker.
    """
    if rl_df.empty:
        logging.error("RL history is empty. Cannot plot agent performance.")
        return
    
    plt.figure(figsize=(15, 10))
    
    # Plot Net Worth
    plt.subplot(3, 1, 1)
    plt.plot(rl_df['Step'], rl_df['Net Worth'], label='Net Worth', color='blue')
    plt.title(f'{ticker} RL Agent Net Worth Over Time')
    plt.xlabel('Step')
    plt.ylabel('Net Worth ($)')
    plt.legend()
    plt.grid(True)
    
    # Plot Reward
    plt.subplot(3, 1, 2)
    plt.plot(rl_df['Step'], rl_df['Reward'], label='Reward', color='green')
    plt.title('Reward Over Time')
    plt.xlabel('Step')
    plt.ylabel('Reward')
    plt.legend()
    plt.grid(True)
    
    # Plot Position
    plt.subplot(3, 1, 3)
    plt.plot(rl_df['Step'], rl_df['Position'], label='Position', color='red')
    plt.title('Position Over Time')
    plt.xlabel('Step')
    plt.ylabel('Position (Shares)')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig(PLOTS_DIR / f"{ticker}_agent_performance.png")
    plt.show()
    logging.info("Agent performance plots generated successfully.")

def plot_action_distribution(rl_df: pd.DataFrame):
    """
    Plots the distribution of actions taken by the agent.
    
    Args:
        rl_df (pd.DataFrame): DataFrame containing the RL agent's trading history.
    """
    if rl_df.empty:
        logging.error("RL history is empty. Cannot plot action distribution.")
        return
    
    action_counts = rl_df['Action'].value_counts().sort_index()
    action_labels = ['Hold', 'Buy', 'Sell']
    
    plt.figure(figsize=(8,6))
    plt.bar(action_labels, action_counts, color=['grey', 'green', 'red'])
    plt.title('Action Distribution')
    plt.xlabel('Action')
    plt.ylabel('Count')
    plt.show()
    logging.info("Action distribution plot generated successfully.")

##############################################
# Callbacks
##############################################

class EarlyStoppingCallback(BaseCallback):
    """
    Custom callback for implementing early stopping based on Sharpe and Sortino Ratios.
    Stops training if neither metric improves for a given number of evaluations (patience).
    """
    def __init__(self, monitor_sharpe='train/sharpe_ratio_env', monitor_sortino='train/sortino_ratio_env',
                 patience=5, min_delta=1e-3, verbose=1):
        super(EarlyStoppingCallback, self).__init__(verbose)
        self.monitor_sharpe = monitor_sharpe
        self.monitor_sortino = monitor_sortino
        self.patience = patience
        self.min_delta = min_delta
        self.best_sharpe = -np.inf
        self.best_sortino = -np.inf
        self.wait = 0

    def _on_step(self) -> bool:
        # Retrieve the latest values of the monitored metrics
        current_sharpe = self.logger.name_to_value.get(self.monitor_sharpe, None)
        current_sortino = self.logger.name_to_value.get(self.monitor_sortino, None)

        if current_sharpe is None or current_sortino is None:
            if self.verbose > 0:
                print(f"EarlyStoppingCallback: Metrics '{self.monitor_sharpe}' or '{self.monitor_sortino}' not found.")
            return True  # Continue training

        improved_sharpe = current_sharpe > self.best_sharpe + self.min_delta
        improved_sortino = current_sortino > self.best_sortino + self.min_delta

        if improved_sharpe:
            self.best_sharpe = current_sharpe
            self.wait = 0
            if self.verbose > 0:
                print(f"EarlyStoppingCallback: Sharpe Ratio improved to {self.best_sharpe:.4f}. Resetting wait counter.")
        if improved_sortino:
            self.best_sortino = current_sortino
            self.wait = 0
            if self.verbose > 0:
                print(f"EarlyStoppingCallback: Sortino Ratio improved to {self.best_sortino:.4f}. Resetting wait counter.")

        if not (improved_sharpe or improved_sortino):
            self.wait += 1
            if self.verbose > 0:
                print(f"EarlyStoppingCallback: No improvement in Sharpe or Sortino Ratio. Wait counter: {self.wait}/{self.patience}")
            if self.wait >= self.patience:
                if self.verbose > 0:
                    print("EarlyStoppingCallback: Patience exceeded. Stopping training.")
                return False  # Stop training

        return True  # Continue training

class CustomTensorboardCallback(BaseCallback):
    """
    Custom callback for logging additional metrics to TensorBoard.
    Logs net worth, reward, Sharpe Ratio, Sortino Ratio, Max Drawdown, elapsed time, and estimated remaining time.
    """
    def __init__(self, verbose=0):
        super(CustomTensorboardCallback, self).__init__(verbose)
        self.start_time = None

    def _on_training_start(self) -> None:
        self.start_time = time.time()

    def _on_step(self) -> bool:
        # Access the environment
        env = self.training_env.envs[0]

        # Log net worth and reward if history is not empty
        if env.history:
            last_history = env.history[-1]
            self.logger.record("train/net_worth_env", last_history['Net Worth'])
            self.logger.record("train/balance_env", last_history['Balance'])
            self.logger.record("train/position_env", last_history['Position'])
            self.logger.record("train/reward_env", last_history['Reward'])
            # Calculate drawdown
            peak = max([h['Net Worth'] for h in env.history])
            drawdown = (last_history['Net Worth'] - peak) / peak
            self.logger.record("train/drawdown_env", drawdown)

        # Calculate and log Sharpe Ratio and Sortino Ratio at the end of an episode
        done = self.locals.get('done', False)
        if done:
            returns = np.array([h['Reward'] for h in env.history])
            if returns.std() != 0:
                sharpe_ratio = (returns.mean() / returns.std()) * math.sqrt(env.annual_trading_days)
                self.logger.record("train/sharpe_ratio_env", sharpe_ratio)
            else:
                self.logger.record("train/sharpe_ratio_env", 0.0)

            # Calculate Sortino Ratio
            downside_returns = returns[returns < 0]
            if downside_returns.size > 0:
                sortino_ratio = (returns.mean() / (downside_returns.std() * math.sqrt(env.annual_trading_days) + 1e-8))
            else:
                sortino_ratio = 0.0
            self.logger.record("train/sortino_ratio_env", sortino_ratio)

            # Log Maximum Drawdown
            net_worth_series = pd.Series([h['Net Worth'] for h in env.history])
            max_drawdown = calculate_max_drawdown(net_worth_series)
            self.logger.record("train/max_drawdown_env", max_drawdown)

        # Log elapsed time
        if self.start_time:
            elapsed_time = time.time() - self.start_time  # in seconds
            elapsed_time_formatted = time.strftime("%H:%M:%S", time.gmtime(elapsed_time))
            self.logger.record("train/elapsed_time_env", elapsed_time)
            self.logger.record("train/elapsed_time_formatted_env", elapsed_time_formatted)

        # Estimate remaining time (simple estimation based on current step and total steps)
        total_steps = self.model.num_timesteps
        current_steps = self.model.num_timesteps
        if total_steps > 0:
            steps_remaining = total_steps - current_steps
            if steps_remaining > 0 and elapsed_time > 0:
                estimated_remaining_time = (elapsed_time / current_steps) * steps_remaining
                estimated_remaining_time_formatted = time.strftime("%H:%M:%S", time.gmtime(estimated_remaining_time))
                self.logger.record("train/remaining_time_env", estimated_remaining_time)
                self.logger.record("train/remaining_time_formatted_env", estimated_remaining_time_formatted)

        return True

def calculate_max_drawdown(net_worth_series: pd.Series) -> float:
    """
    Calculates the Maximum Drawdown of a net worth series.

    Args:
        net_worth_series (pd.Series): Series of net worth over time.

    Returns:
        float: Maximum drawdown value.
    """
    rolling_max = net_worth_series.cummax()
    drawdown = (net_worth_series - rolling_max) / rolling_max
    return drawdown.min()

def calculate_annualized_return(net_worth_series: pd.Series, periods_per_year: int = 252) -> float:
    """
    Calculates the Annualized Return (CAGR).

    Args:
        net_worth_series (pd.Series): Series of net worth over time.
        periods_per_year (int): Number of trading periods in a year.

    Returns:
        float: Annualized return.
    """
    start_value = net_worth_series.iloc[0]
    end_value = net_worth_series.iloc[-1]
    num_periods = len(net_worth_series)
    return (end_value / start_value) ** (periods_per_year / num_periods) - 1

def calculate_sortino_ratio(returns: np.ndarray, periods_per_year: int = 252) -> float:
    """
    Calculates the Sortino Ratio.

    Args:
        returns (np.ndarray): Array of returns.
        periods_per_year (int): Number of trading periods in a year.

    Returns:
        float: Sortino Ratio.
    """
    target = 0.0  # Minimum acceptable return
    downside_returns = returns[returns < target]
    expected_return = returns.mean() * periods_per_year
    downside_std = downside_returns.std() * np.sqrt(periods_per_year) if len(downside_returns) > 0 else 0.0
    if downside_std == 0:
        return 0.0
    return expected_return / downside_std

##############################################
# Optuna Hyperparameter Tuning
##############################################

def generate_unique_study_name(base_name='rl_trading_agent_study'):
    """
    Generates a unique study name by appending the current timestamp.

    Args:
        base_name (str): The base name for the study.

    Returns:
        str: A unique study name.
    """
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"{base_name}_{timestamp}"

def objective(trial, df, scaler, initial_balance, stop_loss, take_profit, max_position_size, max_drawdown, annual_trading_days, transaction_cost):
    """
    Objective function for Optuna to maximize Sharpe and Sortino Ratios.

    Args:
        trial (optuna.trial.Trial): Optuna trial object.
        df (pd.DataFrame): Training data.
        scaler (StandardScaler): Scaler fitted on training data.
        initial_balance (float): Starting capital.
        stop_loss (float): Stop loss threshold.
        take_profit (float): Take profit threshold.
        max_position_size (float): Maximum position size as a fraction of net worth.
        max_drawdown (float): Maximum allowable drawdown.
        annual_trading_days (int): Number of trading days in a year.
        transaction_cost (float): Transaction cost per trade.

    Returns:
        float: Combined Sharpe and Sortino Ratio as the objective to maximize.
    """
    # Define hyperparameter search space
    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)
    n_steps = trial.suggest_categorical('n_steps', [128, 256, 512])
    batch_size = trial.suggest_categorical('batch_size', [32, 64])
    gamma = trial.suggest_uniform('gamma', 0.90, 0.99)
    gae_lambda = trial.suggest_uniform('gae_lambda', 0.80, 1.00)
    clip_range = trial.suggest_uniform('clip_range', 0.1, 0.3)
    ent_coef = trial.suggest_loguniform('ent_coef', 1e-5, 1e-3)
    vf_coef = trial.suggest_uniform('vf_coef', 0.1, 0.5)
    max_grad_norm = trial.suggest_uniform('max_grad_norm', 0.5, 1.0)
    net_arch = trial.suggest_categorical('net_arch', ['128_128', '256_256'])

    # Tune reward component weights (not used directly in this objective)
    gamma_profit = trial.suggest_uniform('gamma_profit', 5, 15)
    gamma_sharpe = trial.suggest_uniform('gamma_sharpe', 0.5, 5)
    gamma_sortino = trial.suggest_uniform('gamma_sortino', 0.5, 5)
    gamma_drawdown = trial.suggest_uniform('gamma_drawdown', 1, 5)
    gamma_inactivity = trial.suggest_uniform('gamma_inactivity', 0.1, 1)

    # Map net_arch string to list
    if net_arch == '256_256':
        net_arch_list = [256, 256]
    else:
        net_arch_list = [128, 128]

    # Initialize environment
    vec_env = DummyVecEnv([
        make_env(env_params={
            'df': df,
            'scaler': scaler,
            'initial_balance': initial_balance,
            'stop_loss': stop_loss,
            'take_profit': take_profit,
            'max_position_size': max_position_size,
            'max_drawdown': max_drawdown,
            'annual_trading_days': annual_trading_days,
            'transaction_cost': transaction_cost
        }, env_rank=trial.number, seed=RANDOM_SEED)
    ])

    # Define the policy network architecture
    policy_kwargs = dict(
        activation_fn=torch.nn.ReLU,
        net_arch=net_arch_list
    )

    # Initialize the PPO model on CPU (since we're using MlpPolicy)
    try:
        device = 'cpu'  # Force CPU usage for MlpPolicy
        model = PPO(
            'MlpPolicy',
            vec_env,
            verbose=0,
            seed=RANDOM_SEED,
            policy_kwargs=policy_kwargs,
            learning_rate=learning_rate,
            n_steps=n_steps,
            batch_size=batch_size,
            gamma=gamma,
            gae_lambda=gae_lambda,
            clip_range=clip_range,
            ent_coef=ent_coef,
            vf_coef=vf_coef,
            max_grad_norm=max_grad_norm,
            device=device
        )
    except Exception as e:
        logging.critical(f"[Trial {trial.number}] Model initialization failed: {e}")
        return 0.0

    # Adjust the environment's dynamic drawdown based on trial parameters
    env = vec_env.envs[0]
    env.base_drawdown = max_drawdown  # Ensure base_drawdown remains consistent

    # Train the model (extended training steps for better convergence)
    try:
        model.learn(total_timesteps=50000)
    except Exception as e:
        logging.critical(f"[Trial {trial.number}] Training failed with params {trial.params}: {e}")
        return 0.0

    # Evaluate the trained model
    try:
        obs = vec_env.reset()
    except Exception as e:
        logging.critical(f"[Trial {trial.number}] Reset failed during evaluation: {e}")
        return 0.0

    done = False
    rewards = []
    while not done:
        try:
            action, _ = model.predict(obs, deterministic=True)
            obs, rewards_step, dones, infos = vec_env.step(action)
            rewards.append(rewards_step[0])  # Since only one environment
            done = dones[0]
        except Exception as e:
            logging.critical(f"[Trial {trial.number}] Step failed during evaluation: {e}")
            return 0.0  # Abort the trial if a step fails

    # Calculate Sharpe Ratio and Sortino Ratio
    returns = np.array(rewards)
    if returns.size == 0 or returns.std() == 0:
        sharpe_ratio = 0
        sortino_ratio = 0
    else:
        sharpe_ratio = (returns.mean() / returns.std()) * math.sqrt(annual_trading_days)
        downside_returns = returns[returns < 0]
        sortino_ratio = (returns.mean() / (downside_returns.std() * math.sqrt(annual_trading_days) + 1e-8)) if len(downside_returns) > 0 else 0.0

    # Combine Sharpe and Sortino Ratios for a balanced objective
    combined_ratio = sharpe_ratio + sortino_ratio

    logging.critical(f"[Trial {trial.number}] Completed with Sharpe Ratio: {sharpe_ratio:.4f}, Sortino Ratio: {sortino_ratio:.4f}")

    return combined_ratio

##############################################
# Main Execution
##############################################

def make_env(env_params, env_rank, seed=RANDOM_SEED):
    """
    Creates and returns a callable that initializes the SingleStockTradingEnv.

    Args:
        env_params (dict): Parameters to initialize the environment.
        env_rank (int): Unique identifier for the environment.
        seed (int): Random seed.

    Returns:
        callable: A function that creates and returns a SingleStockTradingEnv instance when called.
    """
    def _init():
        env_instance = SingleStockTradingEnv(
            df=env_params['df'],
            scaler=env_params['scaler'],
            initial_balance=env_params['initial_balance'],
            stop_loss=env_params['stop_loss'],
            take_profit=env_params['take_profit'],
            max_position_size=env_params['max_position_size'],
            max_drawdown=env_params['max_drawdown'],
            annual_trading_days=env_params['annual_trading_days'],
            transaction_cost=env_params['transaction_cost'],
            env_rank=env_rank
        )
        env_instance.seed(seed + env_rank)
        return env_instance
    return _init

if __name__ == "__main__":
    # Define parameters
    TICKER = 'APOLLOTYRE.NS'
    START_DATE = '2018-01-01'
    END_DATE = datetime.datetime.now().strftime('%Y-%m-%d')  # Current date
    INITIAL_BALANCE = 100000
    STOP_LOSS = 0.90
    TAKE_PROFIT = 1.10
    MAX_POSITION_SIZE = 0.25
    MAX_DRAWDOWN = 0.20
    ANNUAL_TRADING_DAYS = 252
    TRANSACTION_COST = 0.001  # 0.1% per trade

    # Fetch and prepare data
    df = get_data(TICKER, START_DATE, END_DATE)
    if df.empty:
        logging.critical("No data fetched. Exiting.")
        exit()

    # Split into training and testing datasets
    split_idx = int(len(df) * 0.8)
    train_df = df.iloc[:split_idx].reset_index(drop=True)
    test_df = df.iloc[split_idx:].reset_index(drop=True)

    logging.info(f"Training data: {len(train_df)} samples")
    logging.info(f"Testing data: {len(test_df)} samples")

    # Fit scaler on training data
    features_to_scale = FEATURES_TO_SCALE
    scaler = StandardScaler()
    train_df[features_to_scale] = scaler.fit_transform(train_df[features_to_scale])

    # Apply the same scaler to testing data
    test_df[features_to_scale] = scaler.transform(test_df[features_to_scale])

    # Save the scaler for future use (e.g., deployment)
    scaler_filename = RESULTS_DIR / 'scaler.pkl'
    joblib.dump(scaler, scaler_filename)
    logging.info(f"Scaler fitted on training data and saved as {scaler_filename}")

    # Check environment validity
    logging.info("Checking environment compatibility with SB3...")
    env_checker = SingleStockTradingEnv(train_df, scaler=scaler, initial_balance=INITIAL_BALANCE,
                                       stop_loss=STOP_LOSS, take_profit=TAKE_PROFIT,
                                       max_position_size=MAX_POSITION_SIZE,
                                       max_drawdown=MAX_DRAWDOWN,
                                       annual_trading_days=ANNUAL_TRADING_DAYS,
                                       transaction_cost=TRANSACTION_COST,
                                       env_rank=-1)  # Assign a default env_rank for the checker
    try:
        check_env(env_checker, warn=True)
        logging.info("Environment is valid!")
    except Exception as e:
        logging.critical(f"Environment check failed: {e}")
        exit()

    # Optuna hyperparameter tuning with SubprocVecEnv for parallel trials
    logging.info("Starting hyperparameter tuning with Optuna...")
    storage = optuna.storages.RDBStorage(
        url='sqlite:///optuna_study.db',
        engine_kwargs={'connect_args': {'check_same_thread': False}}
    )

    # Generate a unique study name
    unique_study_name = generate_unique_study_name()

    study = optuna.create_study(
        direction='maximize',
        sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED),
        storage=storage,
        study_name=unique_study_name,  # Unique Study Name
        load_if_exists=False  # Ensure a new study is created
    )
    study.optimize(
        lambda trial: objective(trial, train_df, scaler, INITIAL_BALANCE, STOP_LOSS, TAKE_PROFIT,
                               MAX_POSITION_SIZE, MAX_DRAWDOWN, ANNUAL_TRADING_DAYS, TRANSACTION_COST),
        n_trials=100,  # Adjust as needed
        n_jobs=-1  # Use all available CPU cores
    )

    if study.best_params:
        best_params = study.best_params
        logging.info(f"Best hyperparameters found: {best_params}")
    else:
        logging.critical("No successful trials found in Optuna study.")
        exit()

    # Train final model with best hyperparameters and enable TensorBoard logging
    if best_params.get('net_arch', '128_128') == '256_256':
        net_arch_list = [256, 256]
    else:
        net_arch_list = [128, 128]

    # Initialize environment
    env_params = {
        'df': train_df,
        'scaler': scaler,
        'initial_balance': INITIAL_BALANCE,
        'stop_loss': STOP_LOSS,
        'take_profit': TAKE_PROFIT,
        'max_position_size': MAX_POSITION_SIZE,
        'max_drawdown': MAX_DRAWDOWN,
        'annual_trading_days': ANNUAL_TRADING_DAYS,
        'transaction_cost': TRANSACTION_COST
    }

    # Determine the number of CPU cores available
    num_cpu = multiprocessing.cpu_count()
    logging.info(f"Number of CPU cores available: {num_cpu}")

    # Create a list of environment initializers with unique ranks
    env_list = [make_env(env_params, i, RANDOM_SEED) for i in range(num_cpu)]

    # Initialize SubprocVecEnv with the list of callable environments
    vec_env = SubprocVecEnv(env_list)

    logging.info(f"Initialized SubprocVecEnv with {num_cpu} parallel environments.")

    # Define policy kwargs
    policy_kwargs = dict(
        activation_fn=torch.nn.ReLU,
        net_arch=net_arch_list
    )

    # Initialize PPO model with best hyperparameters and enable CPU usage
    try:
        device = 'cpu'  # Force CPU usage for MlpPolicy
        model = PPO(
            'MlpPolicy',
            vec_env,
            verbose=1,  # Set verbose to 1 to enable logging
            seed=RANDOM_SEED,
            policy_kwargs=policy_kwargs,
            learning_rate=best_params.get('learning_rate', 3e-4),
            n_steps=best_params.get('n_steps', 128),
            batch_size=best_params.get('batch_size', 64),
            gamma=best_params.get('gamma', 0.99),
            gae_lambda=best_params.get('gae_lambda', 0.95),
            clip_range=best_params.get('clip_range', 0.2),
            ent_coef=best_params.get('ent_coef', 0.01),  # Increased entropy coefficient
            vf_coef=best_params.get('vf_coef', 0.5),
            max_grad_norm=best_params.get('max_grad_norm', 0.5),
            tensorboard_log=str(TB_LOG_DIR),
            device=device  # Use CPU
        )
    except Exception as e:
        logging.critical(f"Model initialization failed: {e}")
        exit()

    # Define checkpoint and custom callbacks
    checkpoint_callback = CheckpointCallback(
        save_freq=10000,  # Save model every 10,000 steps
        save_path=str(RESULTS_DIR / "checkpoints"),
        name_prefix="ppo_model"
    )
    custom_callback = CustomTensorboardCallback()
    early_stopping_callback = EarlyStoppingCallback(
        monitor_sharpe='train/sharpe_ratio_env',
        monitor_sortino='train/sortino_ratio_env',
        patience=5,  # Number of evaluations with no improvement
        min_delta=1e-3,  # Minimum improvement to reset patience
        verbose=1
    )

    # Create a CallbackList
    callback_list = CallbackList([custom_callback, checkpoint_callback, early_stopping_callback])

    # Train the PPO agent with all callbacks
    logging.info("Starting training of PPO agent with Early Stopping...")
    try:
        model.learn(
            total_timesteps=50000,  # Extended training steps for better convergence
            callback=callback_list
        )
    except Exception as e:
        logging.critical(f"Training failed: {e}")
        exit()
    model_path = RESULTS_DIR / f"ppo_model_{TICKER}.zip"
    model.save(str(model_path))
    logging.info(f"Model trained and saved at {model_path}")

    # Test the trained model
    logging.info("Starting testing of PPO agent...")
    test_env = SingleStockTradingEnv(test_df, scaler=scaler, initial_balance=INITIAL_BALANCE,
                                     stop_loss=STOP_LOSS, take_profit=TAKE_PROFIT,
                                     max_position_size=MAX_POSITION_SIZE,
                                     max_drawdown=MAX_DRAWDOWN,
                                     annual_trading_days=ANNUAL_TRADING_DAYS,
                                     transaction_cost=TRANSACTION_COST,
                                     env_rank=999)  # Assign a unique env_rank for testing
    test_vec_env = DummyVecEnv([make_env(env_params={
        'df': test_df,
        'scaler': scaler,
        'initial_balance': INITIAL_BALANCE,
        'stop_loss': STOP_LOSS,
        'take_profit': TAKE_PROFIT,
        'max_position_size': MAX_POSITION_SIZE,
        'max_drawdown': MAX_DRAWDOWN,
        'annual_trading_days': ANNUAL_TRADING_DAYS,
        'transaction_cost': TRANSACTION_COST
    }, env_rank=999, seed=RANDOM_SEED)])

    try:
        obs = test_vec_env.reset()
    except Exception as e:
        logging.critical(f"Reset failed during testing: {e}")
        exit()

    done = False
    while not done:
        try:
            action, _ = model.predict(obs, deterministic=True)
            obs, rewards_step, dones, infos = test_vec_env.step(action)
            done = dones[0]
            logging.debug(f"[Test Env {test_env.env_rank}] Step: Action Taken = {action}, Reward = {rewards_step[0]}")
        except Exception as e:
            logging.critical(f"Step failed during testing: {e}")
            break

    # Create DataFrame for RL Agent Performance from environment history
    rl_df = pd.DataFrame(test_env.history)

    # Ensure 'Reward' column exists
    if 'Reward' not in rl_df.columns:
        rl_df['Reward'] = 0.0
        logging.critical("'Reward' column not found in RL history. Defaulting rewards to 0.")

    # Calculate Performance Metrics
    returns = np.array(rl_df['Reward'])
    if returns.size == 0 or returns.std() == 0:
        rl_sharpe_ratio = 0
        rl_sortino_ratio = 0
    else:
        rl_sharpe_ratio = (returns.mean() / returns.std()) * math.sqrt(ANNUAL_TRADING_DAYS)
        downside_returns = returns[returns < 0]
        rl_sortino_ratio = (returns.mean() / (downside_returns.std() * math.sqrt(ANNUAL_TRADING_DAYS) + 1e-8)) if len(downside_returns) > 0 else 0.0

    if not rl_df.empty:
        rl_final_net_worth = rl_df['Net Worth'].iloc[-1]
        rl_profit = rl_final_net_worth - INITIAL_BALANCE
        rl_max_dd = calculate_max_drawdown(rl_df['Net Worth'])
        rl_annualized_return = calculate_annualized_return(rl_df['Net Worth'])
    else:
        rl_final_net_worth = INITIAL_BALANCE
        rl_profit = 0
        rl_max_dd = 0
        rl_annualized_return = 0

    # Evaluate Baseline Strategies on Test Data
    logging.info("Evaluating Baseline Strategies on Test Data...")
    bh_result = buy_and_hold(test_df, initial_balance=INITIAL_BALANCE, transaction_cost=TRANSACTION_COST)
    macd_result = macd_strategy(test_df, initial_balance=INITIAL_BALANCE, transaction_cost=TRANSACTION_COST)
    ma_crossover_result = moving_average_crossover(test_df, initial_balance=INITIAL_BALANCE, transaction_cost=TRANSACTION_COST)
    bb_result = bollinger_bands_strategy(test_df, initial_balance=INITIAL_BALANCE, transaction_cost=TRANSACTION_COST)
    random_result = random_strategy(test_df, initial_balance=INITIAL_BALANCE, transaction_cost=TRANSACTION_COST)

    # Log Baseline Results
    for result in [bh_result, macd_result, ma_crossover_result, bb_result, random_result]:
        logging.critical(f"Strategy: {result['Strategy']}")
        logging.critical(f"  Initial Balance: ${result['Initial Balance']}")
        logging.critical(f"  Final Net Worth: ${result['Final Net Worth']:.2f}")
        logging.critical(f"  Profit: ${result['Profit']:.2f}")
        if 'Invested Capital' in result:
            logging.critical(f"  Invested Capital: ${result['Invested Capital']:.2f}")
            logging.critical(f"  Transaction Costs: ${result['Transaction Costs']:.2f}")
        logging.critical("-" * 50)

    # Log and print RL Agent Results
    logging.critical("RL Agent Performance:")
    logging.critical(f"  Final Net Worth: ${rl_final_net_worth:.2f}")
    logging.critical(f"  Profit: ${rl_profit:.2f}")
    logging.critical(f"  Sharpe Ratio: {rl_sharpe_ratio:.4f}")
    logging.critical(f"  Sortino Ratio: {rl_sortino_ratio:.4f}")
    logging.critical(f"  Annualized Return: {rl_annualized_return*100:.2f}%")
    logging.critical(f"  Max Drawdown: {rl_max_dd:.2f}")

    # Plot RL Training History
    plot_rl_training_history(rl_df)

    # Plot Trading Results
    plot_results(test_df, rl_df, TICKER)

    # Plot Agent's Performance Metrics
    plot_agent_performance(rl_df, TICKER)

    # Plot Action Distribution
    plot_action_distribution(rl_df)

    # Instructions for TensorBoard
    logging.critical("Training logs are stored for TensorBoard.")
    logging.critical("To view them, run the following command in your terminal:")
    logging.critical(f"tensorboard --logdir {TB_LOG_DIR}")
    logging.critical("Then open http://localhost:6006 in your browser to visualize the training metrics.")

    # Optional: Visualize Optuna study results
    try:
        import optuna.visualization as vis

        # Plot optimization history
        fig1 = vis.plot_optimization_history(study)
        fig1.savefig(PLOTS_DIR / "optuna_optimization_history.png")
        fig1.show()

        # Plot parameter importances
        fig2 = vis.plot_param_importances(study)
        fig2.savefig(PLOTS_DIR / "optuna_param_importances.png")
        fig2.show()
    except ImportError:
        logging.warning("Optuna visualization module not found. Install it via pip if you wish to visualize study results.")

    logging.info("Script execution completed successfully.")
