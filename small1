import os
import gymnasium as gym
from gymnasium import spaces
import numpy as np
import pandas as pd
import yfinance as yf
from ta import trend, momentum, volatility, volume
import matplotlib.pyplot as plt
import seaborn as sns
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.env_checker import check_env
from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback, CallbackList
import torch
import warnings
from typing import Optional, Tuple
import random
import datetime
from sklearn.preprocessing import StandardScaler
import math
import logging
from pathlib import Path
import optuna
import joblib
import time

# Import ConcurrentRotatingFileHandler for robust multi-process logging
try:
    from concurrent_log_handler import ConcurrentRotatingFileHandler
except ImportError:
    raise ImportError("Please install 'concurrent-log-handler' package via pip: pip install concurrent-log-handler")

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

# Set random seeds for reproducibility
RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)

# Define feature sets as constants
FEATURES_TO_SCALE = [
    'Close', 'SMA10', 'SMA50', 'RSI', 'MACD', 'ADX',  # Added 'ADX'
    'BB_Upper', 'BB_Lower', 'Bollinger_Width',
    'EMA20', 'VWAP', 'Lagged_Return', 'Volatility'
]

UNSCALED_FEATURES = [
    f"{feature}_unscaled" for feature in FEATURES_TO_SCALE
]

# Define directories for results and plots
BASE_DIR = Path('.').resolve()
RESULTS_DIR = BASE_DIR / 'results'
PLOTS_DIR = BASE_DIR / 'plots'
TB_LOG_DIR = BASE_DIR / 'tensorboard_logs'
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
PLOTS_DIR.mkdir(parents=True, exist_ok=True)
TB_LOG_DIR.mkdir(parents=True, exist_ok=True)

# Function to set up separate loggers
def setup_logger(name: str, log_file: Path, level=logging.INFO) -> logging.Logger:
    """
    Sets up a logger with the specified name and log file.

    Args:
        name (str): Name of the logger.
        log_file (Path): Path to the log file.
        level (int, optional): Logging level. Defaults to logging.INFO.

    Returns:
        logging.Logger: Configured logger.
    """
    logger = logging.getLogger(name)
    logger.setLevel(level)
    # Prevent adding multiple handlers to the logger
    if not logger.handlers:
        handler = ConcurrentRotatingFileHandler(str(log_file), maxBytes=10**6, backupCount=5, encoding='utf-8')
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    return logger

# Initialize separate loggers
main_logger = setup_logger('main_logger', RESULTS_DIR / 'main.log', level=logging.DEBUG)
training_logger = setup_logger('training_logger', RESULTS_DIR / 'training.log', level=logging.DEBUG)
testing_logger = setup_logger('testing_logger', RESULTS_DIR / 'testing.log', level=logging.DEBUG)
phase_logger = setup_logger('phase_logger', RESULTS_DIR / 'phase.log', level=logging.INFO)

# Log the absolute paths
main_logger.info(f"Base Directory: {BASE_DIR}")
main_logger.info(f"Results Directory: {RESULTS_DIR}")
main_logger.info(f"Plots Directory: {PLOTS_DIR}")
main_logger.info(f"TensorBoard Logs Directory: {TB_LOG_DIR}")

# Function to log phase indicators
def log_phase(phase: str, status: str = "Starting", env_details: dict = None, duration: float = None):
    """
    Logs the current phase of the program.

    Args:
        phase (str): The name of the phase (e.g., 'Hyperparameter Tuning').
        status (str, optional): The status of the phase (e.g., 'Starting', 'Completed'). Defaults to "Starting".
        env_details (dict, optional): Key-value pairs describing environment details.
        duration (float, optional): Duration of the phase in seconds.
    """
    log_message = f"***** {status} {phase} *****"
    if env_details:
        log_message += f"\nEnvironment Details: {env_details}"
    if duration is not None:
        log_message += f"\nDuration: {duration:.2f} seconds ({duration/60:.2f} minutes)"
    phase_logger.info(log_message)

# Configure Logging for Main Logger
main_logger.info("Logging has been configured with separate loggers for main, training, testing, and phases.")

##############################################
# Version Checks
##############################################

def check_versions():
    """
    Checks and logs the versions of key libraries to ensure compatibility.
    """
    import stable_baselines3
    import gymnasium
    import optuna

    sb3_version = stable_baselines3.__version__
    gymnasium_version = gymnasium.__version__
    optuna_version = optuna.__version__

    main_logger.debug(f"Stable Baselines3 version: {sb3_version}")
    main_logger.debug(f"Gymnasium version: {gymnasium_version}")
    main_logger.debug(f"Optuna version: {optuna_version}")

    # Ensure SB3 is at least version 2.0.0 for Gymnasium support
    try:
        sb3_major, sb3_minor, sb3_patch = map(int, sb3_version.split('.')[:3])
        if sb3_major < 2:
            main_logger.error("Stable Baselines3 version must be at least 2.0.0. Please upgrade SB3.")
            exit()
    except:
        main_logger.error("Unable to parse Stable Baselines3 version. Please ensure it's installed correctly.")
        exit()

    # Ensure Gymnasium is updated
    if gymnasium_version < '0.28.1':  # Example minimum version
        main_logger.warning("Consider upgrading Gymnasium to the latest version for better compatibility.")

check_versions()

##############################################
# Fetch and Prepare Data
##############################################

def get_data(ticker: str, start_date: str, end_date: str, scaler: Optional[StandardScaler] = None, fit_scaler: bool = False) -> Tuple[pd.DataFrame, Optional[StandardScaler]]:
    """
    Fetches historical stock data from Yahoo Finance, calculates technical indicators,
    and performs scaling on the features.

    Args:
        ticker (str): Stock ticker symbol.
        start_date (str): Start date in 'YYYY-MM-DD' format.
        end_date (str): End date in 'YYYY-MM-DD' format.
        scaler (Optional[StandardScaler], optional): Scaler object. Defaults to None.
        fit_scaler (bool, optional): Whether to fit the scaler on the data. Defaults to False.

    Returns:
        Tuple[pd.DataFrame, Optional[StandardScaler]]: Processed DataFrame with technical indicators and scaled features, and the scaler.
    """
    main_logger.info(f"Fetching data for {ticker} from {start_date} to {end_date}")
    
    # Initial Cleanup: Fetch simple data
    df = yf.download(ticker, start=start_date, end=end_date, progress=False)
    if df.empty:
        main_logger.error(f"No data fetched for {ticker}")
        # Save empty DataFrame to CSV
        empty_file = RESULTS_DIR / f"{ticker}_data_fetched.csv"
        df.to_csv(empty_file, index=True)
        main_logger.info(f"Empty fetched data saved to {empty_file}")
        return df, scaler

    # Check if columns are MultiIndex and flatten them
    if isinstance(df.columns, pd.MultiIndex):
        if df.columns.nlevels == 2 and df.columns.get_level_values(1).unique().size == 1:
            # Flatten columns by taking the first level if second level has only one unique value
            df.columns = df.columns.get_level_values(0)
            main_logger.debug("Flattened MultiIndex columns to single level.")
        else:
            main_logger.error("DataFrame has MultiIndex columns with multiple tickers. Please adjust the data fetching.")
            # Save DataFrame with MultiIndex columns
            multiindex_file = RESULTS_DIR / f"{ticker}_data_multiindex_columns.csv"
            df.to_csv(multiindex_file, index=True)
            main_logger.info(f"Data with MultiIndex columns saved to {multiindex_file}")
            return pd.DataFrame(), scaler

    # Ensure the Date index is converted to a column and parse dates
    df.reset_index(inplace=True)
    df['Date'] = pd.to_datetime(df['Date'])
    # Select specific columns and ensure they are of the correct type
    try:
        df = df[['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']]
    except KeyError as e:
        main_logger.error(f"Missing required columns in fetched data: {e}")
        error_file = RESULTS_DIR / f"{ticker}_data_error_missing_columns.csv"
        df.to_csv(error_file, index=True)
        main_logger.info(f"Data with missing columns saved to {error_file}")
        return pd.DataFrame(), scaler

    # Save fetched data immediately after fetching
    fetched_data_file = RESULTS_DIR / f"{ticker}_data_fetched.csv"
    df.to_csv(fetched_data_file, index=True)
    main_logger.info(f"Fetched data saved to {fetched_data_file}")

    # Convert columns to numeric, coercing errors
    numeric_cols = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']
    df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')

    # Select relevant columns
    required_columns = ['Close', 'High', 'Low', 'Volume']
    for col in required_columns:
        if col not in df.columns:
            main_logger.error(f"{col} not in downloaded data for {ticker}.")
            # Save error state DataFrame
            error_file = RESULTS_DIR / f"{ticker}_data_error_missing_columns.csv"
            df.to_csv(error_file, index=True)
            main_logger.info(f"Data with missing columns saved to {error_file}")
            return pd.DataFrame(), scaler

    if len(df) < 200:
        main_logger.error(f"Not enough data points for {ticker}.")
        # Save insufficient data DataFrame
        insufficient_file = RESULTS_DIR / f"{ticker}_data_insufficient.csv"
        df.to_csv(insufficient_file, index=True)
        main_logger.info(f"Insufficient data saved to {insufficient_file}")
        return pd.DataFrame(), scaler

    close_col = 'Close'
    try:
        close = df[close_col].squeeze()
        high = df['High'].squeeze()
        low = df['Low'].squeeze()
        volume_col = df['Volume'].squeeze()

        # Calculate technical indicators
        sma10 = trend.SMAIndicator(close=close, window=10).sma_indicator()
        sma50 = trend.SMAIndicator(close=close, window=50).sma_indicator()
        rsi = momentum.RSIIndicator(close=close, window=14).rsi()
        macd = trend.MACD(close=close).macd()
        adx = trend.ADXIndicator(high=high, low=low, close=close, window=14).adx()
        bollinger = volatility.BollingerBands(close=close, window=20, window_dev=2)
        bb_upper = bollinger.bollinger_hband()
        bb_lower = bollinger.bollinger_lband()
        bollinger_width = bollinger.bollinger_wband()
        ema20 = trend.EMAIndicator(close=close, window=20).ema_indicator()
        vwap = volume.VolumeWeightedAveragePrice(high=high, low=low, close=close, volume=volume_col, window=14).volume_weighted_average_price()
        lagged_return = close.pct_change().fillna(0)
        atr = volatility.AverageTrueRange(high=high, low=low, close=close, window=14).average_true_range()

        # Verify that all indicators were calculated successfully
        indicators = {
            'SMA10': sma10,
            'SMA50': sma50,
            'RSI': rsi,
            'MACD': macd,
            'ADX': adx,
            'BB_Upper': bb_upper,
            'BB_Lower': bb_lower,
            'Bollinger_Width': bollinger_width,
            'EMA20': ema20,
            'VWAP': vwap,
            'Lagged_Return': lagged_return,
            'Volatility': atr
        }

        for key, value in indicators.items():
            if value.isnull().all():
                main_logger.error(f"Technical indicator {key} could not be calculated properly.")
                # Save DataFrame with failed indicator
                failed_indicator_file = RESULTS_DIR / f"{ticker}_data_failed_indicator_{key}.csv"
                df.to_csv(failed_indicator_file, index=True)
                main_logger.info(f"Data with failed indicator {key} saved to {failed_indicator_file}")
                return pd.DataFrame(), scaler

    except Exception as e:
        main_logger.error(f"Error calculating indicators for {ticker}: {e}")
        # Save DataFrame with error
        error_calculation_file = RESULTS_DIR / f"{ticker}_data_error_calculation.csv"
        df.to_csv(error_calculation_file, index=True)
        main_logger.info(f"Data with calculation error saved to {error_calculation_file}")
        return pd.DataFrame(), scaler

    # Append indicators to DataFrame
    for key, value in indicators.items():
        df[key] = value

    # Save DataFrame after adding indicators (before scaling)
    df_before_scaling_file = RESULTS_DIR / f"{ticker}_data_before_scaling.csv"
    df.to_csv(df_before_scaling_file, index=True)
    main_logger.info(f"Data with indicators saved before scaling to {df_before_scaling_file}")

    # Add unscaled versions of relevant features
    for feature in FEATURES_TO_SCALE:
        if feature in df.columns:
            df[f"{feature}_unscaled"] = df[feature].copy()  # Use copy to ensure separation
            main_logger.debug(f"Added column: {feature}_unscaled")
        else:
            main_logger.error(f"Feature {feature} is missing from DataFrame. Cannot create {feature}_unscaled.")
            # Save DataFrame with missing feature
            missing_feature_file = RESULTS_DIR / f"{ticker}_data_missing_feature_{feature}.csv"
            df.to_csv(missing_feature_file, index=True)
            main_logger.info(f"Data with missing feature {feature} saved to {missing_feature_file}")
            return pd.DataFrame(), scaler

    # Save DataFrame after adding unscaled columns
    df_after_unscaled_file = RESULTS_DIR / f"{ticker}_data_after_unscaled.csv"
    df.to_csv(df_after_unscaled_file, index=True)
    main_logger.info(f"Data with unscaled features saved to {df_after_unscaled_file}")

    # Handle missing values
    df.fillna(method='ffill', inplace=True)
    df.fillna(0, inplace=True)
    df.reset_index(inplace=True)

    # Data Validation: Check for columns filled with zeros
    zero_filled_columns = df[required_columns].columns[(df[required_columns] == 0).all()].tolist()
    if zero_filled_columns:
        main_logger.error(f"One or more required columns are entirely filled with zeros: {zero_filled_columns}. Aborting data processing.")
        # Save DataFrame with zero-filled columns
        zero_filled_file = RESULTS_DIR / f"{ticker}_data_zero_filled_columns.csv"
        df.to_csv(zero_filled_file, index=True)
        main_logger.info(f"Data with zero-filled columns saved to {zero_filled_file}")
        return pd.DataFrame(), scaler

    main_logger.info(f"Data for {ticker} fetched and processed successfully.")

    # Scaling Features
    if fit_scaler:
        scaler = StandardScaler()
        df[FEATURES_TO_SCALE] = scaler.fit_transform(df[FEATURES_TO_SCALE])
        main_logger.debug("Features scaled and scaler fitted.")
    elif scaler is not None:
        df[FEATURES_TO_SCALE] = scaler.transform(df[FEATURES_TO_SCALE])
        main_logger.debug("Features scaled using existing scaler.")
    else:
        main_logger.error("Scaler not provided for scaling. Returning unscaled data.")
        # Save unscaled DataFrame
        unscaled_file = RESULTS_DIR / f"{ticker}_data_unscaled_final.csv"
        df.to_csv(unscaled_file, index=True)
        main_logger.info(f"Unscaled data saved to {unscaled_file}")
        return df, scaler

    # Save DataFrame after scaling
    df_scaled_file = RESULTS_DIR / f"{ticker}_data_scaled.csv"
    df.to_csv(df_scaled_file, index=True)
    main_logger.info(f"Scaled data saved to {df_scaled_file}")

    return df, scaler

##############################################
# Custom Trading Environment
##############################################

class SingleStockTradingEnv(gym.Env):
    """
    A custom Gym environment for single stock trading with continuous action space.
    """
    metadata = {'render.modes': ['human']}

    def __init__(self, df: pd.DataFrame, scaler: StandardScaler,
                 initial_balance: float = 100000,
                 stop_loss: float = 0.90, take_profit: float = 1.10,
                 max_position_size: float = 0.5, max_drawdown: float = 0.20,
                 annual_trading_days: int = 252, transaction_cost: float = 0.001,
                 env_rank: int = 0,
                 reward_weights: Optional[dict] = None):
        super(SingleStockTradingEnv, self).__init__()

        self.env_rank = env_rank  # Unique identifier for the environment

        self.df = df.copy().reset_index(drop=True)
        self.scaler = scaler
        self.initial_balance = initial_balance
        self.current_step = 0
        self.stop_loss = stop_loss
        self.take_profit = take_profit
        self.max_position_size = max_position_size
        self.max_drawdown = max_drawdown
        self.annual_trading_days = annual_trading_days
        self.transaction_cost = transaction_cost  # 0.1% per trade

        # Action space: Continuous actions between -1 and 1
        # Negative values: Sell proportion of holdings
        # Positive values: Buy proportion of available balance
        # Zero: Hold
        self.action_space = spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32)

        # Observation space: features + balance, net worth, position + market phase
        self.num_features = len(FEATURES_TO_SCALE)
        self.market_phase = ['Bull', 'Bear', 'Sideways']
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf,
            shape=(self.num_features + 3 + len(self.market_phase),),
            dtype=np.float32
        )

        self.feature_names = FEATURES_TO_SCALE

        # Initialize environment state
        self.reset()

        # Initialize reward weights
        if reward_weights is not None:
            self.reward_weights = reward_weights
        else:
            self.reward_weights = {'reward_scale': 1.0}  # Renamed to prevent confusion

        training_logger.debug(f"[Env {self.env_rank}] Initialized with reward_weights: {self.reward_weights}")

    def seed(self, seed=None):
        """
        Sets the seed for the environment's random number generators.

        Args:
            seed (int, optional): Seed value. Defaults to None.
        """
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        training_logger.debug(f"[Env {self.env_rank}] Seed set to {seed}")

    def _next_observation(self) -> np.ndarray:
        """
        Returns the next observation.
        """
        if self.current_step >= len(self.df):
            self.current_step = len(self.df) - 1
        current_data = self.df.iloc[self.current_step]
        features = current_data[self.feature_names].values
        obs = list(features)

        # Append balance, net worth, and position
        obs.append(self.balance / self.initial_balance)
        obs.append(self.net_worth / self.initial_balance)
        obs.append(self.position / self.initial_balance)

        # Determine market phase
        try:
            adx = float(current_data['ADX_unscaled'])  # Ensure this matches the DataFrame's feature name
        except KeyError:
            training_logger.error(f"[Env {self.env_rank}] 'ADX_unscaled' not found in current_data at step {self.current_step}. Setting ADX to 0.")
            adx = 0.0

        if adx > 25:
            try:
                sma10 = float(current_data['SMA10_unscaled'])
                sma50 = float(current_data['SMA50_unscaled'])
                if sma10 > sma50:
                    phase = 'Bull'
                else:
                    phase = 'Bear'
            except KeyError as e:
                training_logger.error(f"[Env {self.env_rank}] Missing SMA columns: {e}. Setting phase to 'Sideways'.")
                phase = 'Sideways'
        else:
            phase = 'Sideways'

        # One-hot encode market phase
        for p in self.market_phase:
            obs.append(1.0 if phase == p else 0.0)

        obs = np.array(obs, dtype=np.float32)
        if np.isnan(obs).any() or np.isinf(obs).any():
            obs = np.nan_to_num(obs, nan=0.0, posinf=0.0, neginf=0.0)

        # Sanity check
        assert obs.shape[0] == self.observation_space.shape[0], "Observation shape mismatch!"
        assert not np.isnan(obs).any(), "Observation contains NaN!"

        return obs

    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):
        """
        Resets the state of the environment to an initial state.

        Returns:
            Tuple[np.ndarray, dict]: The initial observation and an empty info dictionary.
        """
        super().reset(seed=seed)
        self.balance = self.initial_balance
        self.position = 0
        self.net_worth = self.initial_balance
        self.current_step = 0
        self.history = []
        self.prev_net_worth = self.net_worth
        self.last_action = 0.0  # Initialize last_action as Hold
        self.peak = self.net_worth
        self.returns_window = []
        self.transaction_count = 0  # Initialize transaction count
        training_logger.debug(f"[Env {self.env_rank}] Environment reset.")
        return self._next_observation(), {}

    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, dict]:
        """
        Executes one time step within the environment based on the continuous action.

        Args:
            action (np.ndarray): Action to take (array with single float between -1 and 1)

        Returns:
            Tuple containing:
            - obs (np.ndarray): Next observation.
            - reward (float): Reward obtained.
            - terminated (bool): Whether the episode has terminated.
            - truncated (bool): Whether the episode was truncated.
            - info (dict): Additional information.
        """
        training_logger.debug(f"[Env {self.env_rank}] step() called at current_step={self.current_step} with action={action}")
        try:
            # Validate action
            action_value = float(action[0])
            assert self.action_space.contains(action), f"[Env {self.env_rank}] Invalid action: {action}"
        except Exception as e:
            training_logger.error(f"[Env {self.env_rank}] Action validation failed: {e}")
            return self._next_observation(), -1000.0, True, False, {}

        # Fetch current data
        if self.current_step >= len(self.df):
            training_logger.debug(f"[Env {self.env_rank}] Current step {self.current_step} exceeds data length. Terminating episode.")
            terminated = True
            truncated = False
            reward = -1000  # Penalty for exceeding data
            obs = self._next_observation()

            # Append to history even if terminated
            self.history.append({
                'Date': self.df.iloc[self.current_step]['Date'],
                'Action': np.nan,
                'Buy_Signal_Price': np.nan,
                'Sell_Signal_Price': np.nan,
                'Net Worth': self.net_worth,
                'Balance': self.balance,
                'Position': self.position,
                'Reward': reward
            })
            training_logger.debug(f"[Env {self.env_rank}] History appended with termination data at step {self.current_step}")

            training_logger.error(f"[Env {self.env_rank}] Terminating episode at step {self.current_step} due to data overflow.")
            return obs, reward, terminated, truncated, {}

        current_data = self.df.iloc[self.current_step]
        current_price = float(current_data['Close_unscaled'])
        current_date = current_data['Date']

        # Interpret action
        if action_value > 0:
            # Buy proportion of available balance
            investment_amount = self.balance * action_value * self.max_position_size
            shares_to_buy = math.floor(investment_amount / current_price)

            # If calculated shares_to_buy is zero but you can afford at least one share, buy one
            if shares_to_buy == 0:
                one_share_cost = current_price * (1 + self.transaction_cost)
                if one_share_cost <= self.balance:
                    shares_to_buy = 1

            # Always Check Affordability
            total_cost = shares_to_buy * current_price * (1 + self.transaction_cost)
            if shares_to_buy > 0 and total_cost <= self.balance:
                self.balance -= total_cost
                self.position += shares_to_buy
                self.transaction_count += 1
                training_logger.debug(f"[Env {self.env_rank}] Step {self.current_step}: Bought {shares_to_buy} shares at {current_price:.2f}")
            else:
                # Either can't afford or shares_to_buy is 0; no trade occurs
                training_logger.debug(f"[Env {self.env_rank}] Step {self.current_step}: Not enough balance to buy shares or no shares calculated.")

        elif action_value < 0:
            # Sell proportion of current holdings
            proportion_to_sell = abs(action_value) * self.max_position_size
            shares_to_sell = math.floor(self.position * proportion_to_sell)

            # If shares_to_sell ended up as 0 but you have shares, try selling at least 1 share
            if shares_to_sell == 0 and self.position > 0:
                shares_to_sell = 1

            # Always Check Affordability
            if shares_to_sell > 0 and shares_to_sell <= self.position:
                proceeds = shares_to_sell * current_price * (1 - self.transaction_cost)
                self.position -= shares_to_sell
                self.balance += proceeds
                training_logger.debug(f"[Env {self.env_rank}] Step {self.current_step}: Sold {shares_to_sell} shares at {current_price:.2f}")
            else:
                # Either can't sell or shares_to_sell is 0; no trade occurs
                training_logger.debug(f"[Env {self.env_rank}] Step {self.current_step}: Not enough shares to sell or shares_to_sell=0.")

        else:
            # Hold action; no operation
            training_logger.debug(f"[Env {self.env_rank}] Step {self.current_step}: Hold action received.")

        # Update net worth
        net_worth = self.balance + self.position * current_price
        net_worth = float(net_worth)  # Ensure net_worth is a float

        # Calculate Reward based on net worth change
        net_worth_change = net_worth - self.prev_net_worth
        profit_reward = net_worth_change / self.initial_balance

        # Update returns window
        self.returns_window.append(profit_reward)
        if len(self.returns_window) > 30:
            self.returns_window.pop(0)

        # Penalty for holding
        if action_value == 0:
            hold_penalty = -0.001  # Small penalty for holding
        else:
            hold_penalty = 0.0

        # Penalty for high drawdown
        self.peak = max(self.peak, net_worth)
        current_drawdown = (self.peak - net_worth) / self.peak if self.peak > 0 else 0.0
        if current_drawdown > self.max_drawdown:
            drawdown_penalty = -0.01  # Significant penalty
        else:
            drawdown_penalty = 0.0

        # Total Reward
        reward = (profit_reward + hold_penalty + drawdown_penalty) * (self.reward_weights['reward_scale'] if self.reward_weights else 1.0)

        # Normalize the reward
        normalized_reward = reward

        # Append to history with updated reward and date
        self.history.append({
            'Date': current_date,
            'Action': action_value,
            'Buy_Signal_Price': current_price if action_value > 0 else np.nan,
            'Sell_Signal_Price': current_price if action_value < 0 else np.nan,
            'Net Worth': net_worth,
            'Balance': self.balance,
            'Position': self.position,
            'Reward': normalized_reward
        })
        training_logger.debug(f"[Env {self.env_rank}] History appended at step {self.current_step}. Current History Length: {len(self.history)}")

        # Check for episode termination
        terminated = False
        truncated = False

        # Implement a minimum number of steps before allowing termination
        MIN_STEPS = 10
        if self.current_step >= MIN_STEPS:
            if net_worth <= 0:
                terminated = True
                normalized_reward -= 1.0  # Severe penalty for bankruptcy
                training_logger.error(f"[Env {self.env_rank}] Bankruptcy occurred. Terminating episode at step {self.current_step}.")
            elif self.current_step >= len(self.df) - 1:
                terminated = True
                training_logger.info(f"[Env {self.env_rank}] Reached end of data at step {self.current_step}. Terminating episode.")

        # Advance to next step if not terminated
        if not terminated:
            self.prev_net_worth = net_worth
            training_logger.debug(f"[Env {self.env_rank}] Before increment: Step {self.current_step}")
            self.current_step += 1  # Ensure step increments
            training_logger.debug(f"[Env {self.env_rank}] After increment: Step {self.current_step}")
        else:
            # If terminated, do not increment step to prevent overshooting
            training_logger.debug(f"[Env {self.env_rank}] Episode terminated at step {self.current_step}")

        # Ensure current_step does not exceed data length
        self.current_step = min(self.current_step, len(self.df) - 1)

        # Observation after action
        obs = self._next_observation()

        # Logging every 100 steps or upon termination
        if self.current_step % 100 == 0 or terminated:
            training_logger.debug(f"[Env {self.env_rank}] Step {self.current_step}: Reward = {normalized_reward:.4f}, "
                                  f"Net Worth = {net_worth:.2f}, Balance = {self.balance:.2f}, Position = {self.position}")
            training_logger.info(f"[Env {self.env_rank}] Step {self.current_step}: Reward = {normalized_reward:.4f}, "
                                f"Net Worth = {net_worth:.2f}, Balance = {self.balance:.2f}, Position = {self.position}")

        # Log detailed state changes
        training_logger.debug(f"[Env {self.env_rank}] After Action {action_value}: Balance = {self.balance}, Position = {self.position}, Net Worth = {net_worth}")

        return obs, normalized_reward, terminated, truncated, {}

##############################################
# Baseline Strategies
##############################################

def buy_and_hold_with_iloc(df: pd.DataFrame, initial_balance: float = 100000, transaction_cost: float = 0.001) -> dict:
    """
    Implements a Buy and Hold strategy with DataFrame adjustments to prevent KeyError.

    Args:
        df (pd.DataFrame): DataFrame containing stock prices.
        initial_balance (float): Starting balance.
        transaction_cost (float): Transaction cost per trade.

    Returns:
        dict: Results of the strategy.
    """
    # Reset index without changing column cases
    df = df.reset_index(drop=True)

    # Save DataFrame before strategy execution
    bh_before_file = RESULTS_DIR / "buy_and_hold_before.csv"
    df.to_csv(bh_before_file, index=True)
    main_logger.info(f"[Strategy: Buy and Hold] DataFrame before strategy saved to {bh_before_file}")

    balance = initial_balance
    holdings = 0
    net_worth = initial_balance

    required_cols = ['Close_unscaled']
    if not all(col in df.columns for col in required_cols):
        main_logger.error(f"[Strategy: Buy and Hold] Required columns are missing: {required_cols}")
        return {
            'Strategy': 'Buy and Hold',
            'Initial Balance': initial_balance,
            'Final Net Worth': net_worth,
            'Profit': 0.0
        }

    df = df.dropna(subset=required_cols)

    # Save DataFrame after dropping NaNs
    bh_after_dropna_file = RESULTS_DIR / "buy_and_hold_after_dropna.csv"
    df.to_csv(bh_after_dropna_file, index=True)
    main_logger.info(f"[Strategy: Buy and Hold] DataFrame after dropping NaNs saved to {bh_after_dropna_file}")

    if not all(pd.api.types.is_numeric_dtype(df[col]) for col in required_cols):
        main_logger.error(f"[Strategy: Buy and Hold] Required columns have non-numeric data.")
        return {
            'Strategy': 'Buy and Hold',
            'Initial Balance': initial_balance,
            'Final Net Worth': net_worth,
            'Profit': 0.0
        }

    if 'Close_unscaled' not in df.columns:
        main_logger.error(f"[Strategy: Buy and Hold] 'Close_unscaled' column is missing.")
        return {
            'Strategy': 'Buy and Hold',
            'Initial Balance': initial_balance,
            'Final Net Worth': net_worth,
            'Profit': 0.0
        }

    try:
        # Invest the entire initial balance
        investment_percentage = 1.0  # 100% investment
        investment_amount = initial_balance * investment_percentage

        buy_price = df.iloc[0]['Close_unscaled']
        shares_to_buy = math.floor(investment_amount / buy_price)
        invested_capital = shares_to_buy * buy_price
        cost = shares_to_buy * buy_price * transaction_cost
        balance -= invested_capital + cost  # Remaining balance after buying
        holdings += shares_to_buy

        # Save DataFrame after buying
        bh_after_buy_file = RESULTS_DIR / "buy_and_hold_after_buy.csv"
        temp_df = df.copy()
        temp_df['Holdings'] = holdings
        temp_df['Balance'] = balance
        temp_df.to_csv(bh_after_buy_file, index=True)
        main_logger.info(f"[Strategy: Buy and Hold] DataFrame after buying saved to {bh_after_buy_file}")

        # Calculate final net worth
        final_price = df.iloc[-1]['Close_unscaled']
        net_worth = balance + holdings * final_price
        profit = net_worth - initial_balance

        main_logger.info(f"[Strategy: Buy and Hold] Bought {shares_to_buy} shares at {buy_price:.2f}")
        main_logger.info(f"[Strategy: Buy and Hold] Final Net Worth: ${net_worth:.2f}, Profit: ${profit:.2f}")
    except Exception as e:
        main_logger.error(f"[Strategy: Buy and Hold] Error during strategy execution: {e}")
        return {
            'Strategy': 'Buy and Hold',
            'Initial Balance': initial_balance,
            'Final Net Worth': net_worth,
            'Profit': 0.0
        }

    return {
        'Strategy': 'Buy and Hold',
        'Initial Balance': initial_balance,
        'Final Net Worth': net_worth,
        'Profit': profit,
        'Invested Capital': invested_capital,
        'Transaction Costs': cost
    }

def moving_average_crossover_with_iloc(df: pd.DataFrame, initial_balance: float = 100000, transaction_cost: float = 0.001, max_position_size: float = 0.5) -> dict:
    """
    Implements a Moving Average Crossover strategy with DataFrame adjustments to prevent KeyError.

    Args:
        df (pd.DataFrame): DataFrame containing stock prices and indicators.
        initial_balance (float): Starting balance.
        transaction_cost (float): Transaction cost per trade.
        max_position_size (float): Maximum position size as a fraction of net worth.

    Returns:
        dict: Results of the strategy.
    """
    # Reset index without changing column cases
    df = df.reset_index(drop=True)

    # Save DataFrame before strategy execution
    ma_before_file = RESULTS_DIR / "moving_average_crossover_before.csv"
    df.to_csv(ma_before_file, index=True)
    main_logger.info(f"[Strategy: Moving Average Crossover] DataFrame before strategy saved to {ma_before_file}")

    balance = initial_balance
    holdings = 0
    net_worth = initial_balance

    required_cols = ['SMA10_unscaled', 'SMA50_unscaled', 'Close_unscaled']
    if not all(col in df.columns for col in required_cols):
        main_logger.error(f"[Strategy: Moving Average Crossover] Required columns are missing: {required_cols}")
        return {
            'Strategy': 'Moving Average Crossover',
            'Initial Balance': initial_balance,
            'Final Net Worth': net_worth,
            'Profit': 0.0
        }

    df = df.dropna(subset=required_cols)

    # Save DataFrame after dropping NaNs
    ma_after_dropna_file = RESULTS_DIR / "moving_average_crossover_after_dropna.csv"
    df.to_csv(ma_after_dropna_file, index=True)
    main_logger.info(f"[Strategy: Moving Average Crossover] DataFrame after dropping NaNs saved to {ma_after_dropna_file}")

    if not all(pd.api.types.is_numeric_dtype(df[col]) for col in required_cols):
        main_logger.error(f"[Strategy: Moving Average Crossover] Required columns have non-numeric data.")
        return {
            'Strategy': 'Moving Average Crossover',
            'Initial Balance': initial_balance,
            'Final Net Worth': net_worth,
            'Profit': 0.0
        }

    try:
        for i in range(1, len(df)):
            # Golden cross
            if (df.iloc[i-1]['SMA10_unscaled'] < df.iloc[i-1]['SMA50_unscaled']) and (df.iloc[i]['SMA10_unscaled'] >= df.iloc[i]['SMA50_unscaled']):
                # Buy
                shares_to_buy = math.floor(max_position_size * balance / df.iloc[i]['Close_unscaled'])
                # If calculated shares_to_buy is zero but you can afford at least one share, buy one
                if shares_to_buy == 0:
                    one_share_cost = df.iloc[i]['Close_unscaled'] * (1 + transaction_cost)
                    if one_share_cost <= balance:
                        shares_to_buy = 1

                # Always Check Affordability
                total_cost = shares_to_buy * df.iloc[i]['Close_unscaled'] * (1 + transaction_cost)
                if shares_to_buy > 0 and total_cost <= balance:
                    balance -= total_cost
                    holdings += shares_to_buy
                    training_logger.debug(f"[Strategy: Moving Average Crossover] Step {i}: Bought {shares_to_buy} shares at {df.iloc[i]['Close_unscaled']:.2f}")
                    training_logger.info(f"[Strategy: Moving Average Crossover] Step {i}: Bought {shares_to_buy} shares at {df.iloc[i]['Close_unscaled']:.2f}")
                else:
                    # Either can't afford or shares_to_buy is 0; no trade occurs
                    training_logger.debug(f"[Strategy: Moving Average Crossover] Step {i}: Not enough balance to buy shares or no shares calculated.")
                    training_logger.info(f"[Strategy: Moving Average Crossover] Step {i}: Not enough balance to buy shares or no shares calculated.")

            # Death cross
            elif (df.iloc[i-1]['SMA10_unscaled'] > df.iloc[i-1]['SMA50_unscaled']) and (df.iloc[i]['SMA10_unscaled'] <= df.iloc[i]['SMA50_unscaled']):
                # Sell
                shares_to_sell = math.floor(0.5 * holdings)
                # If shares_to_sell ended up as 0 but you have shares, try selling at least 1 share
                if shares_to_sell == 0 and holdings > 0:
                    shares_to_sell = 1

                # Always Check Affordability
                if shares_to_sell > 0 and shares_to_sell <= holdings:
                    proceeds = shares_to_sell * df.iloc[i]['Close_unscaled'] * (1 - transaction_cost)
                    holdings -= shares_to_sell
                    balance += proceeds
                    training_logger.debug(f"[Strategy: Moving Average Crossover] Step {i}: Sold {shares_to_sell} shares at {df.iloc[i]['Close_unscaled']:.2f}")
                    training_logger.info(f"[Strategy: Moving Average Crossover] Step {i}: Sold {shares_to_sell} shares at {df.iloc[i]['Close_unscaled']:.2f}")
                else:
                    # Either can't sell or shares_to_sell is 0; no trade occurs
                    training_logger.debug(f"[Strategy: Moving Average Crossover] Step {i}: Not enough shares to sell or shares_to_sell=0.")
                    training_logger.info(f"[Strategy: Moving Average Crossover] Step {i}: Not enough shares to sell or shares_to_sell=0.")

            # If Hold, do nothing
            net_worth = balance + holdings * df.iloc[i]['Close_unscaled']
            net_worth = float(net_worth)  # Ensure net_worth is a float

    except Exception as e:
        main_logger.error(f"[Strategy: Moving Average Crossover] Error during strategy execution: {e}")
        return {
            'Strategy': 'Moving Average Crossover',
            'Initial Balance': initial_balance,
            'Final Net Worth': net_worth,
            'Profit': 0.0
        }

    # Save DataFrame after strategy execution
    ma_after_strategy_file = RESULTS_DIR / "moving_average_crossover_after_strategy.csv"
    strategy_df = df.copy()
    strategy_df['Holdings'] = holdings
    strategy_df['Balance'] = balance
    strategy_df.to_csv(ma_after_strategy_file, index=True)
    main_logger.info(f"[Strategy: Moving Average Crossover] DataFrame after strategy saved to {ma_after_strategy_file}")

    profit = net_worth - initial_balance
    profit = float(profit)  # Ensure profit is a float
    training_logger.info(f"[Strategy: Moving Average Crossover] Final Net Worth: ${net_worth:.2f}, Profit: ${profit:.2f}")

    return {
        'Strategy': 'Moving Average Crossover',
        'Initial Balance': initial_balance,
        'Final Net Worth': net_worth,
        'Profit': profit
    }

def macd_strategy_with_iloc(df: pd.DataFrame, initial_balance: float = 100000, transaction_cost: float = 0.001, max_position_size: float = 0.5) -> dict:
    """
    Implements a MACD strategy with DataFrame adjustments to prevent KeyError.

    Args:
        df (pd.DataFrame): DataFrame containing stock prices and MACD indicators.
        initial_balance (float): Starting balance.
        transaction_cost (float): Transaction cost per trade.
        max_position_size (float): Maximum position size as a fraction of net worth.

    Returns:
        dict: Results of the strategy.
    """
    df = df.reset_index(drop=True)  # Reset index to ensure proper index

    # Save DataFrame before strategy execution
    macd_before_file = RESULTS_DIR / "macd_strategy_before.csv"
    df.to_csv(macd_before_file, index=True)
    main_logger.info(f"[Strategy: MACD Crossover] DataFrame before strategy saved to {macd_before_file}")

    balance = initial_balance
    holdings = 0
    net_worth = initial_balance

    required_cols = ['MACD_unscaled', 'Close_unscaled']
    if not all(col in df.columns for col in required_cols):
        main_logger.error(f"[Strategy: MACD Crossover] Required columns are missing: {required_cols}")
        return {
            'Strategy': 'MACD Crossover',
            'Initial Balance': initial_balance,
            'Final Net Worth': net_worth,
            'Profit': 0.0
        }

    df = df.dropna(subset=required_cols)

    # Save DataFrame after dropping NaNs
    macd_after_dropna_file = RESULTS_DIR / "macd_strategy_after_dropna.csv"
    df.to_csv(macd_after_dropna_file, index=True)
    main_logger.info(f"[Strategy: MACD Crossover] DataFrame after dropping NaNs saved to {macd_after_dropna_file}")

    if not all(pd.api.types.is_numeric_dtype(df[col]) for col in required_cols):
        main_logger.error(f"[Strategy: MACD Crossover] Required columns have non-numeric data.")
        return {
            'Strategy': 'MACD Crossover',
            'Initial Balance': initial_balance,
            'Final Net Worth': net_worth,
            'Profit': 0.0
        }

    try:
        for i in range(1, len(df)):
            current_macd = df.iloc[i]['MACD_unscaled']
            previous_macd = df.iloc[i-1]['MACD_unscaled']
            current_price = df.iloc[i]['Close_unscaled']

            if (previous_macd < 0) and (current_macd >= 0):
                # Buy logic
                shares_to_buy = math.floor(max_position_size * balance / current_price)
                if shares_to_buy > 0:
                    total_cost = shares_to_buy * current_price * (1 + transaction_cost)
                    if total_cost <= balance:
                        balance -= total_cost
                        holdings += shares_to_buy
                        training_logger.debug(f"[Strategy: MACD Crossover] Step {i}: Bought {shares_to_buy} shares at {current_price:.2f}")
                        training_logger.info(f"[Strategy: MACD Crossover] Step {i}: Bought {shares_to_buy} shares at {current_price:.2f}")
            elif (previous_macd > 0) and (current_macd <= 0):
                # Sell logic
                shares_to_sell = math.floor(0.5 * holdings)
                if shares_to_sell > 0:
                    proceeds = shares_to_sell * current_price * (1 - transaction_cost)
                    holdings -= shares_to_sell
                    balance += proceeds
                    training_logger.debug(f"[Strategy: MACD Crossover] Step {i}: Sold {shares_to_sell} shares at {current_price:.2f}")
                    training_logger.info(f"[Strategy: MACD Crossover] Step {i}: Sold {shares_to_sell} shares at {current_price:.2f}")
            # If Hold, do nothing
            net_worth = balance + holdings * df.iloc[i]['Close_unscaled']
            net_worth = float(net_worth)  # Ensure net_worth is a float

    except Exception as e:
        main_logger.error(f"[Strategy: MACD Crossover] Error during strategy execution: {e}")
        return {
            'Strategy': 'MACD Crossover',
            'Initial Balance': initial_balance,
            'Final Net Worth': net_worth,
            'Profit': 0.0
        }

    # Save DataFrame after strategy execution
    macd_after_strategy_file = RESULTS_DIR / "macd_strategy_after_strategy.csv"
    strategy_df = df.copy()
    strategy_df['Holdings'] = holdings
    strategy_df['Balance'] = balance
    strategy_df.to_csv(macd_after_strategy_file, index=True)
    main_logger.info(f"[Strategy: MACD Crossover] DataFrame after strategy saved to {macd_after_strategy_file}")

    profit = net_worth - initial_balance
    profit = float(profit)  # Ensure profit is a float
    training_logger.info(f"[Strategy: MACD Crossover] Final Net Worth: ${net_worth:.2f}, Profit: ${profit:.2f}")

    return {
        'Strategy': 'MACD Crossover',
        'Initial Balance': initial_balance,
        'Final Net Worth': net_worth,
        'Profit': profit
    }

def bollinger_bands_strategy_with_iloc(df: pd.DataFrame, initial_balance: float = 100000, transaction_cost: float = 0.001, max_position_size: float = 0.5) -> dict:
    """
    Implements a Bollinger Bands strategy with DataFrame adjustments to prevent KeyError.

    Args:
        df (pd.DataFrame): DataFrame containing stock prices and Bollinger Bands.
        initial_balance (float): Starting balance.
        transaction_cost (float): Transaction cost per trade.
        max_position_size (float): Maximum position size as a fraction of net worth.

    Returns:
        dict: Results of the strategy.
    """
    # Reset index without changing column cases
    df = df.reset_index(drop=True)

    # Save DataFrame before strategy execution
    bb_before_file = RESULTS_DIR / "bollinger_bands_before.csv"
    df.to_csv(bb_before_file, index=True)
    main_logger.info(f"[Strategy: Bollinger Bands] DataFrame before strategy saved to {bb_before_file}")

    balance = initial_balance
    holdings = 0
    net_worth = initial_balance

    required_cols = ['BB_Upper_unscaled', 'BB_Lower_unscaled', 'Close_unscaled']
    if not all(col in df.columns for col in required_cols):
        main_logger.error(f"[Strategy: Bollinger Bands] Required columns are missing: {required_cols}")
        return {
            'Strategy': 'Bollinger Bands',
            'Initial Balance': initial_balance,
            'Final Net Worth': net_worth,
            'Profit': 0.0
        }

    df = df.dropna(subset=required_cols)

    # Save DataFrame after dropping NaNs
    bb_after_dropna_file = RESULTS_DIR / "bollinger_bands_after_dropna.csv"
    df.to_csv(bb_after_dropna_file, index=True)
    main_logger.info(f"[Strategy: Bollinger Bands] DataFrame after dropping NaNs saved to {bb_after_dropna_file}")

    if not all(pd.api.types.is_numeric_dtype(df[col]) for col in required_cols):
        main_logger.error(f"[Strategy: Bollinger Bands] Required columns have non-numeric data.")
        return {
            'Strategy': 'Bollinger Bands',
            'Initial Balance': initial_balance,
            'Final Net Worth': net_worth,
            'Profit': 0.0
        }

    try:
        for i in range(len(df)):
            # Buy when price crosses below lower band
            if df.iloc[i]['Close_unscaled'] < df.iloc[i]['BB_Lower_unscaled']:
                shares_to_buy = math.floor(max_position_size * balance / df.iloc[i]['Close_unscaled'])
                # If calculated shares_to_buy is zero but you can afford at least one share, buy one
                if shares_to_buy == 0:
                    one_share_cost = df.iloc[i]['Close_unscaled'] * (1 + transaction_cost)
                    if one_share_cost <= balance:
                        shares_to_buy = 1

                # Always Check Affordability
                total_cost = shares_to_buy * df.iloc[i]['Close_unscaled'] * (1 + transaction_cost)
                if shares_to_buy > 0 and total_cost <= balance:
                    balance -= total_cost
                    holdings += shares_to_buy
                    training_logger.debug(f"[Strategy: Bollinger Bands] Step {i}: Bought {shares_to_buy} shares at {df.iloc[i]['Close_unscaled']:.2f}")
                    training_logger.info(f"[Strategy: Bollinger Bands] Step {i}: Bought {shares_to_buy} shares at {df.iloc[i]['Close_unscaled']:.2f}")
                else:
                    # Either can't afford or shares_to_buy is 0; no trade occurs
                    training_logger.debug(f"[Strategy: Bollinger Bands] Step {i}: Not enough balance to buy shares or no shares calculated.")
                    training_logger.info(f"[Strategy: Bollinger Bands] Step {i}: Not enough balance to buy shares or no shares calculated.")

            # Sell when price crosses above upper band
            elif df.iloc[i]['Close_unscaled'] > df.iloc[i]['BB_Upper_unscaled']:
                shares_to_sell = math.floor(0.5 * holdings)
                # If shares_to_sell ended up as 0 but you have shares, try selling at least 1 share
                if shares_to_sell == 0 and holdings > 0:
                    shares_to_sell = 1

                # Always Check Affordability
                if shares_to_sell > 0 and shares_to_sell <= holdings:
                    proceeds = shares_to_sell * df.iloc[i]['Close_unscaled'] * (1 - transaction_cost)
                    holdings -= shares_to_sell
                    balance += proceeds
                    training_logger.debug(f"[Strategy: Bollinger Bands] Step {i}: Sold {shares_to_sell} shares at {df.iloc[i]['Close_unscaled']:.2f}")
                    training_logger.info(f"[Strategy: Bollinger Bands] Step {i}: Sold {shares_to_sell} shares at {df.iloc[i]['Close_unscaled']:.2f}")
                else:
                    # Either can't sell or shares_to_sell is 0; no trade occurs
                    training_logger.debug(f"[Strategy: Bollinger Bands] Step {i}: Not enough shares to sell or shares_to_sell=0.")
                    training_logger.info(f"[Strategy: Bollinger Bands] Step {i}: Not enough shares to sell or shares_to_sell=0.")

            # If Hold, do nothing
            net_worth = balance + holdings * df.iloc[i]['Close_unscaled']
            net_worth = float(net_worth)  # Ensure net_worth is a float

    except Exception as e:
        main_logger.error(f"[Strategy: Bollinger Bands] Error during strategy execution: {e}")
        return {
            'Strategy': 'Bollinger Bands',
            'Initial Balance': initial_balance,
            'Final Net Worth': net_worth,
            'Profit': 0.0
        }

    # Save DataFrame after strategy execution
    bb_after_strategy_file = RESULTS_DIR / "bollinger_bands_after_strategy.csv"
    strategy_df = df.copy()
    strategy_df['Holdings'] = holdings
    strategy_df['Balance'] = balance
    strategy_df.to_csv(bb_after_strategy_file, index=True)
    main_logger.info(f"[Strategy: Bollinger Bands] DataFrame after strategy saved to {bb_after_strategy_file}")

    profit = net_worth - initial_balance
    profit = float(profit)  # Ensure profit is a float
    training_logger.info(f"[Strategy: Bollinger Bands] Final Net Worth: ${net_worth:.2f}, Profit: ${profit:.2f}")

    return {
        'Strategy': 'Bollinger Bands',
        'Initial Balance': initial_balance,
        'Final Net Worth': net_worth,
        'Profit': profit
    }

def random_strategy_with_iloc(df: pd.DataFrame, initial_balance: float = 100000, transaction_cost: float = 0.001, max_position_size: float = 0.5) -> dict:
    """
    Implements a Random strategy with DataFrame adjustments to prevent KeyError.

    Args:
        df (pd.DataFrame): DataFrame containing stock prices.
        initial_balance (float): Starting balance.
        transaction_cost (float): Transaction cost per trade.
        max_position_size (float): Maximum position size as a fraction of net worth.

    Returns:
        dict: Results of the strategy.
    """
    # Reset index without changing column cases
    df = df.reset_index(drop=True)

    # Save DataFrame before strategy execution
    random_before_file = RESULTS_DIR / "random_strategy_before.csv"
    df.to_csv(random_before_file, index=True)
    main_logger.info(f"[Strategy: Random Strategy] DataFrame before strategy saved to {random_before_file}")

    balance = initial_balance
    holdings = 0
    net_worth = initial_balance

    required_cols = ['Close_unscaled']
    if not all(col in df.columns for col in required_cols):
        main_logger.error(f"[Strategy: Random Strategy] Required columns are missing: {required_cols}")
        return {
            'Strategy': 'Random Strategy',
            'Initial Balance': initial_balance,
            'Final Net Worth': net_worth,
            'Profit': 0.0
        }

    df = df.dropna(subset=required_cols)

    # Save DataFrame after dropping NaNs
    random_after_dropna_file = RESULTS_DIR / "random_strategy_after_dropna.csv"
    df.to_csv(random_after_dropna_file, index=True)
    main_logger.info(f"[Strategy: Random Strategy] DataFrame after dropping NaNs saved to {random_after_dropna_file}")

    if not all(pd.api.types.is_numeric_dtype(df[col]) for col in required_cols):
        main_logger.error(f"[Strategy: Random Strategy] Required columns have non-numeric data.")
        return {
            'Strategy': 'Random Strategy',
            'Initial Balance': initial_balance,
            'Final Net Worth': net_worth,
            'Profit': 0.0
        }

    try:
        for i in range(len(df)):
            action = random.choice(['Buy', 'Sell', 'Hold'])
            if action == 'Buy':
                shares_to_buy = math.floor(max_position_size * balance / df.iloc[i]['Close_unscaled'])
                # If calculated shares_to_buy is zero but you can afford at least one share, buy one
                if shares_to_buy == 0:
                    one_share_cost = df.iloc[i]['Close_unscaled'] * (1 + transaction_cost)
                    if one_share_cost <= balance:
                        shares_to_buy = 1

                # Always Check Affordability
                total_cost = shares_to_buy * df.iloc[i]['Close_unscaled'] * (1 + transaction_cost)
                if shares_to_buy > 0 and total_cost <= balance:
                    balance -= total_cost
                    holdings += shares_to_buy
                    training_logger.debug(f"[Strategy: Random Strategy] Step {i}: Bought {shares_to_buy} shares at {df.iloc[i]['Close_unscaled']:.2f}")
                    training_logger.info(f"[Strategy: Random Strategy] Step {i}: Bought {shares_to_buy} shares at {df.iloc[i]['Close_unscaled']:.2f}")
                else:
                    # Either can't afford or shares_to_buy is 0; no trade occurs
                    training_logger.debug(f"[Strategy: Random Strategy] Step {i}: Not enough balance to buy shares or no shares calculated.")
                    training_logger.info(f"[Strategy: Random Strategy] Step {i}: Not enough balance to buy shares or no shares calculated.")

            elif action == 'Sell':
                shares_to_sell = math.floor(0.5 * holdings)
                # If shares_to_sell ended up as 0 but you have shares, try selling at least 1 share
                if shares_to_sell == 0 and holdings > 0:
                    shares_to_sell = 1

                # Always Check Affordability
                if shares_to_sell > 0 and shares_to_sell <= holdings:
                    proceeds = shares_to_sell * df.iloc[i]['Close_unscaled'] * (1 - transaction_cost)
                    holdings -= shares_to_sell
                    balance += proceeds
                    training_logger.debug(f"[Strategy: Random Strategy] Step {i}: Sold {shares_to_sell} shares at {df.iloc[i]['Close_unscaled']:.2f}")
                    training_logger.info(f"[Strategy: Random Strategy] Step {i}: Sold {shares_to_sell} shares at {df.iloc[i]['Close_unscaled']:.2f}")
                else:
                    # Either can't sell or shares_to_sell is 0; no trade occurs
                    training_logger.debug(f"[Strategy: Random Strategy] Step {i}: Not enough shares to sell or shares_to_sell=0.")
                    training_logger.info(f"[Strategy: Random Strategy] Step {i}: Not enough shares to sell or shares_to_sell=0.")

            # If Hold, do nothing
            net_worth = balance + holdings * df.iloc[i]['Close_unscaled']
            net_worth = float(net_worth)  # Ensure net_worth is a float

    except Exception as e:
        main_logger.error(f"[Strategy: Random Strategy] Error during strategy execution: {e}")
        return {
            'Strategy': 'Random Strategy',
            'Initial Balance': initial_balance,
            'Final Net Worth': net_worth,
            'Profit': 0.0
        }

    # Save DataFrame after strategy execution
    random_after_strategy_file = RESULTS_DIR / "random_strategy_after_strategy.csv"
    strategy_df = df.copy()
    strategy_df['Holdings'] = holdings
    strategy_df['Balance'] = balance
    strategy_df.to_csv(random_after_strategy_file, index=True)
    main_logger.info(f"[Strategy: Random Strategy] DataFrame after strategy saved to {random_after_strategy_file}")

    profit = net_worth - initial_balance
    profit = float(profit)  # Ensure profit is a float
    training_logger.info(f"[Strategy: Random Strategy] Final Net Worth: ${net_worth:.2f}, Profit: ${profit:.2f}")

    return {
        'Strategy': 'Random Strategy',
        'Initial Balance': initial_balance,
        'Final Net Worth': net_worth,
        'Profit': profit
    }

##############################################
# Evaluation and Plotting Functions
##############################################

def plot_rl_training_history(rl_df: pd.DataFrame):
    """
    Plots the RL agent's net worth and rewards over time.

    Args:
        rl_df (pd.DataFrame): DataFrame containing the RL agent's trading history.
    """
    if rl_df.empty:
        main_logger.error("RL history is empty. Cannot plot training history.")
        return

    plt.figure(figsize=(14,7))

    # Plot Net Worth
    plt.subplot(2, 1, 1)
    plt.plot(rl_df.index, rl_df['Net Worth'], label='Net Worth', color='blue')
    plt.title('RL Agent Net Worth Over Time')
    plt.xlabel('Step')
    plt.ylabel('Net Worth ($)')
    plt.legend()
    plt.grid(True)

    # Plot Reward
    plt.subplot(2, 1, 2)
    plt.plot(rl_df.index, rl_df['Reward'], label='Reward', color='green')
    plt.title('RL Agent Reward Over Time')
    plt.xlabel('Step')
    plt.ylabel('Reward')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.savefig(PLOTS_DIR / "rl_training_history.png")
    plt.show()
    main_logger.info("RL training history plotted successfully.")

def plot_results(test_df: pd.DataFrame, rl_test_df: pd.DataFrame, initial_balance: float, ticker: str):
    """
    Plots trading actions and net worth on test data.

    Args:
        test_df (pd.DataFrame): Stock test data.
        rl_test_df (pd.DataFrame): RL agent's test history.
        initial_balance (float): Initial balance for strategies.
        ticker (str): Stock ticker.
    """
    if rl_test_df.empty:
        main_logger.critical("RL test history is empty. Skipping plots.")
        return

    # Ensure rl_test_df and test_df are aligned
    min_length = min(len(test_df), len(rl_test_df))
    aligned_df = test_df.iloc[:min_length].reset_index(drop=True)
    aligned_rl_df = rl_test_df.iloc[:min_length].reset_index(drop=True)

    fig, axs = plt.subplots(2, 1, figsize=(14, 12))

    # Plot Price with Buy/Sell Signals using unscaled data
    axs[0].plot(aligned_df['Date'], aligned_df['Close_unscaled'], label='Close Price (Unscaled)', color='blue', alpha=0.6)

    # Plot Buy Signals
    buy_signals = aligned_rl_df[aligned_rl_df['Buy_Signal_Price'].notna()]
    if not buy_signals.empty:
        axs[0].scatter(buy_signals['Date'], buy_signals['Buy_Signal_Price'], color='green', marker='^', s=100, label='Buy Signal')

    # Plot Sell Signals
    sell_signals = aligned_rl_df[aligned_rl_df['Sell_Signal_Price'].notna()]
    if not sell_signals.empty:
        axs[0].scatter(sell_signals['Date'], sell_signals['Sell_Signal_Price'], color='red', marker='v', s=100, label='Sell Signal')

    axs[0].set_title(f'{ticker} Price with Buy/Sell Signals on Test Data (Unscaled)', fontsize=16)
    axs[0].set_xlabel('Date', fontsize=14)
    axs[0].set_ylabel('Price ($)', fontsize=14)
    axs[0].legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=12)
    axs[0].tick_params(axis='both', which='major', labelsize=12)
    axs[0].grid(True)

    # Plot Net Worth
    axs[1].plot(aligned_rl_df['Date'], aligned_rl_df['Net Worth'], label='RL Agent Net Worth', color='blue')
    axs[1].set_title(f'{ticker} RL Agent Net Worth Over Test Data', fontsize=16)
    axs[1].set_xlabel('Date', fontsize=14)
    axs[1].set_ylabel('Net Worth ($)', fontsize=14)
    axs[1].legend()
    axs[1].tick_params(axis='both', which='major', labelsize=12)
    axs[1].grid(True)

    plt.tight_layout()
    plt.savefig(PLOTS_DIR / f"{ticker}_test_combined_plot.png")
    plt.close()

    main_logger.critical(f"Test combined plots saved in {PLOTS_DIR}")

def plot_agent_performance(rl_test_df: pd.DataFrame, ticker: str):
    """
    Plots the RL agent's performance metrics over test time.

    Args:
        rl_test_df (pd.DataFrame): DataFrame containing the RL agent's test trading history.
        ticker (str): Stock ticker.
    """
    if rl_test_df.empty:
        main_logger.error("RL test history is empty. Cannot plot agent performance.")
        return

    plt.figure(figsize=(15, 10))

    # Plot Net Worth
    plt.subplot(3, 1, 1)
    plt.plot(rl_test_df.index, rl_test_df['Net Worth'], label='Net Worth', color='blue')
    plt.title(f'{ticker} RL Agent Net Worth Over Test Data')
    plt.xlabel('Step')
    plt.ylabel('Net Worth ($)')
    plt.legend()
    plt.grid(True)

    # Plot Reward
    plt.subplot(3, 1, 2)
    plt.plot(rl_test_df.index, rl_test_df['Reward'], label='Reward', color='green')
    plt.title('Reward Over Test Data')
    plt.xlabel('Step')
    plt.ylabel('Reward')
    plt.legend()
    plt.grid(True)

    # Plot Position
    plt.subplot(3, 1, 3)
    plt.plot(rl_test_df.index, rl_test_df['Position'], label='Position', color='red')
    plt.title('Position Over Test Data')
    plt.xlabel('Step')
    plt.ylabel('Position (Shares)')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.savefig(PLOTS_DIR / f"{ticker}_agent_performance_test.png")
    plt.close()
    main_logger.info("Agent performance plots on test data generated successfully.")

def plot_action_distribution(rl_test_df: pd.DataFrame):
    """
    Plots the distribution of actions taken by the agent.

    Args:
        rl_test_df (pd.DataFrame): DataFrame containing the RL agent's test trading history.
    """
    if rl_test_df.empty:
        main_logger.error("RL test history is empty. Cannot plot action distribution.")
        return

    if 'Action' not in rl_test_df.columns:
        rl_test_df['Action'] = 0.0
        main_logger.critical("'Action' column not found in RL test history. Defaulting actions to 0.")

    # Categorize actions
    conditions = [
        (rl_test_df['Action'] > 0.05),
        (rl_test_df['Action'] < -0.05),
        (rl_test_df['Action'].between(-0.05, 0.05))
    ]
    choices = ['Buy', 'Sell', 'Hold']
    rl_test_df['action_category'] = np.select(conditions, choices, default='Hold')

    action_counts = rl_test_df['action_category'].value_counts().reindex(['Hold', 'Buy', 'Sell']).fillna(0)

    plt.figure(figsize=(8,6))
    plt.bar(action_counts.index, action_counts.values, color=['grey', 'green', 'red'])
    plt.title('Action Distribution on Test Data')
    plt.xlabel('Action')
    plt.ylabel('Count')
    plt.show()
    main_logger.info("Action distribution plot generated successfully.")

def plot_comparison(test_df: pd.DataFrame, rl_test_df: pd.DataFrame, baseline_results: list, initial_balance: float, ticker: str):
    """
    Plots the RL agent's performance against baseline strategies.

    Args:
        test_df (pd.DataFrame): Test data.
        rl_test_df (pd.DataFrame): RL agent's test history.
        baseline_results (list): List of baseline strategy results.
        initial_balance (float): Initial balance for strategies.
        ticker (str): Stock ticker.
    """
    if rl_test_df.empty:
        main_logger.error("RL test history is empty. Cannot plot comparison.")
        return

    plt.figure(figsize=(14, 7))

    # Plot RL Agent's Net Worth
    plt.plot(rl_test_df.index, rl_test_df['Net Worth'], label='RL Agent', color='blue')

    # Plot Baseline Strategies' Net Worth
    for result in baseline_results:
        strategy = result['Strategy']
        final_net_worth = result['Final Net Worth']
        if final_net_worth == 0.0:
            continue  # Skip strategies that were skipped or failed
        # Calculate step-wise net worth assuming the strategy's profit is spread linearly
        net_worth = np.linspace(initial_balance, final_net_worth, len(test_df))
        plt.plot(range(len(net_worth)), net_worth, label=strategy, linestyle='--')

    plt.title(f'RL Agent vs Baseline Strategies on {ticker} Test Data')
    plt.xlabel('Step')
    plt.ylabel('Net Worth ($)')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(PLOTS_DIR / f"{ticker}_comparison_plot.png")
    plt.close()
    main_logger.critical(f"Comparison plot between RL agent and baseline strategies saved successfully in {PLOTS_DIR}")

##############################################
# Callbacks
##############################################

class EarlyStoppingCallback(BaseCallback):
    """
    Custom callback for implementing early stopping based on the normalized reward.
    Stops training if the normalized reward does not improve for a given number of evaluations (patience).
    """
    def __init__(self, monitor='train/reward_env', patience=20, min_delta=1e-5, verbose=1):
        super(EarlyStoppingCallback, self).__init__(verbose)
        self.monitor = monitor
        self.patience = patience
        self.min_delta = min_delta
        self.best_reward = -np.inf
        self.wait = 0

    def _on_step(self) -> bool:
        # Retrieve the latest value of the monitored metric
        current_reward = self.logger.name_to_value.get(self.monitor, None)

        if current_reward is None:
            if self.verbose > 0:
                print(f"EarlyStoppingCallback: Metric '{self.monitor}' not found.")
            return True  # Continue training

        if current_reward > self.best_reward + self.min_delta:
            self.best_reward = current_reward
            self.wait = 0
            if self.verbose > 0:
                print(f"EarlyStoppingCallback: Reward improved to {self.best_reward:.4f}. Resetting wait counter.")
        else:
            self.wait += 1
            if self.verbose > 0:
                print(f"EarlyStoppingCallback: No improvement in reward. Wait counter: {self.wait}/{self.patience}")
            if self.wait >= self.patience:
                if self.verbose > 0:
                    print("EarlyStoppingCallback: Patience exceeded. Stopping training.")
                return False  # Stop training

        return True  # Continue training

class CustomTensorboardCallback(BaseCallback):
    """
    Custom callback for logging additional metrics to TensorBoard.
    Logs net worth, reward, and elapsed time.
    """
    def __init__(self, verbose=0):
        super(CustomTensorboardCallback, self).__init__(verbose)
        self.start_time = None

    def _on_training_start(self) -> None:
        self.start_time = time.time()

    def _on_step(self) -> bool:
        # Access the environment
        env = self.training_env.envs[0]

        # Log net worth and reward if history is not empty
        if hasattr(env, 'history') and env.history:
            last_history = env.history[-1]
            self.logger.record("train/net_worth_env", last_history.get('Net Worth', 0.0))
            self.logger.record("train/balance_env", last_history.get('Balance', 0.0))
            self.logger.record("train/position_env", last_history.get('Position', 0.0))
            self.logger.record("train/reward_env", last_history.get('Reward', 0.0))

        # Log elapsed time
        if self.start_time:
            elapsed_time = time.time() - self.start_time  # in seconds
            elapsed_time_formatted = time.strftime("%H:%M:%S", time.gmtime(elapsed_time))
            self.logger.record("train/elapsed_time_env", elapsed_time)
            self.logger.record("train/elapsed_time_formatted_env", elapsed_time_formatted)

        return True

    def _on_training_end(self) -> None:
        env = self.training_env.envs[0]
        if hasattr(env, 'history') and env.history:
            self.logger.record("train/final_net_worth", env.history[-1].get('Net Worth', 0.0))
            self.logger.record("train/final_reward", sum(h['Reward'] for h in env.history))
            self.logger.record("train/final_balance", env.history[-1].get('Balance', 0.0))
            self.logger.record("train/final_position", env.history[-1].get('Position', 0.0))
        else:
            self.logger.record("train/final_net_worth", 0.0)
            self.logger.record("train/final_reward", 0.0)
            self.logger.record("train/final_balance", 0.0)
            self.logger.record("train/final_position", 0.0)

##############################################
# Additional Utility Functions
##############################################

def calculate_max_drawdown(net_worth_series: pd.Series) -> float:
    """
    Calculates the Maximum Drawdown of a net worth series.

    Args:
        net_worth_series (pd.Series): Series of net worth over time.

    Returns:
        float: Maximum drawdown value.
    """
    rolling_max = net_worth_series.cummax()
    drawdown = (net_worth_series - rolling_max) / rolling_max
    return drawdown.min()

def calculate_annualized_return(net_worth_series: pd.Series, periods_per_year: int = 252) -> float:
    """
    Calculates the Annualized Return (CAGR).

    Args:
        net_worth_series (pd.Series): Series of net worth over time.
        periods_per_year (int): Number of trading periods in a year.

    Returns:
        float: Annualized return.
    """
    start_value = net_worth_series.iloc[0]
    end_value = net_worth_series.iloc[-1]
    num_periods = len(net_worth_series)
    if num_periods == 0:
        return 0.0
    return (end_value / start_value) ** (periods_per_year / num_periods) - 1

##############################################
# Optuna Hyperparameter Tuning
##############################################

def generate_unique_study_name(base_name='rl_trading_agent_study'):
    """
    Generates a unique study name by appending the current timestamp.

    Args:
        base_name (str): The base name for the study.

    Returns:
        str: A unique study name.
    """
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"{base_name}_{timestamp}"

def objective(trial, df, scaler, initial_balance, stop_loss, take_profit, max_position_size, max_drawdown, annual_trading_days, transaction_cost):
    """
    Objective function for Optuna to maximize final performance metric based on cumulative reward.

    This version logs all individual properties of the environment at the end of each trial
    for comprehensive debugging and analysis.
    """
    # Define hyperparameter search space    
    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)
    n_steps = trial.suggest_categorical('n_steps', [128, 256, 512])
    batch_size = trial.suggest_categorical('batch_size', [32, 64])
    gamma = trial.suggest_uniform('gamma', 0.90, 0.99)
    gae_lambda = trial.suggest_uniform('gae_lambda', 0.80, 1.00)
    clip_range = trial.suggest_uniform('clip_range', 0.1, 0.3)
    ent_coef = trial.suggest_loguniform('ent_coef', 1e-5, 1e-3)
    vf_coef = trial.suggest_uniform('vf_coef', 0.1, 0.5)
    max_grad_norm = trial.suggest_uniform('max_grad_norm', 0.5, 1.0)
    net_arch = trial.suggest_categorical('net_arch', ['128_128', '256_256', '128_256_128'])

    # Initialize training environment with these hyperparameters
    env_train = SingleStockTradingEnv(
        df=df,
        scaler=scaler,
        initial_balance=initial_balance,
        stop_loss=stop_loss,
        take_profit=take_profit,
        max_position_size=max_position_size,
        max_drawdown=max_drawdown,
        annual_trading_days=annual_trading_days,
        transaction_cost=transaction_cost,
        env_rank=trial.number + 1,  # Unique identifier for the environment
        reward_weights={'reward_scale': 1.0}  # Renamed to prevent confusion
    )
    env_train.seed(RANDOM_SEED + trial.number + 1)

    # Using DummyVecEnv for consistency with SB3 training
    vec_env_train = DummyVecEnv([lambda: env_train])

    # Define policy network architecture
    policy_kwargs = dict(
        activation_fn=torch.nn.ReLU,
        net_arch=[int(x) for x in net_arch.split('_')]
    )

    # Define unique TensorBoard log directory for the trial
    trial_log_dir = TB_LOG_DIR / f"trial_{trial.number}"
    trial_log_dir.mkdir(parents=True, exist_ok=True)

    # Initialize PPO model
    model = PPO(
        'MlpPolicy',
        vec_env_train,
        verbose=0,  # Set to 0 to prevent SB3 logs from cluttering
        seed=RANDOM_SEED,
        policy_kwargs=policy_kwargs,
        learning_rate=learning_rate,
        n_steps=n_steps,
        batch_size=batch_size,
        gamma=gamma,
        gae_lambda=gae_lambda,
        clip_range=clip_range,
        ent_coef=ent_coef,
        vf_coef=vf_coef,
        max_grad_norm=max_grad_norm,
        tensorboard_log=str(trial_log_dir),
        device='cpu'  # Ensure CPU usage
    )

    # Define checkpoint callback
    trial_checkpoint_dir = RESULTS_DIR / f"checkpoints_trial_{trial.number}"
    trial_checkpoint_dir.mkdir(parents=True, exist_ok=True)
    checkpoint_callback = CheckpointCallback(
        save_freq=50000,  # Reduced frequency to prevent overload
        save_path=str(trial_checkpoint_dir),
        name_prefix="ppo_model"
    )
    custom_callback = CustomTensorboardCallback()
    early_stopping_callback = EarlyStoppingCallback(
        monitor='train/reward_env',
        patience=20,
        min_delta=1e-5,
        verbose=1
    )
    callback_list = CallbackList([custom_callback, checkpoint_callback, early_stopping_callback])

    # Start training
    start_time = time.time()
    try:
        model.learn(
            total_timesteps=500000,
            callback=callback_list
        )
    except Exception as e:
        main_logger.critical(f"[Trial {trial.number}] Training failed: {e}")
        return -np.inf  # Assign a very low value to failed trials
    duration = time.time() - start_time

    # After training, access the history from the training environment
    env_train_history = env_train.history

    if env_train_history:
        cumulative_reward = sum([entry['Reward'] for entry in env_train_history])
    else:
        main_logger.warning(f"[Trial {trial.number}] Environment history is empty or not available.")
        cumulative_reward = 0.0

    # Log all individual properties
    main_logger.critical(f"[Trial {trial.number}] Cumulative Reward: {cumulative_reward:.4f}")
    main_logger.critical(f"[Trial {trial.number}] Final Net Worth: ${env_train.net_worth:.2f}")
    main_logger.critical(f"[Trial {trial.number}] Final Balance: ${env_train.balance:.2f}")
    main_logger.critical(f"[Trial {trial.number}] Final Position: {env_train.position} shares")
    main_logger.critical(f"[Trial {trial.number}] Total Transactions: {env_train.transaction_count}")
    main_logger.critical(f"[Trial {trial.number}] Final Peak Net Worth: ${env_train.peak:.2f}")
    final_drawdown = (env_train.peak - env_train.net_worth) / env_train.peak if env_train.peak > 0 else 0.0
    main_logger.critical(f"[Trial {trial.number}] Final Drawdown: {final_drawdown*100:.2f}%")

    # Optionally, log additional properties from the history
    if env_train_history:
        last_history = env_train_history[-1]
        history_details = "\n".join([f"{key}: {value}" for key, value in last_history.items()])
        main_logger.critical(f"[Trial {trial.number}] Last Step History:\n{history_details}")
    else:
        main_logger.warning(f"[Trial {trial.number}] Environment history is empty or not available.")

    # Persist Environment History to CSV
    trial_log_file = RESULTS_DIR / f"trial_{trial.number}_history.csv"
    if env_train_history:
        pd.DataFrame(env_train_history).to_csv(trial_log_file, index=False)
        main_logger.info(f"[Trial {trial.number}] Environment history saved to {trial_log_file}")
    else:
        # Save an empty DataFrame or with default values if history is empty
        pd.DataFrame().to_csv(trial_log_file, index=False)
        main_logger.warning(f"[Trial {trial.number}] Environment history was empty. Saved empty CSV at {trial_log_file}")

    return cumulative_reward

##############################################
# Main Execution
##############################################

def make_env(env_params, env_rank, seed=RANDOM_SEED):
    """
    Creates and returns a callable that initializes the SingleStockTradingEnv.

    Args:
        env_params (dict): Parameters to initialize the environment.
        env_rank (int): Unique identifier for the environment.
        seed (int): Random seed.

    Returns:
        callable: A function that creates and returns a SingleStockTradingEnv instance when called.
    """
    def _init():
        env_instance = SingleStockTradingEnv(
            df=env_params['df'],
            scaler=env_params['scaler'],
            initial_balance=env_params['initial_balance'],
            stop_loss=env_params['stop_loss'],
            take_profit=env_params['take_profit'],
            max_position_size=env_params['max_position_size'],
            max_drawdown=env_params['max_drawdown'],
            annual_trading_days=env_params['annual_trading_days'],
            transaction_cost=env_params['transaction_cost'],
            env_rank=env_rank,
            reward_weights=env_params.get('reward_weights', None)
        )
        env_instance.seed(seed + env_rank)
        return env_instance
    return _init

if __name__ == "__main__":
    # Define parameters
    TICKER = 'APOLLOTYRE.NS'
    START_DATE = '2018-01-01'
    END_DATE = datetime.datetime.now().strftime('%Y-%m-%d')  # Current date
    INITIAL_BALANCE = 100000
    STOP_LOSS = 0.90
    TAKE_PROFIT = 1.10
    MAX_POSITION_SIZE = 0.5  # Increased from 0.25 to 0.5
    MAX_DRAWDOWN = 0.20
    ANNUAL_TRADING_DAYS = 252
    TRANSACTION_COST = 0.001  # 0.1% per trade

    # Fetch and prepare data
    # Scaling is now handled within get_data
    df, scaler = get_data(TICKER, START_DATE, END_DATE, scaler=None, fit_scaler=True)
    if df.empty:
        main_logger.critical("No data fetched. Exiting.")
        exit()

    # Split into training and testing datasets
    split_ratio = 0.8  # 80% training, 20% testing
    split_idx = int(len(df) * split_ratio)
    train_df = df.iloc[:split_idx].reset_index(drop=True)
    test_df = df.iloc[split_idx:].reset_index(drop=True)

    main_logger.info(f"Training data: {len(train_df)} samples")
    main_logger.info(f"Testing data: {len(test_df)} samples")

    # Save train and test data to CSV
    train_data_file = RESULTS_DIR / f"{TICKER}_train_data.csv"
    test_data_file = RESULTS_DIR / f"{TICKER}_test_data.csv"
    train_df.to_csv(train_data_file, index=False)
    test_df.to_csv(test_data_file, index=False)
    main_logger.info(f"Training data saved to {train_data_file}")
    main_logger.info(f"Testing data saved to {test_data_file}")

    # Log test_df size and columns
    main_logger.info(f"Test DataFrame Size: {test_df.shape}")
    main_logger.info(f"Test DataFrame Columns: {test_df.columns.tolist()}")

    # Verify test_df integrity
    if test_df.empty:
        main_logger.critical("Test DataFrame is empty. Exiting.")
        exit()

    # Save the scaler for future use (e.g., deployment)
    scaler_filename = RESULTS_DIR / 'scaler.pkl'
    joblib.dump(scaler, scaler_filename)
    main_logger.info(f"Scaler fitted on training data and saved as {scaler_filename}")

    # Optionally, copy test_df to another variable for baseline strategies
    baseline_test_df = test_df.copy()
    main_logger.info("Copied test_df to baseline_test_df for baseline strategies evaluation.")

    # Check if test_df has sufficient data
    MIN_TEST_SAMPLES = 500  # Define a minimum number of samples for testing
    if len(test_df) < MIN_TEST_SAMPLES:
        main_logger.warning(f"Testing data has only {len(test_df)} samples. Increasing test set size to {MIN_TEST_SAMPLES}.")
        # Adjust split to ensure test_df has at least MIN_TEST_SAMPLES
        split_idx = len(df) - MIN_TEST_SAMPLES
        train_df = df.iloc[:split_idx].reset_index(drop=True)
        test_df = df.iloc[split_idx:].reset_index(drop=True)
        baseline_test_df = test_df.copy()
        main_logger.info(f"Adjusted Training data: {len(train_df)} samples")
        main_logger.info(f"Adjusted Testing data: {len(test_df)} samples")
        main_logger.info(f"Adjusted baseline_test_df Size: {baseline_test_df.shape}")
        main_logger.info(f"Adjusted baseline_test_df Columns: {baseline_test_df.columns.tolist()}")

    # Verify unscaled columns in test_df
    missing_unscaled_test = [feature for feature in UNSCALED_FEATURES if feature not in test_df.columns]
    if missing_unscaled_test:
        main_logger.error(f"Missing unscaled features in test DataFrame: {missing_unscaled_test}")
        exit()
    else:
        main_logger.debug("All unscaled features are present in the test DataFrame.")
        main_logger.debug(f"Columns in test_df: {test_df.columns.tolist()}")

    # Additional Verification: Ensure required columns are present
    required_cols_verification = ['MACD_unscaled', 'Close_unscaled']
    missing_cols_verification = [col for col in required_cols_verification if col not in test_df.columns]
    if missing_cols_verification:
        main_logger.error(f"Missing required columns for MACD strategy in test DataFrame: {missing_cols_verification}")
        # Decide whether to proceed or skip certain strategies
    else:
        main_logger.debug("All required columns for MACD strategy are present in test_df.")

    # Check environment validity
    main_logger.info("Checking environment compatibility with SB3...")
    env_checker = SingleStockTradingEnv(
        df=train_df,
        scaler=scaler,
        initial_balance=INITIAL_BALANCE,
        stop_loss=STOP_LOSS,
        take_profit=TAKE_PROFIT,
        max_position_size=MAX_POSITION_SIZE,
        max_drawdown=MAX_DRAWDOWN,
        annual_trading_days=ANNUAL_TRADING_DAYS,
        transaction_cost=TRANSACTION_COST,
        env_rank=-1  # Assign a default env_rank for the checker
    )
    try:
        check_env(env_checker, warn=True)
        main_logger.info("Environment is valid!")
    except Exception as e:
        main_logger.critical(f"Environment check failed: {e}")
        exit()

    ##############################################
    # Optuna Hyperparameter Tuning
    ##############################################

    # Log Phase: Hyperparameter Tuning Starting
    log_phase("Hyperparameter Tuning", "Starting", {"total_trials": 10})

    # Optuna hyperparameter tuning with limited concurrent trials to prevent overload
    main_logger.info("Starting hyperparameter tuning with Optuna...")

    # Optuna storage using SQLite for persistence
    storage = optuna.storages.RDBStorage(
        url='sqlite:///optuna_study.db',
        engine_kwargs={'connect_args': {'check_same_thread': False}}
    )

    # Generate a unique study name
    unique_study_name = generate_unique_study_name()

    study = optuna.create_study(
        direction='maximize',
        sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED),
        storage=storage,
        study_name=unique_study_name,  # Unique Study Name
        load_if_exists=False  # Ensure a new study is created
    )
    study.optimize(
        lambda trial: objective(trial, train_df, scaler, INITIAL_BALANCE, STOP_LOSS, TAKE_PROFIT,
                               MAX_POSITION_SIZE, MAX_DRAWDOWN, ANNUAL_TRADING_DAYS, TRANSACTION_COST),
        n_trials=10,  # Adjust as needed (e.g., 50 for full tuning)
        n_jobs=4  # Limited to 4 concurrent trials to prevent system overload
    )

    if study.best_params:
        best_params = study.best_params
        main_logger.info(f"Best hyperparameters found: {best_params}")
    else:
        main_logger.critical("No successful trials found in Optuna study.")
        exit()

    # Log Phase: Hyperparameter Tuning Completed
    log_phase("Hyperparameter Tuning", "Completed", {"best_params": best_params})

    ##############################################
    # Main Training
    ##############################################

    # Assign unique env_rank for main training
    main_env_rank = 0  # Unique ID for main training

    # Create environment parameters with best reward weights
    env_params = {
        'df': train_df,
        'scaler': scaler,
        'initial_balance': INITIAL_BALANCE,
        'stop_loss': STOP_LOSS,
        'take_profit': TAKE_PROFIT,
        'max_position_size': MAX_POSITION_SIZE,
        'max_drawdown': MAX_DRAWDOWN,
        'annual_trading_days': ANNUAL_TRADING_DAYS,
        'transaction_cost': TRANSACTION_COST,
        'reward_weights': {'reward_scale': 1.0}  # Renamed to prevent confusion
    }

    # Log Phase: Main Training Starting
    log_phase("Main Training", "Starting", {"env_rank": main_env_rank, "total_timesteps": 500000, "reward_weights": env_params['reward_weights']})

    # Initialize training environment
    vec_env_train = DummyVecEnv([make_env(env_params, main_env_rank, RANDOM_SEED)])

    main_logger.info(f"Initialized DummyVecEnv with env_rank={main_env_rank} for main training.")

    # Define policy kwargs
    policy_kwargs = dict(
        activation_fn=torch.nn.ReLU,
        net_arch=[int(x) for x in best_params.get('net_arch', '128_128').split('_')]
    )

    # Define unique TensorBoard log directory for main training
    main_training_log_dir = TB_LOG_DIR / f"main_training_{main_env_rank}"
    main_training_log_dir.mkdir(parents=True, exist_ok=True)

    # Initialize PPO model with best hyperparameters and enable CPU usage
    try:
        model = PPO(
            'MlpPolicy',
            vec_env_train,
            verbose=1,  # Set verbose to 1 to enable logging
            seed=RANDOM_SEED,
            policy_kwargs=policy_kwargs,
            learning_rate=best_params.get('learning_rate', 3e-4),
            n_steps=best_params.get('n_steps', 128),
            batch_size=best_params.get('batch_size', 64),
            gamma=best_params.get('gamma', 0.99),
            gae_lambda=best_params.get('gae_lambda', 0.95),
            clip_range=best_params.get('clip_range', 0.2),
            ent_coef=best_params.get('ent_coef', 0.02),  # Increased entropy coefficient for exploration
            vf_coef=best_params.get('vf_coef', 0.5),
            max_grad_norm=best_params.get('max_grad_norm', 0.5),
            tensorboard_log=str(main_training_log_dir),
            device='cpu'  # Use CPU
        )
    except Exception as e:
        main_logger.critical(f"Model initialization failed: {e}")
        exit()

    # Define checkpoint and custom callbacks
    checkpoint_callback = CheckpointCallback(
        save_freq=50000,  # Reduced frequency to prevent overload
        save_path=str(RESULTS_DIR / "checkpoints"),
        name_prefix="ppo_model"
    )
    custom_callback = CustomTensorboardCallback()
    early_stopping_callback = EarlyStoppingCallback(
        monitor='train/reward_env',
        patience=20,  # Increased patience
        min_delta=1e-5,  # Lowered min_delta
        verbose=1
    )

    # Create a CallbackList
    callback_list = CallbackList([custom_callback, checkpoint_callback, early_stopping_callback])

    # Start training
    start_time = time.time()
    try:
        model.learn(
            total_timesteps=500000,  # Increased training steps
            callback=callback_list
        )
    except Exception as e:
        main_logger.critical(f"Training failed: {e}")
        exit()
    duration = time.time() - start_time

    # Log Phase: Main Training Completed
    log_phase("Main Training", "Completed", {"env_rank": main_env_rank, "total_timesteps": 500000}, duration)

    # Save the trained model
    model_path = RESULTS_DIR / f"ppo_model_{TICKER}.zip"
    model.save(str(model_path))
    main_logger.info(f"Model trained and saved at {model_path}")

    ##############################################
    # Testing
    ##############################################

    # Log Phase: Testing Starting
    log_phase("Testing Phase", "Starting", {"env_rank": 999, "data_points": len(test_df)})

    main_logger.info("Starting testing of PPO agent...")

    # Assign unique env_rank for testing
    test_env_rank = 999  # Unique ID for testing

    # Initialize evaluation environment (separate from training) without vectorization
    env_test = SingleStockTradingEnv(
        df=test_df,
        scaler=scaler,
        initial_balance=INITIAL_BALANCE,
        stop_loss=STOP_LOSS,
        take_profit=TAKE_PROFIT,
        max_position_size=MAX_POSITION_SIZE,
        max_drawdown=MAX_DRAWDOWN,
        annual_trading_days=ANNUAL_TRADING_DAYS,
        transaction_cost=TRANSACTION_COST,
        env_rank=test_env_rank,
        reward_weights={'reward_scale': 1.0}  # Renamed to prevent confusion
    )
    env_test.seed(RANDOM_SEED + test_env_rank)

    main_logger.info(f"Initialized SingleStockTradingEnv with env_rank={test_env_rank} for testing.")

    # Reset the environment
    try:
        obs, info = env_test.reset()
        main_logger.info(f"Environment reset successfully. Starting steps.")
    except Exception as e:
        main_logger.critical(f"Reset failed during testing: {e}")
        exit()

    done = False
    steps_taken = 0
    max_test_steps = len(test_df)  # Prevent infinite loops

    # Determine steps for first, middle, and last
    first_step = 0
    middle_step = max_test_steps // 2
    last_step = max_test_steps - 1

    while not done and steps_taken < max_test_steps:
        try:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, truncated, info = env_test.step(action)
            steps_taken += 1

            # Log first, middle, and last steps
            if steps_taken in [first_step, middle_step, last_step]:
                testing_logger.info(f"[Test Env {test_env_rank}] Step {steps_taken}: Action Taken = {action}, Reward = {reward}")
        except Exception as e:
            main_logger.critical(f"Step failed during testing: {e}")
            break

    duration = time.time() - start_time
    # Log Phase: Testing Completed
    log_phase("Testing Phase", "Completed", {"env_rank": test_env_rank, "data_points": len(test_df), "steps_taken": steps_taken}, duration)

    # Create DataFrame for RL Agent Performance from testing environment history
    if hasattr(env_test, 'history') and env_test.history:
        rl_test_df = pd.DataFrame(env_test.history)
        main_logger.info(f"Testing environment history has {len(rl_test_df)} entries.")
    else:
        main_logger.error("Testing environment does not have a 'history' attribute or it's empty.")
        rl_test_df = pd.DataFrame()

    # Ensure 'Reward' column exists
    if 'Reward' not in rl_test_df.columns:
        rl_test_df['Reward'] = 0.0
        main_logger.critical("'Reward' column not found in RL test history. Defaulting rewards to 0.")

    # Calculate Performance Metrics for Testing
    if not rl_test_df.empty:
        # Correct the column name from 'net worth' to 'Net Worth'
        if 'Net Worth' in rl_test_df.columns:
            test_final_net_worth = float(rl_test_df['Net Worth'].iloc[-1])  # Ensure float
        else:
            main_logger.error("Neither 'Net Worth' nor 'net worth' column found in RL test history.")
            test_final_net_worth = INITIAL_BALANCE

        test_profit = float(test_final_net_worth - INITIAL_BALANCE)  # Ensure float
        if 'Net Worth' in rl_test_df.columns:
            test_max_dd = calculate_max_drawdown(rl_test_df['Net Worth'])
            test_annualized_return = calculate_annualized_return(rl_test_df['Net Worth'])
        else:
            test_max_dd = 0.0
            test_annualized_return = 0.0
    else:
        test_final_net_worth = INITIAL_BALANCE
        test_profit = 0.0
        test_max_dd = 0.0
        test_annualized_return = 0.0

    # Save Test Environment History to CSV
    test_history_file = RESULTS_DIR / "test_env_history.csv"
    if not rl_test_df.empty:
        rl_test_df.to_csv(test_history_file, index=False)
        main_logger.info(f"Testing environment history saved to {test_history_file}")
    else:
        # Save an empty DataFrame or with default values if history is empty
        pd.DataFrame().to_csv(test_history_file, index=False)
        main_logger.warning(f"Testing environment history was empty. Saved empty CSV at {test_history_file}")

    ##############################################
    # Evaluate Baseline Strategies on Test Data
    ##############################################

    main_logger.info("Evaluating Baseline Strategies on Test Data...")

    # Verify that baseline_test_df has all required columns
    required_baseline_cols = ['MACD_unscaled', 'Close_unscaled', 'SMA10_unscaled', 'SMA50_unscaled',
                              'BB_Upper_unscaled', 'BB_Lower_unscaled']
    missing_baseline_cols = [col for col in required_baseline_cols if col not in baseline_test_df.columns]
    if missing_baseline_cols:
        main_logger.error(f"Missing columns in baseline_test_df required for baseline strategies: {missing_baseline_cols}")
        # Decide whether to proceed or skip certain strategies
    else:
        main_logger.debug("All required columns for baseline strategies are present in baseline_test_df.")

    # Evaluate Buy and Hold Strategy
    bh_result = buy_and_hold_with_iloc(baseline_test_df, initial_balance=INITIAL_BALANCE, transaction_cost=TRANSACTION_COST)

    # Evaluate MACD Strategy
    if 'MACD_unscaled' in baseline_test_df.columns and 'Close_unscaled' in baseline_test_df.columns:
        macd_result = macd_strategy_with_iloc(baseline_test_df, initial_balance=INITIAL_BALANCE, transaction_cost=TRANSACTION_COST, max_position_size=MAX_POSITION_SIZE)
    else:
        main_logger.warning("Skipping MACD strategy due to missing 'MACD_unscaled' or 'Close_unscaled' in baseline_test_df.")
        macd_result = {
            'Strategy': 'MACD Crossover',
            'Initial Balance': INITIAL_BALANCE,
            'Final Net Worth': 0.0,
            'Profit': 0.0
        }

    # Evaluate Moving Average Crossover Strategy
    if all(col in baseline_test_df.columns for col in ['SMA10_unscaled', 'SMA50_unscaled', 'Close_unscaled']):
        ma_crossover_result = moving_average_crossover_with_iloc(baseline_test_df, initial_balance=INITIAL_BALANCE, transaction_cost=TRANSACTION_COST, max_position_size=MAX_POSITION_SIZE)
    else:
        main_logger.warning("Skipping Moving Average Crossover strategy due to missing required columns in baseline_test_df.")
        ma_crossover_result = {
            'Strategy': 'Moving Average Crossover',
            'Initial Balance': INITIAL_BALANCE,
            'Final Net Worth': 0.0,
            'Profit': 0.0
        }

    # Evaluate Bollinger Bands Strategy
    if all(col in baseline_test_df.columns for col in ['BB_Upper_unscaled', 'BB_Lower_unscaled', 'Close_unscaled']):
        bb_result = bollinger_bands_strategy_with_iloc(baseline_test_df, initial_balance=INITIAL_BALANCE, transaction_cost=TRANSACTION_COST, max_position_size=MAX_POSITION_SIZE)
    else:
        main_logger.warning("Skipping Bollinger Bands strategy due to missing required columns in baseline_test_df.")
        bb_result = {
            'Strategy': 'Bollinger Bands',
            'Initial Balance': INITIAL_BALANCE,
            'Final Net Worth': 0.0,
            'Profit': 0.0
        }

    # Evaluate Random Strategy
    random_result = random_strategy_with_iloc(baseline_test_df, initial_balance=INITIAL_BALANCE, transaction_cost=TRANSACTION_COST, max_position_size=MAX_POSITION_SIZE)

    # Log Baseline Results
    for result in [bh_result, macd_result, ma_crossover_result, bb_result, random_result]:
        main_logger.critical(f"Strategy: {result['Strategy']}")
        main_logger.critical(f"  Initial Balance: ${result['Initial Balance']}")
        main_logger.critical(f"  Final Net Worth: ${result['Final Net Worth']:.2f}")
        main_logger.critical(f"  Profit: ${result['Profit']:.2f}")
        if 'Invested Capital' in result:
            main_logger.critical(f"  Invested Capital: ${result['Invested Capital']:.2f}")
            main_logger.critical(f"  Transaction Costs: ${result['Transaction Costs']:.2f}")
        main_logger.critical("-" * 50)

    # Log and print RL Agent Results on Test Data
    main_logger.critical("RL Agent Performance on Test Data:")
    main_logger.critical(f"  Final Net Worth: ${test_final_net_worth:.2f}")
    main_logger.critical(f"  Profit: ${test_profit:.2f}")
    main_logger.critical(f"  Annualized Return: {test_annualized_return*100:.2f}%")
    main_logger.critical(f"  Max Drawdown: {test_max_dd*100:.2f}%")

    ##############################################
    # Plotting Comparison of RL Agent vs Baseline Strategies
    ##############################################

    # Prepare baseline results
    baseline_results = [bh_result, macd_result, ma_crossover_result, bb_result, random_result]

    # Plot the comparison
    plot_comparison(test_df, rl_test_df, baseline_results, INITIAL_BALANCE, TICKER)

    ##############################################
    # Plot RL Training History
    ##############################################

    # Access training history
    training_history = pd.DataFrame(vec_env_train.envs[0].history)
    plot_rl_training_history(training_history)

    ##############################################
    # Plot Trading Results
    ##############################################

    plot_results(test_df, rl_test_df, INITIAL_BALANCE, TICKER)

    ##############################################
    # Plot Agent's Performance Metrics
    ##############################################

    plot_agent_performance(rl_test_df, TICKER)

    ##############################################
    # Plot Action Distribution
    ##############################################

    plot_action_distribution(rl_test_df)

    # Instructions for TensorBoard
    main_logger.critical("Training logs are stored for TensorBoard.")
    main_logger.critical("To view them, run the following command in your terminal:")
    main_logger.critical(f"tensorboard --logdir {TB_LOG_DIR}")
    main_logger.critical("Then open http://localhost:6006 in your browser to visualize the training metrics.")

    # Optional: Visualize Optuna study results
    try:
        import optuna.visualization as vis

        # Plot optimization history
        fig1 = vis.plot_optimization_history(study)
        fig1.savefig(PLOTS_DIR / "optuna_optimization_history.png")
        plt.close(fig1)

        # Plot parameter importances
        fig2 = vis.plot_param_importances(study)
        fig2.savefig(PLOTS_DIR / "optuna_param_importances.png")
        plt.close(fig2)
    except ImportError:
        main_logger.warning("Optuna visualization module not found. Install it via pip if you wish to visualize study results.")

    # Log Phase: All Phases Completed
    total_duration = time.time() - start_time
    log_phase("All Phases", "Completed", {"total_duration_seconds": total_duration}, total_duration)

    main_logger.info("Script execution completed successfully.")

    ##############################################
    # Post-Trial Aggregation
    ##############################################

    # Aggregate history data for visualization and comparison after all trials
    trial_results = []
    for trial in study.trials:
        result_file = RESULTS_DIR / f"trial_{trial.number}_history.csv"
        if result_file.exists():
            trial_data = pd.read_csv(result_file)
            trial_data['Trial_Number'] = trial.number
            trial_results.append(trial_data)
        else:
            main_logger.warning(f"Result file for Trial {trial.number} does not exist at {result_file}.")

    if trial_results:
        aggregated_results = pd.concat(trial_results, ignore_index=True)
        aggregated_results.to_csv(RESULTS_DIR / "aggregated_trial_history.csv", index=False)
        main_logger.info(f"Aggregated trial histories saved to {RESULTS_DIR / 'aggregated_trial_history.csv'}")

        # Optional: Plot aggregated results
        plt.figure(figsize=(14,7))
        sns.lineplot(data=aggregated_results, x=aggregated_results.index, y='Net Worth', hue='Trial_Number', legend=False, alpha=0.3)
        plt.title("Aggregated Net Worth Across All Trials")
        plt.xlabel("Step")
        plt.ylabel("Net Worth ($)")
        plt.grid(True)
        plt.tight_layout()
        plt.savefig(PLOTS_DIR / "aggregated_net_worth.png")
        plt.close()
        main_logger.info("Aggregated net worth plot generated successfully.")
    else:
        main_logger.warning("No trial results to aggregate.")
