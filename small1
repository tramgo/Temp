import os
import gymnasium as gym
from gymnasium import spaces
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.env_checker import check_env
from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback, CallbackList
import torch
import warnings
from typing import Optional, Tuple
import random
import datetime
from sklearn.preprocessing import StandardScaler
import math
import logging
from pathlib import Path
import optuna
import joblib
import time

# Import ConcurrentRotatingFileHandler for robust multi-process logging
try:
    from concurrent_log_handler import ConcurrentRotatingFileHandler
except ImportError:
    raise ImportError("Please install 'concurrent-log-handler' package via pip: pip install concurrent-log-handler")

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

# Set random seeds for reproducibility
RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)

# Define feature sets as constants
FEATURES_TO_SCALE = [
    'Close', 'SMA10', 'SMA50', 'RSI', 'MACD', 'ADX',  # Added 'ADX'
    'BB_Upper', 'BB_Lower', 'Bollinger_Width',
    'EMA20', 'VWAP', 'Lagged_Return', 'Volatility'
]

UNSCALED_FEATURES = [
    f"{feature}_unscaled" for feature in FEATURES_TO_SCALE
]

# Define directories for results and plots
BASE_DIR = Path('.').resolve()
RESULTS_DIR = BASE_DIR / 'results'
PLOTS_DIR = BASE_DIR / 'plots'
TB_LOG_DIR = BASE_DIR / 'tensorboard_logs'
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
PLOTS_DIR.mkdir(parents=True, exist_ok=True)
TB_LOG_DIR.mkdir(parents=True, exist_ok=True)

# Function to set up separate loggers
def setup_logger(name: str, log_file: Path, level=logging.INFO) -> logging.Logger:
    """
    Sets up a logger with the specified name and log file.

    Args:
        name (str): Name of the logger.
        log_file (Path): Path to the log file.
        level (int, optional): Logging level. Defaults to logging.INFO.

    Returns:
        logging.Logger: Configured logger.
    """
    logger = logging.getLogger(name)
    logger.setLevel(level)
    # Prevent adding multiple handlers to the logger
    if not logger.handlers:
        handler = ConcurrentRotatingFileHandler(str(log_file), maxBytes=10**6, backupCount=5, encoding='utf-8')
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    return logger

# Initialize separate loggers
main_logger = setup_logger('main_logger', RESULTS_DIR / 'main.log', level=logging.DEBUG)
training_logger = setup_logger('training_logger', RESULTS_DIR / 'training.log', level=logging.DEBUG)
testing_logger = setup_logger('testing_logger', RESULTS_DIR / 'testing.log', level=logging.DEBUG)
phase_logger = setup_logger('phase_logger', RESULTS_DIR / 'phase.log', level=logging.INFO)

# Log the absolute paths
main_logger.info(f"Base Directory: {BASE_DIR}")
main_logger.info(f"Results Directory: {RESULTS_DIR}")
main_logger.info(f"Plots Directory: {PLOTS_DIR}")
main_logger.info(f"TensorBoard Logs Directory: {TB_LOG_DIR}")

# Function to log phase indicators
def log_phase(phase: str, status: str = "Starting", env_details: dict = None, duration: float = None):
    """
    Logs the current phase of the program.

    Args:
        phase (str): The name of the phase (e.g., 'Hyperparameter Tuning').
        status (str, optional): The status of the phase (e.g., 'Starting', 'Completed'). Defaults to "Starting".
        env_details (dict, optional): Key-value pairs describing environment details.
        duration (float, optional): Duration of the phase in seconds.
    """
    log_message = f"***** {status} {phase} *****"
    if env_details:
        log_message += f"\nEnvironment Details: {env_details}"
    if duration is not None:
        log_message += f"\nDuration: {duration:.2f} seconds ({duration/60:.2f} minutes)"
    phase_logger.info(log_message)

# Configure Logging for Main Logger
main_logger.info("Logging has been configured with separate loggers for main, training, testing, and phases.")

##############################################
# Version Checks
##############################################

def check_versions():
    """
    Checks and logs the versions of key libraries to ensure compatibility.
    """
    import stable_baselines3
    import gymnasium
    import optuna

    sb3_version = stable_baselines3.__version__
    gymnasium_version = gymnasium.__version__
    optuna_version = optuna.__version__

    main_logger.debug(f"Stable Baselines3 version: {sb3_version}")
    main_logger.debug(f"Gymnasium version: {gymnasium_version}")
    main_logger.debug(f"Optuna version: {optuna_version}")

    # Ensure SB3 is at least version 2.0.0 for Gymnasium support
    try:
        sb3_major, sb3_minor, sb3_patch = map(int, sb3_version.split('.')[:3])
        if sb3_major < 2:
            main_logger.error("Stable Baselines3 version must be at least 2.0.0. Please upgrade SB3.")
            exit()
    except:
        main_logger.error("Unable to parse Stable Baselines3 version. Please ensure it's installed correctly.")
        exit()

    # Ensure Gymnasium is updated
    if gymnasium_version < '0.28.1':  # Example minimum version
        main_logger.warning("Consider upgrading Gymnasium to the latest version for better compatibility.")

check_versions()

##############################################
# Synthetic Data Generation
##############################################

def generate_synthetic_data(num_days: int = 1000,
                            start_price: float = 100.0,
                            mu: float = 0.0002,
                            sigma: float = 0.01,
                            seed: int = RANDOM_SEED) -> pd.DataFrame:
    """
    Generates synthetic stock price data using geometric Brownian motion.

    Args:
        num_days (int, optional): Number of trading days to simulate. Defaults to 1000.
        start_price (float, optional): Starting price of the stock. Defaults to 100.0.
        mu (float, optional): Drift coefficient. Defaults to 0.0002.
        sigma (float, optional): Volatility coefficient. Defaults to 0.01.
        seed (int, optional): Random seed for reproducibility. Defaults to RANDOM_SEED.

    Returns:
        pd.DataFrame: DataFrame containing synthetic stock data with technical indicators.
    """
    np.random.seed(seed)
    random.seed(seed)

    # Generate date range
    dates = pd.bdate_range(end=datetime.datetime.today(), periods=num_days)
    dt = 1/252  # One trading day

    # Geometric Brownian Motion
    price = [start_price]
    for _ in range(1, num_days):
        drift = (mu - 0.5 * sigma**2) * dt
        shock = sigma * np.random.normal() * np.sqrt(dt)
        price.append(price[-1] * np.exp(drift + shock))
    df = pd.DataFrame({'Date': dates, 'Close': price})

    # Generate High and Low prices based on Close price
    df['High'] = df['Close'] * (1 + np.random.uniform(0.001, 0.02, size=num_days))
    df['Low'] = df['Close'] * (1 - np.random.uniform(0.001, 0.02, size=num_days))

    # Generate Volume
    df['Volume'] = np.random.randint(1000, 10000, size=num_days)

    # Calculate Technical Indicators using the 'ta' library
    try:
        import ta
        close = df['Close']
        high = df['High']
        low = df['Low']
        volume_col = df['Volume']

        # Simple Moving Averages
        df['SMA10'] = ta.trend.SMAIndicator(close=close, window=10).sma_indicator()
        df['SMA50'] = ta.trend.SMAIndicator(close=close, window=50).sma_indicator()

        # RSI
        df['RSI'] = ta.momentum.RSIIndicator(close=close, window=14).rsi()

        # MACD
        df['MACD'] = ta.trend.MACD(close=close).macd()

        # ADX
        df['ADX'] = ta.trend.ADXIndicator(high=high, low=low, close=close, window=14).adx()

        # Bollinger Bands
        bollinger = ta.volatility.BollingerBands(close=close, window=20, window_dev=2)
        df['BB_Upper'] = bollinger.bollinger_hband()
        df['BB_Lower'] = bollinger.bollinger_lband()
        df['Bollinger_Width'] = bollinger.bollinger_wband()

        # EMA20
        df['EMA20'] = ta.trend.EMAIndicator(close=close, window=20).ema_indicator()

        # VWAP
        df['VWAP'] = ta.volume.VolumeWeightedAveragePrice(high=high, low=low, close=close, volume=volume_col, window=14).volume_weighted_average_price()

        # Lagged Return
        df['Lagged_Return'] = close.pct_change().fillna(0)

        # Volatility (ATR)
        df['Volatility'] = ta.volatility.AverageTrueRange(high=high, low=low, close=close, window=14).average_true_range()

    except Exception as e:
        main_logger.error(f"Error calculating technical indicators: {e}")
        return pd.DataFrame()

    # Add unscaled versions of relevant features
    for feature in FEATURES_TO_SCALE:
        if feature in df.columns:
            df[f"{feature}_unscaled"] = df[feature]
            main_logger.debug(f"Added column: {feature}_unscaled")
        else:
            main_logger.error(f"Feature {feature} is missing from DataFrame. Cannot create {feature}_unscaled.")
            return pd.DataFrame()

    # Handle missing values
    df.fillna(method='ffill', inplace=True)
    df.fillna(0, inplace=True)
    df.reset_index(drop=True, inplace=True)

    # Data Validation: Check for columns filled with zeros
    zero_filled_columns = df[UNSCALED_FEATURES].columns[(df[UNSCALED_FEATURES] == 0).all()].tolist()
    if zero_filled_columns:
        main_logger.error(f"One or more required columns are entirely filled with zeros: {zero_filled_columns}. Aborting data processing.")
        return pd.DataFrame()

    main_logger.info("Synthetic data generated and processed successfully.")
    return df

##############################################
# Custom Trading Environment
##############################################

class SingleStockTradingEnv(gym.Env):
    """
    A custom Gym environment for single stock trading with continuous action space.
    """
    metadata = {'render.modes': ['human']}

    def __init__(self, df: pd.DataFrame, scaler: StandardScaler,
                 initial_balance: float = 100000,
                 stop_loss: float = 0.90, take_profit: float = 1.10,
                 max_position_size: float = 0.5, max_drawdown: float = 0.20,
                 annual_trading_days: int = 252, transaction_cost: float = 0.001,
                 env_rank: int = 0,
                 reward_weights: Optional[dict] = None):
        super(SingleStockTradingEnv, self).__init__()

        self.env_rank = env_rank  # Unique identifier for the environment

        self.df = df.copy().reset_index(drop=True)
        self.scaler = scaler
        self.initial_balance = initial_balance
        self.current_step = 0
        self.stop_loss = stop_loss
        self.take_profit = take_profit
        self.max_position_size = max_position_size
        self.max_drawdown = max_drawdown
        self.annual_trading_days = annual_trading_days
        self.transaction_cost = transaction_cost  # 0.1% per trade

        # Action space: Continuous actions between -1 and 1
        # Negative values: Sell proportion of holdings
        # Positive values: Buy proportion of available balance
        # Zero: Hold
        self.action_space = spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32)

        # Observation space: features + balance, net worth, position + market phase
        self.num_features = len(FEATURES_TO_SCALE)
        self.market_phase = ['Bull', 'Bear', 'Sideways']
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf,
            shape=(self.num_features + 3 + len(self.market_phase),),
            dtype=np.float32
        )

        self.feature_names = FEATURES_TO_SCALE

        # Initialize environment state
        self.reset()

        # Initialize reward weights
        if reward_weights is not None:
            self.reward_weights = reward_weights
        else:
            self.reward_weights = {'gamma': 1.0}  # Default value to prevent AttributeError

        training_logger.debug(f"[Env {self.env_rank}] Initialized with reward_weights: {self.reward_weights}")

    def seed(self, seed=None):
        """
        Sets the seed for the environment's random number generators.

        Args:
            seed (int, optional): Seed value. Defaults to None.
        """
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        training_logger.debug(f"[Env {self.env_rank}] Seed set to {seed}")

    def _next_observation(self) -> np.ndarray:
        """
        Returns the next observation.
        """
        if self.current_step >= len(self.df):
            self.current_step = len(self.df) - 1
        current_data = self.df.iloc[self.current_step]
        features = current_data[self.feature_names].values
        obs = list(features)

        # Append balance, net worth, and position
        obs.append(self.balance / self.initial_balance)
        obs.append(self.net_worth / self.initial_balance)
        obs.append(self.position / self.initial_balance)

        # Determine market phase
        try:
            adx = float(current_data['ADX_unscaled'])  # Ensure this matches the DataFrame's feature name
        except KeyError:
            training_logger.error(f"[Env {self.env_rank}] 'ADX_unscaled' not found in current_data at step {self.current_step}. Setting ADX to 0.")
            adx = 0.0

        if adx > 25:
            try:
                sma10 = float(current_data['SMA10_unscaled'])
                sma50 = float(current_data['SMA50_unscaled'])
                if sma10 > sma50:
                    phase = 'Bull'
                else:
                    phase = 'Bear'
            except KeyError as e:
                training_logger.error(f"[Env {self.env_rank}] Missing SMA columns: {e}. Setting phase to 'Sideways'.")
                phase = 'Sideways'
        else:
            phase = 'Sideways'

        # One-hot encode market phase
        for p in self.market_phase:
            obs.append(1.0 if phase == p else 0.0)

        obs = np.array(obs, dtype=np.float32)
        if np.isnan(obs).any() or np.isinf(obs).any():
            obs = np.nan_to_num(obs, nan=0.0, posinf=0.0, neginf=0.0)

        # Sanity check
        assert obs.shape[0] == self.observation_space.shape[0], "Observation shape mismatch!"
        assert not np.isnan(obs).any(), "Observation contains NaN!"

        return obs

    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):
        """
        Resets the state of the environment to an initial state.

        Returns:
            Tuple[np.ndarray, dict]: The initial observation and an empty info dictionary.
        """
        super().reset(seed=seed)
        self.balance = self.initial_balance
        self.position = 0
        self.net_worth = self.initial_balance
        self.current_step = 0
        self.history = []
        self.prev_net_worth = self.net_worth
        self.last_action = 0.0  # Initialize last_action as Hold
        self.peak = self.net_worth
        self.returns_window = []
        self.transaction_count = 0  # Initialize transaction count
        training_logger.debug(f"[Env {self.env_rank}] Environment reset.")
        return self._next_observation(), {}

    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, dict]:
        """
        Executes one time step within the environment based on the continuous action.

        Args:
            action (np.ndarray): Action to take (array with single float between -1 and 1)

        Returns:
            Tuple containing:
            - obs (np.ndarray): Next observation.
            - reward (float): Reward obtained.
            - terminated (bool): Whether the episode has terminated.
            - truncated (bool): Whether the episode was truncated.
            - info (dict): Additional information.
        """
        training_logger.debug(f"[Env {self.env_rank}] step() called at current_step={self.current_step} with action={action}")
        try:
            # Validate action
            action_value = float(action[0])
            assert self.action_space.contains(action), f"[Env {self.env_rank}] Invalid action: {action}"
        except Exception as e:
            training_logger.error(f"[Env {self.env_rank}] Action validation failed: {e}")
            return self._next_observation(), -1000.0, True, False, {}

        # Fetch current data
        if self.current_step >= len(self.df):
            training_logger.debug(f"[Env {self.env_rank}] Current step {self.current_step} exceeds data length. Terminating episode.")
            terminated = True
            truncated = False
            reward = -1000  # Penalty for exceeding data
            obs = self._next_observation()
            
            # Append to history even if terminated
            self.history.append({
                'Action': np.nan,
                'Buy_Signal_Price': np.nan,
                'Sell_Signal_Price': np.nan,
                'Net Worth': self.net_worth,
                'Balance': self.balance,
                'Position': self.position,
                'Reward': reward
            })

            training_logger.error(f"[Env {self.env_rank}] Terminating episode at step {self.current_step} due to data overflow.")
            return obs, reward, terminated, truncated, {}

        current_data = self.df.iloc[self.current_step]
        current_price = float(current_data['Close_unscaled'])

        # Non-Linear Mapping of Action
        scaled_action = np.sign(action_value) * (abs(action_value) ** 2)

        # Interpret action
        if scaled_action > 0:
            # Buy proportion of available balance
            available_balance = self.balance
            investment_amount = available_balance * scaled_action * self.max_position_size
            shares_to_buy = math.floor(investment_amount / current_price)

            # If calculated shares_to_buy is zero but you can afford at least one share, buy one
            if shares_to_buy == 0:
                one_share_cost = current_price * (1 + self.transaction_cost)
                if one_share_cost <= self.balance:
                    shares_to_buy = 1

            # Always Check Affordability
            total_cost = shares_to_buy * current_price * (1 + self.transaction_cost)
            if shares_to_buy > 0 and total_cost <= self.balance:
                self.balance -= total_cost
                self.position += shares_to_buy
                self.transaction_count += 1
                training_logger.debug(f"[Env {self.env_rank}] Step {self.current_step}: Bought {shares_to_buy} shares at {current_price:.2f}")
            else:
                # Either can't afford or shares_to_buy is 0; no trade occurs
                training_logger.debug(f"[Env {self.env_rank}] Step {self.current_step}: Not enough balance to buy shares or no shares calculated.")

        elif scaled_action < 0:
            # Sell proportion of current holdings
            proportion_to_sell = abs(scaled_action) * self.max_position_size
            shares_to_sell = math.floor(self.position * proportion_to_sell)

            # If shares_to_sell ended up as 0 but you have shares, try selling at least 1 share
            if shares_to_sell == 0 and self.position > 0:
                shares_to_sell = 1

            # Always Check Affordability
            if shares_to_sell > 0 and shares_to_sell <= self.position:
                proceeds = shares_to_sell * current_price * (1 - self.transaction_cost)
                self.position -= shares_to_sell
                self.balance += proceeds
                self.transaction_count += 1
                training_logger.debug(f"[Env {self.env_rank}] Step {self.current_step}: Sold {shares_to_sell} shares at {current_price:.2f}")
            else:
                # Either can't sell or shares_to_sell is 0; no trade occurs
                training_logger.debug(f"[Env {self.env_rank}] Step {self.current_step}: Not enough shares to sell or shares_to_sell=0.")

        else:
            # Hold action; no operation
            training_logger.debug(f"[Env {self.env_rank}] Step {self.current_step}: Hold action received.")

        # Update net worth
        self.net_worth = self.balance + self.position * current_price

        # Calculate Reward based on net worth change
        net_worth_change = self.net_worth - self.prev_net_worth
        profit_reward = net_worth_change / self.initial_balance

        # Update returns window
        self.returns_window.append(profit_reward)
        if len(self.returns_window) > 30:
            self.returns_window.pop(0)

        # Penalty for holding
        if scaled_action == 0:
            hold_penalty = -0.001  # Small penalty for holding
        else:
            hold_penalty = 0.0

        # Penalty for high drawdown
        self.peak = max(self.peak, self.net_worth)
        current_drawdown = (self.peak - self.net_worth) / self.peak if self.peak > 0 else 0.0
        if current_drawdown > self.max_drawdown:
            drawdown_penalty = -0.01  # Significant penalty
        else:
            drawdown_penalty = 0.0

        # Total Reward
        reward = (profit_reward + hold_penalty + drawdown_penalty) * (self.reward_weights['gamma'] if self.reward_weights else 1.0)

        # Normalize the reward
        normalized_reward = reward

        # Append to history with updated reward
        self.history.append({
            'Action': action_value,
            'Buy_Signal_Price': current_price if scaled_action > 0 else np.nan,
            'Sell_Signal_Price': current_price if scaled_action < 0 else np.nan,
            'Net Worth': self.net_worth,
            'Balance': self.balance,
            'Position': self.position,
            'Reward': normalized_reward
        })

        # Check for episode termination
        terminated = False
        truncated = False

        if self.net_worth <= 0:
            terminated = True
            normalized_reward -= 1.0  # Severe penalty for bankruptcy
            training_logger.error(f"[Env {self.env_rank}] Bankruptcy occurred. Terminating episode at step {self.current_step}.")
        elif self.current_step >= len(self.df) - 1:
            terminated = True
            training_logger.info(f"[Env {self.env_rank}] Reached end of data at step {self.current_step}. Terminating episode.")

        # Advance to next step if not terminated
        if not terminated:
            self.prev_net_worth = self.net_worth
            training_logger.debug(f"[Env {self.env_rank}] Before increment: Step {self.current_step}")
            self.current_step += 1  # Ensure step increments
            training_logger.debug(f"[Env {self.env_rank}] After increment: Step {self.current_step}")
        else:
            # If terminated, do not increment step to prevent overshooting
            training_logger.debug(f"[Env {self.env_rank}] Episode terminated at step {self.current_step}")

        # Ensure current_step does not exceed data length
        self.current_step = min(self.current_step, len(self.df) - 1)

        # Observation after action
        obs = self._next_observation()

        # Logging every 100 steps or upon termination
        if self.current_step % 100 == 0 or terminated:
            training_logger.debug(f"[Env {self.env_rank}] Step {self.current_step}: Reward = {normalized_reward:.4f}, "
                                  f"Net Worth = {self.net_worth:.2f}, Balance = {self.balance:.2f}, Position = {self.position}")
            training_logger.info(f"[Env {self.env_rank}] Step {self.current_step}: Reward = {normalized_reward:.4f}, "
                                f"Net Worth = {self.net_worth:.2f}, Balance = {self.balance:.2f}, Position = {self.position}")

        # Log detailed state changes
        training_logger.debug(f"[Env {self.env_rank}] After Action {action_value}: Balance = {self.balance}, Position = {self.position}, Net Worth = {self.net_worth}")

        return obs, normalized_reward, terminated, truncated, {}

##############################################
# Baseline Strategies
##############################################

def buy_and_hold(df: pd.DataFrame, initial_balance: float = 100000, transaction_cost: float = 0.001) -> dict:
    """
    Implements a Buy and Hold strategy.

    Args:
        df (pd.DataFrame): DataFrame containing stock prices.
        initial_balance (float): Starting balance.
        transaction_cost (float): Transaction cost per trade.

    Returns:
        dict: Results of the strategy.
    """
    # Invest the entire initial balance
    investment_percentage = 1.0  # 100% investment
    investment_amount = initial_balance * investment_percentage

    buy_price = df.iloc[0]['Close_unscaled']
    shares_to_buy = math.floor(investment_amount / buy_price)
    invested_capital = shares_to_buy * buy_price
    cost = shares_to_buy * buy_price * transaction_cost
    balance = initial_balance - invested_capital - cost  # Remaining balance after buying
    net_worth = balance + shares_to_buy * df.iloc[-1]['Close_unscaled']
    profit = net_worth - initial_balance
    return {
        'Strategy': 'Buy and Hold',
        'Initial Balance': initial_balance,
        'Final Net Worth': net_worth,
        'Profit': profit,
        'Invested Capital': invested_capital,
        'Transaction Costs': cost
    }

def moving_average_crossover(df: pd.DataFrame, initial_balance: float = 100000, transaction_cost: float = 0.001, max_position_size: float = 0.5) -> dict:
    """
    Implements a Moving Average Crossover strategy.

    Args:
        df (pd.DataFrame): DataFrame containing stock prices and indicators.
        initial_balance (float): Starting balance.
        transaction_cost (float): Transaction cost per trade.
        max_position_size (float): Maximum position size as a fraction of net worth.

    Returns:
        dict: Results of the strategy.
    """
    balance = initial_balance
    holdings = 0
    net_worth = initial_balance

    # Drop rows with NaN values in required columns
    required_cols = ['SMA10_unscaled', 'SMA50_unscaled', 'Close_unscaled']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        main_logger.error(f"Missing columns in DataFrame for MA Crossover strategy: {missing_cols}")
        return {
            'Strategy': 'Moving Average Crossover',
            'Initial Balance': initial_balance,
            'Final Net Worth': net_worth,
            'Profit': 0.0
        }

    df = df.dropna(subset=required_cols)

    for i in range(1, len(df)):
        # Golden cross
        if (df.loc[i-1, 'SMA10_unscaled'] < df.loc[i-1, 'SMA50_unscaled']) and (df.loc[i, 'SMA10_unscaled'] >= df.loc[i, 'SMA50_unscaled']):
            # Buy
            shares_to_buy = math.floor(max_position_size * balance / df.loc[i, 'Close_unscaled'])
            # If calculated shares_to_buy is zero but you can afford at least one share, buy one
            if shares_to_buy == 0:
                one_share_cost = df.loc[i, 'Close_unscaled'] * (1 + transaction_cost)
                if one_share_cost <= balance:
                    shares_to_buy = 1

            # Always Check Affordability
            total_cost = shares_to_buy * df.loc[i, 'Close_unscaled'] * (1 + transaction_cost)
            if shares_to_buy > 0 and total_cost <= balance:
                balance -= total_cost
                holdings += shares_to_buy
                training_logger.debug(f"[Strategy: MA Crossover] Bought {shares_to_buy} shares at {df.loc[i, 'Close_unscaled']:.2f} on step {i}")
            else:
                # Either can't afford or shares_to_buy is 0; no trade occurs
                training_logger.debug(f"[Strategy: MA Crossover] Not enough balance to buy shares or no shares calculated.")

        # Death cross
        elif (df.loc[i-1, 'SMA10_unscaled'] > df.loc[i-1, 'SMA50_unscaled']) and (df.loc[i, 'SMA10_unscaled'] <= df.loc[i, 'SMA50_unscaled']):
            # Sell
            shares_to_sell = math.floor(0.5 * holdings)

            # If shares_to_sell ended up as 0 but you have shares, try selling at least 1 share
            if shares_to_sell == 0 and holdings > 0:
                shares_to_sell = 1

            # Always Check Affordability
            if shares_to_sell > 0 and shares_to_sell <= holdings:
                proceeds = shares_to_sell * df.loc[i, 'Close_unscaled'] * (1 - transaction_cost)
                holdings -= shares_to_sell
                balance += proceeds
                training_logger.debug(f"[Strategy: MA Crossover] Sold {shares_to_sell} shares at {df.loc[i, 'Close_unscaled']:.2f} on step {i}")
            else:
                # Either can't sell or shares_to_sell is 0; no trade occurs
                training_logger.debug(f"[Strategy: MA Crossover] Not enough shares to sell or shares_to_sell=0.")

        net_worth = balance + holdings * df.loc[i, 'Close_unscaled']

    profit = net_worth - initial_balance
    return {
        'Strategy': 'Moving Average Crossover',
        'Initial Balance': initial_balance,
        'Final Net Worth': net_worth,
        'Profit': profit
    }

def macd_strategy(df: pd.DataFrame, initial_balance: float = 100000, transaction_cost: float = 0.001, max_position_size: float = 0.5) -> dict:
    """
    Implements a MACD strategy.

    Args:
        df (pd.DataFrame): DataFrame containing stock prices and MACD indicators.
        initial_balance (float): Starting balance.
        transaction_cost (float): Transaction cost per trade.
        max_position_size (float): Maximum position size as a fraction of net worth.

    Returns:
        dict: Results of the strategy.
    """
    balance = initial_balance
    holdings = 0
    net_worth = initial_balance

    # Drop rows with NaN values in required columns
    required_cols = ['MACD_unscaled', 'Close_unscaled']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        main_logger.error(f"Missing columns in DataFrame for MACD strategy: {missing_cols}")
        return {
            'Strategy': 'MACD Crossover',
            'Initial Balance': initial_balance,
            'Final Net Worth': net_worth,
            'Profit': 0.0
        }

    df = df.dropna(subset=required_cols)

    for i in range(1, len(df)):
        # MACD crossover
        if (df.loc[i-1, 'MACD_unscaled'] < 0) and (df.loc[i, 'MACD_unscaled'] >= 0):
            # Buy
            shares_to_buy = math.floor(max_position_size * balance / df.loc[i, 'Close_unscaled'])
            # If calculated shares_to_buy is zero but you can afford at least one share, buy one
            if shares_to_buy == 0:
                one_share_cost = df.loc[i, 'Close_unscaled'] * (1 + transaction_cost)
                if one_share_cost <= balance:
                    shares_to_buy = 1

            # Always Check Affordability
            total_cost = shares_to_buy * df.loc[i, 'Close_unscaled'] * (1 + transaction_cost)
            if shares_to_buy > 0 and total_cost <= balance:
                balance -= total_cost
                holdings += shares_to_buy
                training_logger.debug(f"[Strategy: MACD Crossover] Bought {shares_to_buy} shares at {df.loc[i, 'Close_unscaled']:.2f} on step {i}")
            else:
                # Either can't afford or shares_to_buy is 0; no trade occurs
                training_logger.debug(f"[Strategy: MACD Crossover] Not enough balance to buy shares or no shares calculated.")

        elif (df.loc[i-1, 'MACD_unscaled'] > 0) and (df.loc[i, 'MACD_unscaled'] <= 0):
            # Sell
            shares_to_sell = math.floor(0.5 * holdings)

            # If shares_to_sell ended up as 0 but you have shares, try selling at least 1 share
            if shares_to_sell == 0 and holdings > 0:
                shares_to_sell = 1

            # Always Check Affordability
            if shares_to_sell > 0 and shares_to_sell <= holdings:
                proceeds = shares_to_sell * df.loc[i, 'Close_unscaled'] * (1 - transaction_cost)
                holdings -= shares_to_sell
                balance += proceeds
                training_logger.debug(f"[Strategy: MACD Crossover] Sold {shares_to_sell} shares at {df.loc[i, 'Close_unscaled']:.2f} on step {i}")
            else:
                # Either can't sell or shares_to_sell is 0; no trade occurs
                training_logger.debug(f"[Strategy: MACD Crossover] Not enough shares to sell or shares_to_sell=0.")

        net_worth = balance + holdings * df.loc[i, 'Close_unscaled']

    profit = net_worth - initial_balance
    return {
        'Strategy': 'MACD Crossover',
        'Initial Balance': initial_balance,
        'Final Net Worth': net_worth,
        'Profit': profit
    }

def bollinger_bands_strategy(df: pd.DataFrame, initial_balance: float = 100000, transaction_cost: float = 0.001, max_position_size: float = 0.5) -> dict:
    """
    Implements a Bollinger Bands strategy.

    Args:
        df (pd.DataFrame): DataFrame containing stock prices and Bollinger Bands.
        initial_balance (float): Starting balance.
        transaction_cost (float): Transaction cost per trade.
        max_position_size (float): Maximum position size as a fraction of net worth.

    Returns:
        dict: Results of the strategy.
    """
    balance = initial_balance
    holdings = 0
    net_worth = initial_balance

    # Drop rows with NaN values in required columns
    required_cols = ['BB_Upper_unscaled', 'BB_Lower_unscaled', 'Close_unscaled']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        main_logger.error(f"Missing columns in DataFrame for Bollinger Bands strategy: {missing_cols}")
        return {
            'Strategy': 'Bollinger Bands',
            'Initial Balance': initial_balance,
            'Final Net Worth': net_worth,
            'Profit': 0.0
        }

    df = df.dropna(subset=required_cols)

    for i in range(len(df)):
        # Buy when price crosses below lower band
        if df.loc[i, 'Close_unscaled'] < df.loc[i, 'BB_Lower_unscaled']:
            shares_to_buy = math.floor(max_position_size * balance / df.loc[i, 'Close_unscaled'])
            # If calculated shares_to_buy is zero but you can afford at least one share, buy one
            if shares_to_buy == 0:
                one_share_cost = df.loc[i, 'Close_unscaled'] * (1 + transaction_cost)
                if one_share_cost <= balance:
                    shares_to_buy = 1

            # Always Check Affordability
            total_cost = shares_to_buy * df.loc[i, 'Close_unscaled'] * (1 + transaction_cost)
            if shares_to_buy > 0 and total_cost <= balance:
                balance -= total_cost
                holdings += shares_to_buy
                training_logger.debug(f"[Strategy: Bollinger Bands] Bought {shares_to_buy} shares at {df.loc[i, 'Close_unscaled']:.2f} on step {i}")
            else:
                # Either can't afford or shares_to_buy is 0; no trade occurs
                training_logger.debug(f"[Strategy: Bollinger Bands] Not enough balance to buy shares or no shares calculated.")

        # Sell when price crosses above upper band
        elif df.loc[i, 'Close_unscaled'] > df.loc[i, 'BB_Upper_unscaled']:
            shares_to_sell = math.floor(0.5 * holdings)

            # If shares_to_sell ended up as 0 but you have shares, try selling at least 1 share
            if shares_to_sell == 0 and holdings > 0:
                shares_to_sell = 1

            # Always Check Affordability
            if shares_to_sell > 0 and shares_to_sell <= holdings:
                proceeds = shares_to_sell * df.loc[i, 'Close_unscaled'] * (1 - transaction_cost)
                holdings -= shares_to_sell
                balance += proceeds
                training_logger.debug(f"[Strategy: Bollinger Bands] Sold {shares_to_sell} shares at {df.loc[i, 'Close_unscaled']:.2f} on step {i}")
            else:
                # Either can't sell or shares_to_sell is 0; no trade occurs
                training_logger.debug(f"[Strategy: Bollinger Bands] Not enough shares to sell or shares_to_sell=0.")

        net_worth = balance + holdings * df.loc[i, 'Close_unscaled']

    profit = net_worth - initial_balance
    return {
        'Strategy': 'Bollinger Bands',
        'Initial Balance': initial_balance,
        'Final Net Worth': net_worth,
        'Profit': profit
    }

def random_strategy(df: pd.DataFrame, initial_balance: float = 100000, transaction_cost: float = 0.001, max_position_size: float = 0.5) -> dict:
    """
    Implements a Random strategy.

    Args:
        df (pd.DataFrame): DataFrame containing stock prices.
        initial_balance (float): Starting balance.
        transaction_cost (float): Transaction cost per trade.
        max_position_size (float): Maximum position size as a fraction of net worth.

    Returns:
        dict: Results of the strategy.
    """
    balance = initial_balance
    holdings = 0
    net_worth = initial_balance

    # Drop rows with NaN values in required columns
    required_cols = ['Close_unscaled']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        main_logger.error(f"Missing columns in DataFrame for Random strategy: {missing_cols}")
        return {
            'Strategy': 'Random Strategy',
            'Initial Balance': initial_balance,
            'Final Net Worth': net_worth,
            'Profit': 0.0
        }

    df = df.dropna(subset=required_cols)

    for i in range(len(df)):
        action = random.choice(['Buy', 'Sell', 'Hold'])
        if action == 'Buy':
            shares_to_buy = math.floor(max_position_size * balance / df.loc[i, 'Close_unscaled'])
            # If calculated shares_to_buy is zero but you can afford at least one share, buy one
            if shares_to_buy == 0:
                one_share_cost = df.loc[i, 'Close_unscaled'] * (1 + transaction_cost)
                if one_share_cost <= balance:
                    shares_to_buy = 1

            # Always Check Affordability
            total_cost = shares_to_buy * df.loc[i, 'Close_unscaled'] * (1 + transaction_cost)
            if shares_to_buy > 0 and total_cost <= balance:
                balance -= total_cost
                holdings += shares_to_buy
                training_logger.debug(f"[Strategy: Random] Bought {shares_to_buy} shares at {df.loc[i, 'Close_unscaled']:.2f} on step {i}")
            else:
                # Either can't afford or shares_to_buy is 0; no trade occurs
                training_logger.debug(f"[Strategy: Random] Not enough balance to buy shares or no shares calculated.")

        elif action == 'Sell':
            shares_to_sell = math.floor(0.5 * holdings)

            # If shares_to_sell ended up as 0 but you have shares, try selling at least 1 share
            if shares_to_sell == 0 and holdings > 0:
                shares_to_sell = 1

            # Always Check Affordability
            if shares_to_sell > 0 and shares_to_sell <= holdings:
                proceeds = shares_to_sell * df.loc[i, 'Close_unscaled'] * (1 - transaction_cost)
                holdings -= shares_to_sell
                balance += proceeds
                training_logger.debug(f"[Strategy: Random] Sold {shares_to_sell} shares at {df.loc[i, 'Close_unscaled']:.2f} on step {i}")
            else:
                # Either can't sell or shares_to_sell is 0; no trade occurs
                training_logger.debug(f"[Strategy: Random] Not enough shares to sell or shares_to_sell=0.")

        # If Hold, do nothing
        net_worth = balance + holdings * df.loc[i, 'Close_unscaled']

    profit = net_worth - initial_balance
    return {
        'Strategy': 'Random Strategy',
        'Initial Balance': initial_balance,
        'Final Net Worth': net_worth,
        'Profit': profit
    }

##############################################
# Evaluation and Plotting Functions
##############################################

def plot_rl_training_history(rl_df: pd.DataFrame):
    """
    Plots the RL agent's net worth and rewards over time.

    Args:
        rl_df (pd.DataFrame): DataFrame containing the RL agent's trading history.
    """
    if rl_df.empty:
        main_logger.error("RL history is empty. Cannot plot training history.")
        return

    plt.figure(figsize=(14,7))

    # Plot Net Worth
    plt.subplot(2, 1, 1)
    plt.plot(rl_df.index, rl_df['Net Worth'], label='Net Worth', color='blue')
    plt.title('RL Agent Net Worth Over Time')
    plt.xlabel('Step')
    plt.ylabel('Net Worth ($)')
    plt.legend()
    plt.grid(True)

    # Plot Reward
    plt.subplot(2, 1, 2)
    plt.plot(rl_df.index, rl_df['Reward'], label='Reward', color='green')
    plt.title('RL Agent Reward Over Time')
    plt.xlabel('Step')
    plt.ylabel('Reward')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.savefig(PLOTS_DIR / "rl_training_history.png")
    plt.show()
    main_logger.info("RL training history plotted successfully.")

def plot_results(df: pd.DataFrame, rl_df: pd.DataFrame, ticker: str):
    """
    Plots trading actions and net worth.

    Args:
        df (pd.DataFrame): Stock data.
        rl_df (pd.DataFrame): RL agent's trading history.
        ticker (str): Stock ticker.
    """
    if rl_df.empty:
        main_logger.critical("RL history is empty. Skipping plots.")
        return

    # Ensure rl_df and df are aligned
    min_length = min(len(df), len(rl_df))
    aligned_df = df.iloc[:min_length].reset_index(drop=True)
    aligned_rl_df = rl_df.iloc[:min_length].reset_index(drop=True)

    fig, axs = plt.subplots(2, 1, figsize=(14, 12))

    # Plot Price with Buy/Sell Signals using unscaled data
    axs[0].plot(aligned_df['Date'], aligned_df['Close_unscaled'], label='Close Price (Unscaled)', color='blue', alpha=0.6)

    # Plot Buy Signals
    buy_signals = aligned_rl_df[aligned_rl_df['Buy_Signal_Price'].notna()]
    if not buy_signals.empty:
        axs[0].scatter(buy_signals['Date'], buy_signals['Buy_Signal_Price'], color='green', marker='^', s=100, label='Buy Signal')

    # Plot Sell Signals
    sell_signals = aligned_rl_df[aligned_rl_df['Sell_Signal_Price'].notna()]
    if not sell_signals.empty:
        axs[0].scatter(sell_signals['Date'], sell_signals['Sell_Signal_Price'], color='red', marker='v', s=100, label='Sell Signal')

    axs[0].set_title(f'{ticker} Price with Buy/Sell Signals (Unscaled)', fontsize=16)
    axs[0].set_xlabel('Date', fontsize=14)
    axs[0].set_ylabel('Price ($)', fontsize=14)
    axs[0].legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=12)
    axs[0].tick_params(axis='both', which='major', labelsize=12)
    axs[0].grid(True)

    # Plot Net Worth
    axs[1].plot(aligned_rl_df['Date'], aligned_rl_df['Net Worth'], label='Net Worth', color='blue')
    axs[1].set_title(f'{ticker} RL Agent Net Worth Over Time', fontsize=16)
    axs[1].set_xlabel('Date', fontsize=14)
    axs[1].set_ylabel('Net Worth ($)', fontsize=14)
    axs[1].legend(fontsize=12)
    axs[1].tick_params(axis='both', which='major', labelsize=12)
    axs[1].grid(True)

    plt.tight_layout()
    plt.savefig(PLOTS_DIR / f"{ticker}_combined_plot.png")
    plt.close()

    main_logger.critical(f"Combined plots saved in {PLOTS_DIR}")

def plot_agent_performance(rl_df: pd.DataFrame, ticker: str):
    """
    Plots the RL agent's performance metrics over time.

    Args:
        rl_df (pd.DataFrame): DataFrame containing the RL agent's trading history.
        ticker (str): Stock ticker.
    """
    if rl_df.empty:
        main_logger.error("RL history is empty. Cannot plot agent performance.")
        return

    plt.figure(figsize=(15, 10))

    # Plot Net Worth
    plt.subplot(3, 1, 1)
    plt.plot(rl_df.index, rl_df['Net Worth'], label='Net Worth', color='blue')
    plt.title(f'{ticker} RL Agent Net Worth Over Time')
    plt.xlabel('Step')
    plt.ylabel('Net Worth ($)')
    plt.legend()
    plt.grid(True)

    # Plot Reward
    plt.subplot(3, 1, 2)
    plt.plot(rl_df.index, rl_df['Reward'], label='Reward', color='green')
    plt.title('Reward Over Time')
    plt.xlabel('Step')
    plt.ylabel('Reward')
    plt.legend()
    plt.grid(True)

    # Plot Position
    plt.subplot(3, 1, 3)
    plt.plot(rl_df.index, rl_df['Position'], label='Position', color='red')
    plt.title('Position Over Time')
    plt.xlabel('Step')
    plt.ylabel('Position (Shares)')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.savefig(PLOTS_DIR / f"{ticker}_agent_performance.png")
    plt.show()
    main_logger.info("Agent performance plots generated successfully.")

def plot_action_distribution(rl_df: pd.DataFrame):
    """
    Plots the distribution of actions taken by the agent.

    Args:
        rl_df (pd.DataFrame): DataFrame containing the RL agent's trading history.
    """
    if rl_df.empty:
        main_logger.error("RL history is empty. Cannot plot action distribution.")
        return

    if 'Action' not in rl_df.columns:
        main_logger.error("'Action' column not found in RL history. Cannot plot action distribution.")
        return

    # Categorize actions
    conditions = [
        (rl_df['Action'] > 0.05),
        (rl_df['Action'] < -0.05),
        (rl_df['Action'].between(-0.05, 0.05))
    ]
    choices = ['Buy', 'Sell', 'Hold']
    rl_df['Action_Category'] = np.select(conditions, choices, default='Hold')

    action_counts = rl_df['Action_Category'].value_counts().reindex(['Hold', 'Buy', 'Sell']).fillna(0)

    plt.figure(figsize=(8,6))
    plt.bar(action_counts.index, action_counts.values, color=['grey', 'green', 'red'])
    plt.title('Action Distribution')
    plt.xlabel('Action')
    plt.ylabel('Count')
    plt.show()
    main_logger.info("Action distribution plot generated successfully.")

##############################################
# Callbacks
##############################################

class EarlyStoppingCallback(BaseCallback):
    """
    Custom callback for implementing early stopping based on the normalized reward.
    Stops training if the normalized reward does not improve for a given number of evaluations (patience).
    """
    def __init__(self, monitor='train/reward_env', patience=20, min_delta=1e-5, verbose=1):
        super(EarlyStoppingCallback, self).__init__(verbose)
        self.monitor = monitor
        self.patience = patience
        self.min_delta = min_delta
        self.best_reward = -np.inf
        self.wait = 0

    def _on_step(self) -> bool:
        # Retrieve the latest value of the monitored metric
        current_reward = self.logger.name_to_value.get(self.monitor, None)

        if current_reward is None:
            if self.verbose > 0:
                print(f"EarlyStoppingCallback: Metric '{self.monitor}' not found.")
            return True  # Continue training

        if current_reward > self.best_reward + self.min_delta:
            self.best_reward = current_reward
            self.wait = 0
            if self.verbose > 0:
                print(f"EarlyStoppingCallback: Reward improved to {self.best_reward:.4f}. Resetting wait counter.")
        else:
            self.wait += 1
            if self.verbose > 0:
                print(f"EarlyStoppingCallback: No improvement in reward. Wait counter: {self.wait}/{self.patience}")
            if self.wait >= self.patience:
                if self.verbose > 0:
                    print("EarlyStoppingCallback: Patience exceeded. Stopping training.")
                return False  # Stop training

        return True  # Continue training

class CustomTensorboardCallback(BaseCallback):
    """
    Custom callback for logging additional metrics to TensorBoard.
    Logs net worth, reward, and elapsed time.
    """
    def __init__(self, verbose=0):
        super(CustomTensorboardCallback, self).__init__(verbose)
        self.start_time = None

    def _on_training_start(self) -> None:
        self.start_time = time.time()

    def _on_step(self) -> bool:
        # Access the environment
        env = self.training_env.envs[0]

        # Log net worth and reward if history is not empty
        if hasattr(env, 'history') and env.history:
            last_history = env.history[-1]
            self.logger.record("train/net_worth_env", last_history.get('Net Worth', 0.0))
            self.logger.record("train/balance_env", last_history.get('Balance', 0.0))
            self.logger.record("train/position_env", last_history.get('Position', 0.0))
            self.logger.record("train/reward_env", last_history.get('Reward', 0.0))

        # Log elapsed time
        if self.start_time:
            elapsed_time = time.time() - self.start_time  # in seconds
            elapsed_time_formatted = time.strftime("%H:%M:%S", time.gmtime(elapsed_time))
            self.logger.record("train/elapsed_time_env", elapsed_time)
            self.logger.record("train/elapsed_time_formatted_env", elapsed_time_formatted)

        return True

def calculate_max_drawdown(net_worth_series: pd.Series) -> float:
    """
    Calculates the Maximum Drawdown of a net worth series.

    Args:
        net_worth_series (pd.Series): Series of net worth over time.

    Returns:
        float: Maximum drawdown value.
    """
    rolling_max = net_worth_series.cummax()
    drawdown = (net_worth_series - rolling_max) / rolling_max
    return drawdown.min()

def calculate_annualized_return(net_worth_series: pd.Series, periods_per_year: int = 252) -> float:
    """
    Calculates the Annualized Return (CAGR).

    Args:
        net_worth_series (pd.Series): Series of net worth over time.
        periods_per_year (int): Number of trading periods in a year.

    Returns:
        float: Annualized return.
    """
    start_value = net_worth_series.iloc[0]
    end_value = net_worth_series.iloc[-1]
    num_periods = len(net_worth_series)
    if num_periods == 0:
        return 0.0
    return (end_value / start_value) ** (periods_per_year / num_periods) - 1

##############################################
# Optuna Hyperparameter Tuning
##############################################

def generate_unique_study_name(base_name='rl_trading_agent_study'):
    """
    Generates a unique study name by appending the current timestamp.

    Args:
        base_name (str): The base name for the study.

    Returns:
        str: A unique study name.
    """
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"{base_name}_{timestamp}"

def objective(trial, df, scaler, initial_balance, stop_loss, take_profit, max_position_size, max_drawdown, annual_trading_days, transaction_cost):
    """
    Objective function for Optuna to maximize final performance metric based on final net worth.

    This version logs all individual properties of the environment at the end of each trial
    for comprehensive debugging and analysis.
    """
    # Define hyperparameter search space    
    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)
    n_steps = trial.suggest_categorical('n_steps', [128, 256, 512])
    batch_size = trial.suggest_categorical('batch_size', [32, 64])
    gamma = trial.suggest_uniform('gamma', 0.90, 0.99)
    gae_lambda = trial.suggest_uniform('gae_lambda', 0.80, 1.00)
    clip_range = trial.suggest_uniform('clip_range', 0.1, 0.3)
    ent_coef = trial.suggest_loguniform('ent_coef', 1e-5, 1e-3)
    vf_coef = trial.suggest_uniform('vf_coef', 0.1, 0.5)
    max_grad_norm = trial.suggest_uniform('max_grad_norm', 0.5, 1.0)
    net_arch = trial.suggest_categorical('net_arch', ['128_128', '256_256', '128_256_128'])

    # Initialize environment with these hyperparameters
    env = SingleStockTradingEnv(
        df=df,
        scaler=scaler,
        initial_balance=initial_balance,
        stop_loss=stop_loss,
        take_profit=take_profit,
        max_position_size=max_position_size,
        max_drawdown=max_drawdown,
        annual_trading_days=annual_trading_days,
        transaction_cost=transaction_cost,
        env_rank=trial.number + 1,  # Unique identifier for the environment
        reward_weights={'gamma': 1.0}  # Simplified reward weights
    )
    env.seed(RANDOM_SEED + trial.number + 1)

    vec_env = DummyVecEnv([lambda: env])

    # Define policy network architecture
    policy_kwargs = dict(
        activation_fn=torch.nn.ReLU,
        net_arch=[int(x) for x in net_arch.split('_')]
    )

    # Define unique TensorBoard log directory for the trial
    trial_log_dir = TB_LOG_DIR / f"trial_{trial.number}"
    trial_log_dir.mkdir(parents=True, exist_ok=True)

    # Initialize PPO model
    model = PPO(
        'MlpPolicy',
        vec_env,
        verbose=0,
        seed=RANDOM_SEED,
        policy_kwargs=policy_kwargs,
        learning_rate=learning_rate,
        n_steps=n_steps,
        batch_size=batch_size,
        gamma=gamma,
        gae_lambda=gae_lambda,
        clip_range=clip_range,
        ent_coef=ent_coef,
        vf_coef=vf_coef,
        max_grad_norm=max_grad_norm,
        tensorboard_log=str(trial_log_dir),
        device='cpu'
    )

    # Define unique checkpoint directory per trial
    trial_checkpoint_dir = RESULTS_DIR / f"checkpoints_trial_{trial.number}"
    trial_checkpoint_dir.mkdir(parents=True, exist_ok=True)
    checkpoint_callback = CheckpointCallback(
        save_freq=50000,  # Reduced frequency to prevent overload
        save_path=str(trial_checkpoint_dir),
        name_prefix="ppo_model"
    )
    custom_callback = CustomTensorboardCallback()
    early_stopping_callback = EarlyStoppingCallback(
        monitor='train/reward_env',
        patience=20,
        min_delta=1e-5,
        verbose=1
    )
    callback_list = CallbackList([custom_callback, checkpoint_callback, early_stopping_callback])

    # Train the model
    try:
        model.learn(
            total_timesteps=500000,
            callback=callback_list
        )
    except Exception as e:
        main_logger.critical(f"[Trial {trial.number}] Training failed: {e}")
        return -np.inf  # Assign a very low value to failed trials

    # Evaluate the trained model using final net worth metric
    try:
        obs = vec_env.reset()
    except Exception as e:
        main_logger.critical(f"[Trial {trial.number}] Reset failed during evaluation: {e}")
        return -np.inf

    done = False
    steps_taken = 0
    while not done and steps_taken < 1000:  # Prevent infinite loops
        try:
            action, _ = model.predict(obs, deterministic=True)
            obs, rewards_step, dones, infos = vec_env.step(action)
            done = dones[0]
            steps_taken += 1
        except Exception as e:
            main_logger.critical(f"[Trial {trial.number}] Step failed during evaluation: {e}")
            break

    # Now that the evaluation episode ended, access the final net worth and other properties
    eval_env = vec_env.envs[0]
    final_net_worth = eval_env.net_worth
    final_balance = eval_env.balance
    final_position = eval_env.position
    final_transaction_count = eval_env.transaction_count
    final_peak = eval_env.peak
    final_drawdown = (final_peak - final_net_worth) / final_peak if final_peak > 0 else 0.0

    # Calculate combined ratio
    combined_ratio = (final_net_worth - initial_balance) / initial_balance

    # Log all individual properties
    main_logger.critical(f"[Trial {trial.number}] Combined Ratio: {combined_ratio:.4f}")
    main_logger.critical(f"[Trial {trial.number}] Final Net Worth: ${final_net_worth:.2f}")
    main_logger.critical(f"[Trial {trial.number}] Final Balance: ${final_balance:.2f}")
    main_logger.critical(f"[Trial {trial.number}] Final Position: {final_position} shares")
    main_logger.critical(f"[Trial {trial.number}] Total Transactions: {final_transaction_count}")
    main_logger.critical(f"[Trial {trial.number}] Final Peak Net Worth: ${final_peak:.2f}")
    main_logger.critical(f"[Trial {trial.number}] Final Drawdown: {final_drawdown*100:.2f}%")

    # Optionally, log additional properties from the history
    if hasattr(eval_env, 'history') and eval_env.history:
        last_history = eval_env.history[-1]
        history_details = "\n".join([f"{key}: {value}" for key, value in last_history.items()])
        main_logger.critical(f"[Trial {trial.number}] Last Step History:\n{history_details}")
    else:
        main_logger.warning(f"[Trial {trial.number}] Environment history is empty or not available.")

    return combined_ratio

##############################################
# Main Execution
##############################################

def make_env(env_params, env_rank, seed=RANDOM_SEED):
    """
    Creates and returns a callable that initializes the SingleStockTradingEnv.

    Args:
        env_params (dict): Parameters to initialize the environment.
        env_rank (int): Unique identifier for the environment.
        seed (int): Random seed.

    Returns:
        callable: A function that creates and returns a SingleStockTradingEnv instance when called.
    """
    def _init():
        env_instance = SingleStockTradingEnv(
            df=env_params['df'],
            scaler=env_params['scaler'],
            initial_balance=env_params['initial_balance'],
            stop_loss=env_params['stop_loss'],
            take_profit=env_params['take_profit'],
            max_position_size=env_params['max_position_size'],
            max_drawdown=env_params['max_drawdown'],
            annual_trading_days=env_params['annual_trading_days'],
            transaction_cost=env_params['transaction_cost'],
            env_rank=env_rank,
            reward_weights=env_params.get('reward_weights', None)
        )
        env_instance.seed(seed + env_rank)
        return env_instance
    return _init

if __name__ == "__main__":
    # Define parameters
    TICKER = 'SYNTHETIC'  # Renamed ticker for synthetic data
    START_DATE = '2010-01-01'
    END_DATE = datetime.datetime.now().strftime('%Y-%m-%d')  # Current date
    INITIAL_BALANCE = 100000
    STOP_LOSS = 0.90
    TAKE_PROFIT = 1.10
    MAX_POSITION_SIZE = 0.5  # Increased from 0.25 to 0.5
    MAX_DRAWDOWN = 0.20
    ANNUAL_TRADING_DAYS = 252
    TRANSACTION_COST = 0.001  # 0.1% per trade

    # Generate and prepare synthetic data
    df = generate_synthetic_data(num_days=1500, start_price=100.0, mu=0.0002, sigma=0.01, seed=RANDOM_SEED)
    if df.empty:
        main_logger.critical("Synthetic data generation failed. Exiting.")
        exit()

    # Initialize scaler
    scaler = StandardScaler()
    try:
        df[FEATURES_TO_SCALE] = scaler.fit_transform(df[FEATURES_TO_SCALE])
        main_logger.debug("Features scaled and scaler fitted.")
    except Exception as e:
        main_logger.error(f"Error during feature scaling: {e}")
        exit()

    # Split into training and testing datasets
    split_ratio = 0.8  # 80% training, 20% testing
    split_idx = int(len(df) * split_ratio)
    train_df = df.iloc[:split_idx].reset_index(drop=True)
    test_df = df.iloc[split_idx:].reset_index(drop=True)

    main_logger.info(f"Training data: {len(train_df)} samples")
    main_logger.info(f"Testing data: {len(test_df)} samples")

    # Check if test_df has sufficient data
    MIN_TEST_SAMPLES = 500  # Define a minimum number of samples for testing
    if len(test_df) < MIN_TEST_SAMPLES:
        main_logger.warning(f"Testing data has only {len(test_df)} samples. Increasing test set size to {MIN_TEST_SAMPLES}.")
        # Adjust split to ensure test_df has at least MIN_TEST_SAMPLES
        split_idx = len(df) - MIN_TEST_SAMPLES
        train_df = df.iloc[:split_idx].reset_index(drop=True)
        test_df = df.iloc[split_idx:].reset_index(drop=True)
        main_logger.info(f"Adjusted Training data: {len(train_df)} samples")
        main_logger.info(f"Adjusted Testing data: {len(test_df)} samples")

    # Save the scaler for future use (e.g., deployment)
    scaler_filename = RESULTS_DIR / 'scaler.pkl'
    joblib.dump(scaler, scaler_filename)
    main_logger.info(f"Scaler fitted on training data and saved as {scaler_filename}")

    # Verify unscaled columns in test_df
    missing_unscaled_test = [feature for feature in UNSCALED_FEATURES if feature not in test_df.columns]
    if missing_unscaled_test:
        main_logger.error(f"Missing unscaled features in test DataFrame: {missing_unscaled_test}")
        exit()
    else:
        main_logger.debug("All unscaled features are present in the test DataFrame.")
        main_logger.debug(f"Columns in test_df: {test_df.columns.tolist()}")

    # Additional Verification: Ensure required columns are present
    required_cols_verification = ['MACD_unscaled', 'Close_unscaled']
    missing_cols_verification = [col for col in required_cols_verification if col not in test_df.columns]
    if missing_cols_verification:
        main_logger.error(f"Missing required columns for MACD strategy in test DataFrame: {missing_cols_verification}")
        exit()
    else:
        main_logger.debug("All required columns for MACD strategy are present in test_df.")

    # Check environment validity
    main_logger.info("Checking environment compatibility with SB3...")
    env_checker = SingleStockTradingEnv(
        df=train_df,
        scaler=scaler,
        initial_balance=INITIAL_BALANCE,
        stop_loss=STOP_LOSS,
        take_profit=TAKE_PROFIT,
        max_position_size=MAX_POSITION_SIZE,
        max_drawdown=MAX_DRAWDOWN,
        annual_trading_days=ANNUAL_TRADING_DAYS,
        transaction_cost=TRANSACTION_COST,
        env_rank=-1  # Assign a default env_rank for the checker
    )
    try:
        check_env(env_checker, warn=True)
        main_logger.info("Environment is valid!")
    except Exception as e:
        main_logger.critical(f"Environment check failed: {e}")
        exit()

    ##############################################
    # Optuna Hyperparameter Tuning
    ##############################################

    # Log Phase: Hyperparameter Tuning Starting
    log_phase("Hyperparameter Tuning", "Starting", {"total_trials": 100})

    # Optuna hyperparameter tuning with limited concurrent trials to prevent overload
    main_logger.info("Starting hyperparameter tuning with Optuna...")

    # Optuna storage using SQLite for persistence
    storage = optuna.storages.RDBStorage(
        url='sqlite:///optuna_study.db',
        engine_kwargs={'connect_args': {'check_same_thread': False}}
    )

    # Generate a unique study name
    unique_study_name = generate_unique_study_name()

    study = optuna.create_study(
        direction='maximize',
        sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED),
        storage=storage,
        study_name=unique_study_name,  # Unique Study Name
        load_if_exists=False  # Ensure a new study is created
    )
    study.optimize(
        lambda trial: objective(trial, train_df, scaler, INITIAL_BALANCE, STOP_LOSS, TAKE_PROFIT,
                               MAX_POSITION_SIZE, MAX_DRAWDOWN, ANNUAL_TRADING_DAYS, TRANSACTION_COST),
        n_trials=100,  # Adjust as needed
        n_jobs=4  # Limit to 4 concurrent trials to prevent system overload
    )

    if study.best_params:
        best_params = study.best_params
        main_logger.info(f"Best hyperparameters found: {best_params}")
    else:
        main_logger.critical("No successful trials found in Optuna study.")
        exit()

    # Log Phase: Hyperparameter Tuning Completed
    log_phase("Hyperparameter Tuning", "Completed", {"best_params": best_params})

    ##############################################
    # Main Training
    ##############################################

    # Assign unique env_rank for main training
    main_env_rank = 0  # Unique ID for main training

    # Create environment parameters with best reward weights
    env_params = {
        'df': train_df,
        'scaler': scaler,
        'initial_balance': INITIAL_BALANCE,
        'stop_loss': STOP_LOSS,
        'take_profit': TAKE_PROFIT,
        'max_position_size': MAX_POSITION_SIZE,
        'max_drawdown': MAX_DRAWDOWN,
        'annual_trading_days': ANNUAL_TRADING_DAYS,
        'transaction_cost': TRANSACTION_COST,
        'reward_weights': {'gamma': 1.0}  # Simplified reward weights
    }

    # Log Phase: Main Training Starting
    log_phase("Main Training", "Starting", {"env_rank": main_env_rank, "total_timesteps": 500000, "reward_weights": env_params['reward_weights']})

    # Initialize environment
    vec_env = DummyVecEnv([make_env(env_params, main_env_rank, RANDOM_SEED)])

    main_logger.info(f"Initialized DummyVecEnv with env_rank={main_env_rank} for main training.")

    # Define policy kwargs
    policy_kwargs = dict(
        activation_fn=torch.nn.ReLU,
        net_arch=[int(x) for x in best_params.get('net_arch', '128_128').split('_')]
    )

    # Define unique TensorBoard log directory for main training
    main_training_log_dir = TB_LOG_DIR / f"main_training_{main_env_rank}"
    main_training_log_dir.mkdir(parents=True, exist_ok=True)

    # Initialize PPO model with best hyperparameters and enable CPU usage
    try:
        model = PPO(
            'MlpPolicy',
            vec_env,
            verbose=1,  # Set verbose to 1 to enable logging
            seed=RANDOM_SEED,
            policy_kwargs=policy_kwargs,
            learning_rate=best_params.get('learning_rate', 3e-4),
            n_steps=best_params.get('n_steps', 128),
            batch_size=best_params.get('batch_size', 64),
            gamma=best_params.get('gamma', 0.99),
            gae_lambda=best_params.get('gae_lambda', 0.95),
            clip_range=best_params.get('clip_range', 0.2),
            ent_coef=best_params.get('ent_coef', 0.02),  # Increased entropy coefficient for exploration
            vf_coef=best_params.get('vf_coef', 0.5),
            max_grad_norm=best_params.get('max_grad_norm', 0.5),
            tensorboard_log=str(main_training_log_dir),
            device='cpu'  # Use CPU
        )
    except Exception as e:
        main_logger.critical(f"Model initialization failed: {e}")
        exit()

    # Define checkpoint and custom callbacks
    checkpoint_callback = CheckpointCallback(
        save_freq=50000,  # Reduced frequency to prevent overload
        save_path=str(RESULTS_DIR / "checkpoints"),
        name_prefix="ppo_model"
    )
    custom_callback = CustomTensorboardCallback()
    early_stopping_callback = EarlyStoppingCallback(
        monitor='train/reward_env',
        patience=20,  # Increased patience
        min_delta=1e-5,  # Lowered min_delta
        verbose=1
    )

    # Create a CallbackList
    callback_list = CallbackList([custom_callback, checkpoint_callback, early_stopping_callback])

    # Start training
    start_time = time.time()
    try:
        model.learn(
            total_timesteps=500000,  # Increased training steps
            callback=callback_list
        )
    except Exception as e:
        main_logger.critical(f"Training failed: {e}")
        exit()
    duration = time.time() - start_time

    # Log Phase: Main Training Completed
    log_phase("Main Training", "Completed", {"env_rank": main_env_rank, "total_timesteps": 500000}, duration)

    # Save the trained model
    model_path = RESULTS_DIR / f"ppo_model_{TICKER}.zip"
    model.save(str(model_path))
    main_logger.info(f"Model trained and saved at {model_path}")

    ##############################################
    # Testing
    ##############################################

    # Log Phase: Testing Starting
    log_phase("Testing Phase", "Starting", {"env_rank": 999, "data_points": len(test_df)})

    main_logger.info("Starting testing of PPO agent...")

    # Assign unique env_rank for testing
    test_env_rank = 999  # Unique ID for testing

    # Initialize test environment
    test_env = SingleStockTradingEnv(
        df=test_df,
        scaler=scaler,
        initial_balance=INITIAL_BALANCE,
        stop_loss=STOP_LOSS,
        take_profit=TAKE_PROFIT,
        max_position_size=MAX_POSITION_SIZE,
        max_drawdown=MAX_DRAWDOWN,
        annual_trading_days=ANNUAL_TRADING_DAYS,
        transaction_cost=TRANSACTION_COST,
        env_rank=test_env_rank,
        reward_weights={'gamma': 1.0}  # Simplified reward weights
    )
    test_vec_env = DummyVecEnv([lambda: test_env])
    test_env.seed(RANDOM_SEED + test_env_rank)

    main_logger.info(f"Initialized DummyVecEnv with env_rank={test_env_rank} for testing.")

    # Log the size of test_df
    main_logger.info(f"Testing data has {len(test_df)} samples.")

    # Start testing
    test_start_time = time.time()
    try:
        obs = test_vec_env.reset()
    except Exception as e:
        main_logger.critical(f"Reset failed during testing: {e}")
        exit()

    done = False
    steps_taken = 0
    while not done and steps_taken < 1000:  # Prevent infinite loops
        try:
            action, _ = model.predict(obs, deterministic=True)
            obs, rewards_step, dones, infos = test_vec_env.step(action)
            done = dones[0]
            steps_taken += 1
            testing_logger.debug(f"[Test Env {test_env_rank}] Step {steps_taken}: Action Taken = {action}, Reward = {rewards_step[0]}")
        except Exception as e:
            main_logger.critical(f"Step failed during testing: {e}")
            break

    duration = time.time() - test_start_time
    # Log Phase: Testing Completed
    log_phase("Testing Phase", "Completed", {"env_rank": test_env_rank, "data_points": len(test_df), "steps_taken": steps_taken}, duration)

    # Create DataFrame for RL Agent Performance from environment history
    rl_env = test_vec_env.envs[0]
    if hasattr(rl_env, 'history') and rl_env.history:
        rl_df = pd.DataFrame(rl_env.history)
    else:
        main_logger.error("Test environment does not have a 'history' attribute or it's empty.")
        rl_df = pd.DataFrame()

    # Ensure 'Reward' column exists
    if 'Reward' not in rl_df.columns:
        rl_df['Reward'] = 0.0
        main_logger.critical("'Reward' column not found in RL history. Defaulting rewards to 0.")

    # Calculate Performance Metrics
    if not rl_df.empty:
        returns = np.array(rl_df['Reward'])
        rl_final_net_worth = rl_df['Net Worth'].iloc[-1]
        rl_profit = rl_final_net_worth - INITIAL_BALANCE
        rl_max_dd = calculate_max_drawdown(rl_df['Net Worth'])
        rl_annualized_return = calculate_annualized_return(rl_df['Net Worth'])
    else:
        returns = np.array([])
        rl_final_net_worth = INITIAL_BALANCE
        rl_profit = 0
        rl_max_dd = 0
        rl_annualized_return = 0

    # Evaluate Baseline Strategies on Test Data
    main_logger.info("Evaluating Baseline Strategies on Test Data...")
    bh_result = buy_and_hold(test_df, initial_balance=INITIAL_BALANCE, transaction_cost=TRANSACTION_COST)
    macd_result = macd_strategy(test_df, initial_balance=INITIAL_BALANCE, transaction_cost=TRANSACTION_COST, max_position_size=MAX_POSITION_SIZE)
    ma_crossover_result = moving_average_crossover(test_df, initial_balance=INITIAL_BALANCE, transaction_cost=TRANSACTION_COST, max_position_size=MAX_POSITION_SIZE)
    bb_result = bollinger_bands_strategy(test_df, initial_balance=INITIAL_BALANCE, transaction_cost=TRANSACTION_COST, max_position_size=MAX_POSITION_SIZE)
    random_result = random_strategy(test_df, initial_balance=INITIAL_BALANCE, transaction_cost=TRANSACTION_COST, max_position_size=MAX_POSITION_SIZE)

    # Log Baseline Results
    for result in [bh_result, macd_result, ma_crossover_result, bb_result, random_result]:
        main_logger.critical(f"Strategy: {result['Strategy']}")
        main_logger.critical(f"  Initial Balance: ${result['Initial Balance']}")
        main_logger.critical(f"  Final Net Worth: ${result['Final Net Worth']:.2f}")
        main_logger.critical(f"  Profit: ${result['Profit']:.2f}")
        if 'Invested Capital' in result:
            main_logger.critical(f"  Invested Capital: ${result['Invested Capital']:.2f}")
            main_logger.critical(f"  Transaction Costs: ${result['Transaction Costs']:.2f}")
        main_logger.critical("-" * 50)

    # Log and print RL Agent Results
    main_logger.critical("RL Agent Performance:")
    main_logger.critical(f"  Final Net Worth: ${rl_final_net_worth:.2f}")
    main_logger.critical(f"  Profit: ${rl_profit:.2f}")
    main_logger.critical(f"  Annualized Return: {rl_annualized_return*100:.2f}%")
    main_logger.critical(f"  Max Drawdown: {rl_max_dd*100:.2f}%")

    # Plot RL Training History
    plot_rl_training_history(rl_df)

    # Plot Trading Results
    plot_results(test_df, rl_df, TICKER)

    # Plot Agent's Performance Metrics
    plot_agent_performance(rl_df, TICKER)

    # Plot Action Distribution
    plot_action_distribution(rl_df)

    # Instructions for TensorBoard
    main_logger.critical("Training logs are stored for TensorBoard.")
    main_logger.critical("To view them, run the following command in your terminal:")
    main_logger.critical(f"tensorboard --logdir {TB_LOG_DIR}")
    main_logger.critical("Then open http://localhost:6006 in your browser to visualize the training metrics.")

    # Optional: Visualize Optuna study results
    try:
        import optuna.visualization as vis

        # Plot optimization history
        fig1 = vis.plot_optimization_history(study)
        fig1.savefig(PLOTS_DIR / "optuna_optimization_history.png")
        fig1.show()

        # Plot parameter importances
        fig2 = vis.plot_param_importances(study)
        fig2.savefig(PLOTS_DIR / "optuna_param_importances.png")
        fig2.show()
    except ImportError:
        main_logger.warning("Optuna visualization module not found. Install it via pip if you wish to visualize study results.")

    # Log Phase: All Phases Completed
    total_duration = time.time() - start_time
    log_phase("All Phases", "Completed", {"total_duration_seconds": total_duration}, total_duration)

    main_logger.info("Script execution completed successfully.")
