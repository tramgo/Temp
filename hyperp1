# 1) normal step logic: net_worth, net_worth_change, etc...
# 2) standard profit reward, forced_stop, transaction penalty, etc...
reward += profit_reward + forced_stop_penalty + ...

# 3) LAYERED DRAWDOWN LOGIC with partial/full forced liquidation
self.peak = max(self.peak, net_worth)
if self.peak > 0:
    current_drawdown = (self.peak - net_worth) / self.peak
else:
    current_drawdown = 0.0

layered_penalty = 0.0
if self.position > 0 and current_drawdown > 0:
    # (A) Minor threshold at 1% drawdown
    if current_drawdown > 0.01:
        # Subtract a base penalty (2.0) plus 1% * peak * factor
        layered_penalty -= (2.0 + 0.01 * self.peak * self.some_factor)

    # (B) If drawdown > 2% => multiply penalty by 1.25
    if current_drawdown > 0.02:
        layered_penalty = -abs(layered_penalty) * 1.25

    # (C) If drawdown > 3% => partial forced liquidation + multiply again
    if current_drawdown > 0.03:
        shares_to_sell = math.floor(self.position * 0.5)
        if shares_to_sell > 0:
            proceeds = shares_to_sell * current_price * (1 - self.transaction_cost)
            self.balance += proceeds
            self.position -= shares_to_sell
            self.training_logger.info(
                f"[Env {self.env_rank}] Partial forced liquidation at ~3% drawdown. "
                f"Selling {shares_to_sell} shares..."
            )
        # multiply penalty by 1.25 again
        layered_penalty = -abs(layered_penalty) * 1.25

    # (D) If drawdown > 5% => full forced liquidation + multiply once more
    if current_drawdown > 0.05:
        shares_to_sell = self.position
        if shares_to_sell > 0:
            proceeds = shares_to_sell * current_price * (1 - self.transaction_cost)
            self.balance += proceeds
            self.position = 0
            self.training_logger.info(
                f"[Env {self.env_rank}] Full forced liquidation at ~5% drawdown."
            )
        # multiply penalty by 1.25 again
        layered_penalty = -abs(layered_penalty) * 1.25

# Finally add the layered penalty to the reward
reward += layered_penalty
