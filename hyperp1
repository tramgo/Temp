def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, dict]:
    # 1) Standard check for action, end-of-data, buy/sell logic, etc...
    # net_worth, net_worth_change

    # 2) Basic profit reward
    profit_weight = self.reward_weights.get('profit_weight', 1.5)
    profit_reward = (net_worth_change / self.initial_balance) * profit_weight
    reward = profit_reward

    # 3) forced_stop, transaction_penalty, sharpe_bonus, etc...
    # e.g. reward += forced_stop_penalty + transaction_penalty + sharpe_bonus

    # (A) measure a "typical step magnitude"
    # Suppose we keep a rolling returns_window or net_worth_changes
    # Example: store net_worth_change each step
    self.networth_change_buffer.append(net_worth_change)  # add to a deque
    if len(self.networth_change_buffer) > 500:
        self.networth_change_buffer.popleft()

    # If we have enough data, compute average
    typical_step = 0.0
    if len(self.networth_change_buffer) >= 20:
        # average absolute networth change
        avg_abs_change = np.mean([abs(x) for x in self.networth_change_buffer])
        # scale it up to a "reward range" by multipying by some factor, or keep it raw
        typical_step = avg_abs_change  # e.g. typical net worth change

    # If typical_step is near zero, fallback
    if typical_step < 1e-6:
        typical_step = 1.0  # ensures we don't get weird results

    # 4) Drawdown logic
    self.peak = max(self.peak, net_worth)
    if self.peak > 0:
        current_drawdown = (self.peak - net_worth) / self.peak
    else:
        current_drawdown = 0.0

    layered_penalty = 0.0
    # Only if we have a position and are actually exposed
    if self.position > 0 and current_drawdown > 0:
        # (B) Smaller threshold => penalty ~1.5 * typical
        if current_drawdown > 0.01:  # 1% dd
            layered_penalty -= 1.5 * typical_step

        # 2% => penalty ~2.0 * typical
        if current_drawdown > 0.02:
            layered_penalty -= 2.0 * typical_step

        # 3% => partial liquidation + penalty ~2.5 * typical
        if current_drawdown > 0.03:
            shares_to_sell = math.floor(self.position * 0.5)
            if shares_to_sell > 0:
                proceeds = shares_to_sell * current_price * (1 - self.transaction_cost)
                self.balance += proceeds
                self.position -= shares_to_sell
            layered_penalty -= 2.5 * typical_step

        # 5% => full liquidation + penalty ~3.0 * typical
        if current_drawdown > 0.05:
            shares_to_sell = self.position
            if shares_to_sell > 0:
                proceeds = shares_to_sell * current_price * (1 - self.transaction_cost)
                self.balance += proceeds
                self.position = 0
            layered_penalty -= 3.0 * typical_step

    reward += layered_penalty

    # (C) Now do your dynamic scaling / np.tanh if desired
    if not hasattr(self, 'reward_history'):
        self.reward_history = collections.deque(maxlen=500)
    self.reward_history.append(reward)

    if len(self.reward_history) >= 50:
        local_std = np.std(self.reward_history)
        if local_std < 1e-6:
            local_std = 1.0
        scale_factor = 2.0 * local_std
        scaled_reward = np.tanh(reward / scale_factor)
    else:
        scaled_reward = reward

    # rest of step => store in history, increment step, etc...
    return obs, scaled_reward, terminated, truncated, {}
